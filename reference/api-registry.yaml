# Kailash SDK API Registry
# LLM-optimized reference for quick API lookup
# Version: 0.1.4
# Last Updated: 2025-01-06

workflow:
  class: kailash.Workflow
  description: "Core workflow management class"
  import: "from kailash import Workflow"
  methods:
    add_node:
      signature: "add_node(node_id: str, node_or_type: Any, **config) -> None"
      description: "Add a node to the workflow"
      params:
        node_id: "Unique identifier for the node"
        node_or_type: "Node instance, Node class, or node type name"
        config: "Configuration as keyword arguments"
      example: |
        workflow.add_node('reader', CSVReaderNode(), file_path='data.csv')

    connect:
      signature: "connect(source_node: str, target_node: str, mapping: Optional[Dict[str, str]] = None) -> None"
      description: "Connect two nodes in the workflow"
      params:
        source_node: "Source node ID"
        target_node: "Target node ID"
        mapping: "Dictionary mapping source outputs to target inputs"
      example: |
        workflow.connect('reader', 'processor', mapping={'data': 'data'})
        workflow.connect('processor', 'writer')

    execute:
      signature: "execute(inputs: Optional[Dict[str, Any]] = None, task_manager: Optional[TaskManager] = None) -> Dict[str, Any]"
      description: "Execute the workflow directly"
      params:
        inputs: "Initial inputs to the workflow"
        task_manager: "Optional task manager for tracking"
      example: |
        # Direct execution
        results = workflow.execute(inputs={'input_data': [1, 2, 3]})

        # Or through runtime
        runtime = LocalRuntime()
        results, run_id = runtime.execute(workflow, inputs={'input_data': [1, 2, 3]})

    validate:
      signature: "validate() -> bool"
      description: "Validate workflow structure and connections"
      example: |
        workflow.validate()  # Raises exception if invalid

    to_dict:
      signature: "to_dict() -> dict"
      description: "Export workflow as dictionary"
      example: |
        workflow_dict = workflow.to_dict()

    from_dict:
      signature: "from_dict(data: dict) -> Workflow"
      description: "Create workflow from dictionary"
      example: |
        workflow = Workflow.from_dict(workflow_dict)

workflow_builder:
  class: kailash.WorkflowBuilder
  description: "Fluent API for building workflows (NOTE: Prefer using Workflow.connect() instead)"
  import: "from kailash import WorkflowBuilder"
  deprecated: "Use Workflow class with connect() method for clarity"
  methods:
    create:
      signature: "create(name: str, description: str = '') -> WorkflowBuilder"
      description: "Start building a new workflow"
      example: |
        builder = WorkflowBuilder().create('my_pipeline', 'Data processing pipeline')

    add_node:
      signature: "add_node(node_id: str, node_class: Type[Node], config: dict = None) -> WorkflowBuilder"
      description: "Add a node to the workflow"
      example: |
        builder.add_node('reader', CSVReaderNode, {'file_path': 'data.csv'})

    connect:
      signature: "connect(from_node: str, to_node: str, **kwargs) -> WorkflowBuilder"
      description: "Connect nodes in the workflow"
      example: |
        builder.connect('reader', 'processor')

    build:
      signature: "build() -> Workflow"
      description: "Build and return the workflow"
      example: |
        workflow = builder.build()

runtime:
  local_runtime:
    class: kailash.LocalRuntime
    description: "Execute workflows locally"
    import: "from kailash import LocalRuntime"
    usage: |
      runtime = LocalRuntime()
      results, run_id = runtime.execute(workflow)

  docker_runtime:
    class: kailash.runtime.DockerRuntime
    description: "Execute workflows in Docker containers"
    import: "from kailash.runtime import DockerRuntime"
    usage: |
      runtime = DockerRuntime(base_image='python:3.9')
      results = workflow.execute(runtime)

nodes:
  # Data Input/Output Nodes
  csv_reader:
    class: kailash.nodes.data.CSVReaderNode
    description: "Read CSV files"
    import: "from kailash.nodes.data import CSVReaderNode"
    config:
      file_path: "str - Path to CSV file (required)"
      delimiter: "str - CSV delimiter (default: ',')"
      has_header: "bool - First row contains headers (default: True)"
      encoding: "str - File encoding (default: 'utf-8')"
    outputs:
      default: "List[Dict] - Parsed CSV data as list of dictionaries"
    example: |
      workflow.add_node('reader', CSVReaderNode(), file_path='data.csv', delimiter=',')

  csv_writer:
    class: kailash.nodes.data.CSVWriterNode
    description: "Write data to CSV files"
    import: "from kailash.nodes.data import CSVWriterNode"
    config:
      file_path: "str - Output CSV file path (required)"
      delimiter: "str - CSV delimiter (default: ',')"
      write_header: "bool - Write header row (default: True)"
    inputs:
      default: "List[Dict] - Data to write"
    example: |
      workflow.add_node('writer', CSVWriterNode(), file_path='output.csv')

  json_reader:
    class: kailash.nodes.data.JSONReaderNode
    description: "Read JSON files"
    import: "from kailash.nodes.data import JSONReaderNode"
    config:
      file_path: "str - Path to JSON file (required)"
      encoding: "str - File encoding (default: 'utf-8')"
    outputs:
      default: "Any - Parsed JSON data"
    example: |
      workflow.add_node('reader', JSONReaderNode(), file_path='data.json')

  json_writer:
    class: kailash.nodes.data.JSONWriterNode
    description: "Write data to JSON files"
    import: "from kailash.nodes.data import JSONWriterNode"
    config:
      file_path: "str - Output JSON file path (required)"
      indent: "int - JSON indentation (default: 2)"
      ensure_ascii: "bool - Escape non-ASCII (default: False)"
    inputs:
      default: "Any - Data to write"
    example: |
      workflow.add_node('writer', JSONWriterNode(), file_path='output.json', indent=4)

  text_reader:
    class: kailash.nodes.data.TextReaderNode
    description: "Read text files"
    import: "from kailash.nodes.data import TextReaderNode"
    config:
      file_path: "str - Path to text file (required)"
      encoding: "str - File encoding (default: 'utf-8')"
    outputs:
      default: "str - Text content"
    example: |
      workflow.add_node('reader', TextReaderNode(), file_path='document.txt')

  text_writer:
    class: kailash.nodes.data.TextWriterNode
    description: "Write text to files"
    import: "from kailash.nodes.data import TextWriterNode"
    config:
      file_path: "str - Output text file path (required)"
      encoding: "str - File encoding (default: 'utf-8')"
      append: "bool - Append to file (default: False)"
    inputs:
      default: "str - Text to write"
    example: |
      workflow.add_node('writer', TextWriterNode(), file_path='output.txt')

  # A2A Communication Nodes
  shared_memory_pool:
    class: kailash.nodes.ai.SharedMemoryPoolNode
    description: "Central memory pool for agent-to-agent communication"
    import: "from kailash.nodes.ai.a2a import SharedMemoryPoolNode"
    config:
      action: "str - Memory operation (read, write, subscribe, query)"
      agent_id: "str - ID of the agent performing action"
      content: "str - Content to write (for write action)"
      attention_filter: "dict - Filter criteria for reading memories"
      tags: "List[str] - Memory tags for categorization"
      importance: "float - Importance score (0-1)"
    outputs:
      result: "dict - Memory operation result"
    example: |
      workflow.add_node('memory', SharedMemoryPoolNode(),
          action='write',
          agent_id='researcher_001',
          content='Key finding about correlation',
          tags=['research', 'correlation'],
          importance=0.8
      )

  a2a_agent:
    class: kailash.nodes.ai.A2AAgentNode
    description: "Enhanced LLM agent with A2A communication capabilities"
    import: "from kailash.nodes.ai.a2a import A2AAgentNode"
    config:
      agent_id: "str - Unique agent identifier"
      agent_role: "str - Agent's role (researcher, analyst, etc.)"
      provider: "str - LLM provider (openai, anthropic, ollama)"
      model: "str - LLM model to use"
      memory_pool: "SharedMemoryPoolNode - Reference to memory pool"
      attention_filter: "dict - Criteria for filtering relevant information"
      temperature: "float - Sampling temperature"
      max_tokens: "int - Maximum response tokens"
    outputs:
      result: "dict - Agent execution result with memory updates"
    example: |
      workflow.add_node('agent', A2AAgentNode(),
          agent_id='researcher_001',
          provider='openai',
          model='gpt-4',
          memory_pool=memory_pool,
          attention_filter={'tags': ['data', 'analysis']}
      )

  a2a_coordinator:
    class: kailash.nodes.ai.A2ACoordinatorNode
    description: "Coordinate communication and task delegation between agents"
    import: "from kailash.nodes.ai.a2a import A2ACoordinatorNode"
    config:
      action: "str - Coordination action (register, delegate, broadcast, consensus)"
      agent_info: "dict - Agent information for registration"
      task: "dict - Task to delegate or coordinate"
      coordination_strategy: "str - Strategy (best_match, round_robin, auction)"
      available_agents: "List[dict] - List of available agents"
      proposals: "List[dict] - Proposals for consensus"
      voting_agents: "List[str] - Agent IDs for voting"
    outputs:
      result: "dict - Coordination result"
    example: |
      workflow.add_node('coordinator', A2ACoordinatorNode(),
          action='delegate',
          task={'type': 'research', 'description': 'Analyze trends'},
          available_agents=[{'id': 'agent1', 'skills': ['research']}],
          coordination_strategy='best_match'
      )

  # Self-Organizing Agent Nodes
  agent_pool_manager:
    class: kailash.nodes.ai.AgentPoolManagerNode
    description: "Manage pool of self-organizing agents"
    import: "from kailash.nodes.ai.self_organizing import AgentPoolManagerNode"
    config:
      action: "str - Pool operation (register, find_by_capability, update_status)"
      agent_id: "str - ID of the agent"
      capabilities: "List[str] - List of agent capabilities"
      required_capabilities: "List[str] - Capabilities required for search"
    outputs:
      result: "dict - Pool operation result"
    example: |
      workflow.add_node('pool', AgentPoolManagerNode(),
          action='register',
          agent_id='research_agent_001',
          capabilities=['data_analysis', 'research']
      )

  problem_analyzer:
    class: kailash.nodes.ai.ProblemAnalyzerNode
    description: "Analyze problems for capability requirements"
    import: "from kailash.nodes.ai.self_organizing import ProblemAnalyzerNode"
    config:
      problem_description: "str - Description of problem to solve"
      context: "dict - Additional context about the problem"
      decomposition_strategy: "str - Strategy for decomposing problem"
    outputs:
      analysis: "dict - Problem analysis result"
    example: |
      workflow.add_node('analyzer', ProblemAnalyzerNode(),
          problem_description='Predict customer churn',
          context={'domain': 'business', 'urgency': 'high'}
      )

  team_formation:
    class: kailash.nodes.ai.TeamFormationNode
    description: "Form optimal teams based on problem requirements"
    import: "from kailash.nodes.ai.self_organizing import TeamFormationNode"
    config:
      problem_analysis: "dict - Analysis from ProblemAnalyzerNode"
      available_agents: "List[dict] - List of available agents"
      formation_strategy: "str - Team formation strategy"
      constraints: "dict - Constraints for team formation"
    outputs:
      team: "dict - Formed team configuration"
    example: |
      workflow.add_node('formation', TeamFormationNode(),
          formation_strategy='capability_matching',
          available_agents=agents
      )

  self_organizing_agent:
    class: kailash.nodes.ai.SelfOrganizingAgentNode
    description: "Agent that can autonomously join teams and collaborate"
    import: "from kailash.nodes.ai.self_organizing import SelfOrganizingAgentNode"
    config:
      agent_id: "str - Unique agent identifier"
      capabilities: "List[str] - Agent's capabilities"
      team_context: "dict - Current team information"
      collaboration_mode: "str - Mode (cooperative, competitive, mixed)"
      autonomy_level: "float - Level of autonomous decision making"
    outputs:
      result: "dict - Agent execution result"
    example: |
      workflow.add_node('agent', SelfOrganizingAgentNode(),
          agent_id='adaptive_agent_001',
          capabilities=['data_analysis', 'machine_learning'],
          team_context={'team_id': 'research_team_1'}
      )

  solution_evaluator:
    class: kailash.nodes.ai.SolutionEvaluatorNode
    description: "Evaluate solutions and determine if iteration is needed"
    import: "from kailash.nodes.ai.self_organizing import SolutionEvaluatorNode"
    config:
      solution: "dict - Solution to evaluate"
      problem_requirements: "dict - Original problem requirements"
      team_performance: "dict - Team performance metrics"
      evaluation_criteria: "dict - Custom evaluation criteria"
    outputs:
      evaluation: "dict - Solution evaluation result"
    example: |
      workflow.add_node('evaluator', SolutionEvaluatorNode(),
          solution={'approach': 'ML model', 'confidence': 0.85},
          problem_requirements={'quality_threshold': 0.8}
      )

  # Intelligent Orchestration Nodes
  intelligent_cache:
    class: kailash.nodes.ai.IntelligentCacheNode
    description: "Intelligent caching with semantic similarity"
    import: "from kailash.nodes.ai.intelligent_agent_orchestrator import IntelligentCacheNode"
    config:
      action: "str - Cache operation (cache, get, invalidate, stats, cleanup)"
      cache_key: "str - Unique key for cached item"
      data: "Any - Data to cache"
      metadata: "dict - Metadata including source, cost, semantic tags"
      ttl: "int - Time to live in seconds"
      similarity_threshold: "float - Threshold for semantic matching"
    outputs:
      result: "Any - Cache operation result"
    example: |
      workflow.add_node('cache', IntelligentCacheNode(),
          action='cache',
          cache_key='weather_api_nyc',
          ttl=3600
      )

  mcp_agent:
    class: kailash.nodes.ai.MCPAgentNode
    description: "Self-organizing agent with MCP integration"
    import: "from kailash.nodes.ai.intelligent_agent_orchestrator import MCPAgentNode"
    config:
      agent_id: "str - Unique agent identifier"
      capabilities: "List[str] - Agent capabilities"
      mcp_servers: "List[dict] - MCP server configurations"
      cache_node_id: "str - ID of cache node"
      tool_preferences: "dict - Agent's tool preferences"
      cost_awareness: "float - Cost consciousness (0-1)"
    outputs:
      result: "dict - Agent execution result"
    example: |
      workflow.add_node('mcp_agent', MCPAgentNode(),
          agent_id='mcp_agent_001',
          capabilities=['data_analysis', 'api_integration'],
          mcp_servers=[{'name': 'weather_server', 'command': 'python'}]
      )

  orchestration_manager:
    class: kailash.nodes.ai.OrchestrationManagerNode
    description: "Central coordinator for self-organizing workflows"
    import: "from kailash.nodes.ai.intelligent_agent_orchestrator import OrchestrationManagerNode"
    config:
      query: "str - Main query or problem to solve"
      context: "dict - Additional context for the query"
      agent_pool_size: "int - Number of agents in the pool"
      mcp_servers: "List[dict] - MCP server configurations"
      max_iterations: "int - Maximum number of solution iterations"
      quality_threshold: "float - Quality threshold for solution acceptance"
      time_limit_minutes: "int - Maximum time limit for solution"
      enable_caching: "bool - Enable intelligent caching"
    outputs:
      result: "dict - Orchestration result"
    example: |
      workflow.add_node('orchestrator', OrchestrationManagerNode(),
          query='Analyze market trends and develop strategy',
          agent_pool_size=15,
          max_iterations=3
      )

  # AI/ML Nodes
  llm_agent:
    class: kailash.nodes.ai.LLMAgentNode
    description: "Interact with language models"
    import: "from kailash.nodes.ai import LLMAgentNode"
    config:
      provider: "str - LLM provider ('openai', 'anthropic', 'ollama')"
      model: "str - Model name (e.g., 'gpt-4', 'claude-3')"
      api_key: "str - API key (or use env var)"
      temperature: "float - Sampling temperature (default: 0.7)"
      max_tokens: "int - Maximum tokens (default: 1000)"
      system_prompt: "str - System prompt (optional)"
    inputs:
      prompt: "str - User prompt"
      context: "Any - Additional context (optional)"
    outputs:
      response: "str - Model response"
      metadata: "dict - Response metadata"
    example: |
      workflow.add_node('llm', LLMAgentNode(),
          provider='openai',
          model='gpt-4',
          temperature=0.7,
          system_prompt='You are a helpful assistant.'
      )

  embedding_generator:
    class: kailash.nodes.ai.EmbeddingGeneratorNode
    description: "Generate text embeddings"
    import: "from kailash.nodes.ai import EmbeddingGeneratorNode"
    config:
      provider: "str - Embedding provider"
      model: "str - Embedding model name"
      api_key: "str - API key (or use env var)"
      batch_size: "int - Batch size for processing (default: 100)"
    inputs:
      texts: "List[str] - Texts to embed"
    outputs:
      embeddings: "List[List[float]] - Text embeddings"
      metadata: "dict - Embedding metadata"
    example: |
      workflow.add_node('embedder', EmbeddingGeneratorNode(),
          provider='openai',
          model='text-embedding-ada-002'
      )

  # API Nodes
  http_request:
    class: kailash.nodes.api.HTTPRequestNode
    description: "Make HTTP requests"
    import: "from kailash.nodes.api import HTTPRequestNode"
    config:
      url: "str - Request URL (required)"
      method: "str - HTTP method (default: 'GET')"
      headers: "dict - Request headers (optional)"
      params: "dict - Query parameters (optional)"
      timeout: "int - Request timeout in seconds (default: 30)"
    inputs:
      data: "Any - Request body (for POST/PUT)"
    outputs:
      response: "dict - Response data"
      status_code: "int - HTTP status code"
      headers: "dict - Response headers"
    example: |
      workflow.add_node('http', HTTPRequestNode(),
          url='https://api.example.com/data',
          method='GET',
          headers={'Authorization': 'Bearer token'}
      )

  rest_client:
    class: kailash.nodes.api.RESTClientNode
    description: "RESTful API client with advanced features"
    import: "from kailash.nodes.api import RESTClientNode"
    config:
      base_url: "str - Base URL for API"
      auth_type: "str - Authentication type ('bearer', 'basic', 'api_key')"
      auth_config: "dict - Authentication configuration"
      rate_limit: "int - Requests per second (optional)"
      retry_config: "dict - Retry configuration (optional)"
    inputs:
      endpoint: "str - API endpoint"
      method: "str - HTTP method"
      data: "Any - Request data (optional)"
    outputs:
      response: "Any - API response"
      metadata: "dict - Response metadata"
    example: |
      node = RESTClientNode()
      config = {
          'base_url': 'https://api.example.com',
          'auth_type': 'bearer',
          'auth_config': {'token': 'your-token'},
          'rate_limit': 10
      }

  # Transform Nodes
  data_transformer:
    class: kailash.nodes.transform.DataTransformerNode
    description: "Transform data using operations"
    import: "from kailash.nodes.transform import DataTransformerNode"
    config:
      operations: "List[dict] - List of transformation operations"
    inputs:
      data: "Any - Data to transform"
    outputs:
      transformed: "Any - Transformed data"
    operations:
      - filter: "Filter data based on conditions"
      - map: "Map/transform each item"
      - reduce: "Aggregate data"
      - sort: "Sort data"
      - group: "Group data by key"
    example: |
      node = DataTransformerNode()
      config = {
          'operations': [
              {'type': 'filter', 'condition': 'age > 18'},
              {'type': 'map', 'expression': 'name.upper()'},
              {'type': 'sort', 'key': 'age', 'reverse': True}
          ]
      }

  # Logic Nodes
  switch:
    class: kailash.nodes.logic.SwitchNode
    description: "Route data based on conditions"
    import: "from kailash.nodes.logic import SwitchNode"
    config:
      conditions: "List[dict] - Routing conditions"
      default_output: "str - Default output port (optional)"
    inputs:
      data: "Any - Data to evaluate"
    outputs:
      (dynamic): "Multiple outputs based on conditions"
    example: |
      node = SwitchNode()
      config = {
          'conditions': [
              {'output': 'small', 'expression': 'value < 10'},
              {'output': 'medium', 'expression': 'value < 100'},
              {'output': 'large', 'expression': 'value >= 100'}
          ],
          'default_output': 'unknown'
      }

  merge:
    class: kailash.nodes.logic.MergeNode
    description: "Merge multiple data streams"
    import: "from kailash.nodes.logic import MergeNode"
    config:
      merge_type: "str - Merge strategy ('concat', 'zip', 'dict')"
      wait_for_all: "bool - Wait for all inputs (default: True)"
    inputs:
      (multiple): "Multiple named inputs"
    outputs:
      merged: "Any - Merged data"
    example: |
      node = MergeNode()
      config = {
          'merge_type': 'dict',
          'wait_for_all': True
      }

  workflow_node:
    class: kailash.nodes.logic.WorkflowNode
    description: "Execute nested workflows"
    import: "from kailash.nodes.logic import WorkflowNode"
    config:
      workflow: "Workflow - Nested workflow instance"
      input_mapping: "dict - Map inputs to nested workflow"
      output_mapping: "dict - Map outputs from nested workflow"
    inputs:
      (dynamic): "Based on input_mapping"
    outputs:
      (dynamic): "Based on output_mapping"
    example: |
      nested_workflow = Workflow('nested')
      # ... build nested workflow ...

      node = WorkflowNode()
      config = {
          'workflow': nested_workflow,
          'input_mapping': {'data': 'nested_input'},
          'output_mapping': {'nested_output': 'result'}
      }

  # Code Execution
  python_code:
    class: kailash.nodes.code.PythonCodeNode
    description: "Execute custom Python code"
    import: "from kailash.nodes.code import PythonCodeNode"
    config:
      code: "str - Python code to execute"
      requirements: "List[str] - Required packages (optional)"
      timeout: "int - Execution timeout in seconds (default: 30)"
    inputs:
      (dynamic): "Defined in code"
    outputs:
      (dynamic): "Returned from code"
    example: |
      node = PythonCodeNode()
      config = {
          'code': '''
      def execute(data):
          # Process data
          result = [x * 2 for x in data]
          return {'doubled': result}
      ''',
          'requirements': ['numpy', 'pandas']
      }

# API Wrapper
api_wrapper:
  workflow_api:
    class: kailash.api.WorkflowAPI
    description: "Expose workflows as REST APIs"
    import: "from kailash.api import WorkflowAPI"
    methods:
      create:
        signature: "create(workflow: Workflow, config: dict = None) -> WorkflowAPI"
        description: "Create API from workflow"
        example: |
          api = WorkflowAPI.create(workflow, {
              'endpoint': '/process',
              'method': 'POST',
              'input_schema': {...},
              'output_schema': {...}
          })

      run:
        signature: "run(host: str = '0.0.0.0', port: int = 8000)"
        description: "Start the API server"
        example: |
          api.run(host='localhost', port=8080)

# Node Registration
node_registry:
  register_node:
    description: "Decorator to register custom nodes"
    import: "from kailash.nodes import register_node"
    usage: |
      @register_node()
      class MyCustomNode(Node):
          def validate_config(self, config):
              # Validate configuration
              pass

          def execute(self, inputs):
              # Process inputs and return outputs
              return {'result': processed_data}

# Visualization
visualization:
  workflow_visualizer:
    class: kailash.WorkflowVisualizer
    description: "Visualize workflows"
    import: "from kailash import WorkflowVisualizer"
    methods:
      visualize:
        signature: "visualize(workflow: Workflow, output_path: str = None)"
        description: "Generate workflow visualization"
        example: |
          visualizer = WorkflowVisualizer()
          visualizer.visualize(workflow, 'workflow.png')

  mermaid_visualizer:
    class: kailash.workflow.MermaidVisualizer
    description: "Generate Mermaid diagrams"
    import: "from kailash.workflow import MermaidVisualizer"
    usage: |
      mermaid_code = MermaidVisualizer.generate(workflow)

# Tracking and Metrics
tracking:
  task_tracker:
    class: kailash.tracking.TaskTracker
    description: "Track workflow execution"
    import: "from kailash.tracking import TaskTracker"
    usage: |
      tracker = TaskTracker()
      # Automatically tracks when used with runtime

# Security
security:
  SecurityConfig:
    class: kailash.security.SecurityConfig
    description: "Configuration for security policies and limits"
    import: "from kailash.security import SecurityConfig"
    constructor:
      signature: "SecurityConfig(allowed_directories=None, max_file_size=100MB, execution_timeout=300.0, memory_limit=512MB, allowed_file_extensions=None, enable_audit_logging=True, enable_path_validation=True, enable_command_validation=True)"
      params:
        allowed_directories: "List of directories where file operations are permitted"
        max_file_size: "Maximum file size in bytes (default: 100MB)"
        execution_timeout: "Maximum execution time in seconds (default: 5 minutes)"
        memory_limit: "Maximum memory usage in bytes (default: 512MB)"
        allowed_file_extensions: "List of allowed file extensions"
        enable_audit_logging: "Whether to log security events"
        enable_path_validation: "Whether to validate file paths"
        enable_command_validation: "Whether to validate command strings"
    example: |
      from kailash.security import SecurityConfig, set_security_config
      config = SecurityConfig(
          allowed_directories=["/app/data", "/tmp/kailash"],
          max_file_size=50 * 1024 * 1024,
          execution_timeout=60.0,
          enable_audit_logging=True
      )
      set_security_config(config)

  security_functions:
    validate_file_path:
      signature: "validate_file_path(file_path: Union[str, Path], config: Optional[SecurityConfig] = None, operation: str = 'access') -> Path"
      description: "Validate and sanitize file paths to prevent traversal attacks"
      import: "from kailash.security import validate_file_path"
      example: |
        from kailash.security import validate_file_path
        safe_path = validate_file_path("/app/data/file.txt")

    safe_open:
      signature: "safe_open(file_path: Union[str, Path], mode: str = 'r', config: Optional[SecurityConfig] = None, **kwargs)"
      description: "Safely open a file with security validation"
      import: "from kailash.security import safe_open"
      example: |
        from kailash.security import safe_open
        with safe_open("data/file.txt", "r") as f:
            content = f.read()

    sanitize_input:
      signature: "sanitize_input(value: Any, max_length: int = 10000, allowed_types: Optional[List[type]] = None, config: Optional[SecurityConfig] = None) -> Any"
      description: "Sanitize input values to prevent injection attacks"
      import: "from kailash.security import sanitize_input"
      example: |
        from kailash.security import sanitize_input
        clean_input = sanitize_input(user_input)

    validate_command_string:
      signature: "validate_command_string(command: str, config: Optional[SecurityConfig] = None) -> str"
      description: "Validate command strings to prevent injection attacks"
      import: "from kailash.security import validate_command_string"
      example: |
        from kailash.security import validate_command_string
        safe_command = validate_command_string("python script.py")

  SecurityMixin:
    class: kailash.nodes.mixins.SecurityMixin
    description: "Mixin that adds security features to nodes"
    import: "from kailash.nodes.mixins import SecurityMixin"
    usage: |
      from kailash.nodes.mixins import SecurityMixin
      from kailash.nodes.base import Node

      class MySecureNode(SecurityMixin, Node):
          def run(self, **kwargs):
              safe_params = self.validate_and_sanitize_inputs(kwargs)
              return self.process_safely(safe_params)

# Access Control
access_control:
  UserContext:
    class: kailash.access_control.UserContext
    description: "User information for access control"
    import: "from kailash.access_control import UserContext"
    constructor:
      signature: "UserContext(user_id: str, tenant_id: str, email: str, roles: List[str] = None, permissions: List[str] = None)"
      params:
        user_id: "Unique user identifier"
        tenant_id: "Tenant identifier for multi-tenancy"
        email: "User email address"
        roles: "List of user roles"
        permissions: "List of specific permissions"
    example: |
      from kailash.access_control import UserContext
      user = UserContext(
          user_id="user_001",
          tenant_id="company_abc",
          email="user@company.com",
          roles=["analyst", "viewer"]
      )

  AccessControlledRuntime:
    class: kailash.runtime.access_controlled.AccessControlledRuntime
    description: "Runtime with access control enforcement"
    import: "from kailash.runtime.access_controlled import AccessControlledRuntime"
    constructor:
      signature: "AccessControlledRuntime(user_context: UserContext, **kwargs)"
      params:
        user_context: "User context for permission checks"
    methods:
      execute:
        signature: "execute(workflow: Workflow, parameters: Dict[str, Any] = None) -> Tuple[Dict[str, Any], str]"
        description: "Execute workflow with permission checks"
    example: |
      from kailash.runtime.access_controlled import AccessControlledRuntime
      runtime = AccessControlledRuntime(user_context=user)
      results, run_id = runtime.execute(workflow)

  PermissionRule:
    class: kailash.access_control.PermissionRule
    description: "Define access control rules"
    import: "from kailash.access_control import PermissionRule, PermissionEffect, NodePermission"
    constructor:
      signature: "PermissionRule(id: str, resource_type: str, resource_id: str, permission: Permission, effect: PermissionEffect, **kwargs)"
      params:
        id: "Rule identifier"
        resource_type: "Type of resource (workflow, node)"
        resource_id: "Resource identifier"
        permission: "Permission type"
        effect: "ALLOW or DENY"
    example: |
      from kailash.access_control import PermissionRule, PermissionEffect, NodePermission
      rule = PermissionRule(
          id="allow_read",
          resource_type="node",
          resource_id="csv_reader",
          permission=NodePermission.READ_OUTPUT,
          effect=PermissionEffect.ALLOW,
          roles=["analyst"]
      )

# API Gateway
api_gateway:
  WorkflowAPIGateway:
    class: kailash.api.gateway.WorkflowAPIGateway
    description: "Unified API gateway for multiple workflows"
    import: "from kailash.api.gateway import WorkflowAPIGateway"
    constructor:
      signature: "WorkflowAPIGateway(title: str = 'Kailash Gateway', description: str = '', version: str = '1.0.0')"
    methods:
      register_workflow:
        signature: "register_workflow(prefix: str, workflow: Workflow, **kwargs)"
        description: "Register a workflow with the gateway"
        example: |
          gateway.register_workflow("sales", sales_workflow)
      
      register_mcp_server:
        signature: "register_mcp_server(name: str, mcp_integration: MCPIntegration)"
        description: "Register an MCP server"
        example: |
          gateway.register_mcp_server("tools", mcp_integration)
      
      run:
        signature: "run(host: str = '0.0.0.0', port: int = 8000, **kwargs)"
        description: "Start the gateway server"
    example: |
      from kailash.api.gateway import WorkflowAPIGateway
      
      gateway = WorkflowAPIGateway(
          title="Enterprise Platform",
          description="Unified API for all workflows"
      )
      
      gateway.register_workflow("sales", sales_workflow)
      gateway.register_workflow("analytics", analytics_workflow)
      gateway.run(port=8000)

# MCP Integration
mcp_integration:
  MCPIntegration:
    class: kailash.api.mcp_integration.MCPIntegration
    description: "Model Context Protocol integration"
    import: "from kailash.api.mcp_integration import MCPIntegration"
    constructor:
      signature: "MCPIntegration(name: str)"
    methods:
      add_tool:
        signature: "add_tool(name: str, function: Callable, description: str = '')"
        description: "Add a tool to the MCP server"
      start:
        signature: "start(command: List[str])"
        description: "Start the MCP server"
    example: |
      from kailash.api.mcp_integration import MCPIntegration
      
      mcp = MCPIntegration("ai_tools")
      mcp.add_tool("analyze", analyze_function, "Analyze data")
      mcp.add_tool("predict", predict_function, "Make predictions")

  MCPToolNode:
    class: kailash.nodes.mcp.MCPToolNode
    description: "Execute MCP tools in workflows"
    import: "from kailash.nodes.mcp import MCPToolNode"
    config:
      mcp_server: "str - Name of MCP server"
      tool_name: "str - Name of tool to execute"
      parameters: "dict - Tool parameters"
    example: |
      workflow.add_node('mcp_tool', MCPToolNode(),
          mcp_server='ai_tools',
          tool_name='analyze',
          parameters={'method': 'regression'}
      )

# SharePoint Integration
sharepoint:
  SharePointGraphReader:
    class: kailash.nodes.data.sharepoint_graph.SharePointGraphReader
    description: "Read files from SharePoint using Microsoft Graph API"
    import: "from kailash.nodes.data import SharePointGraphReader"
    config:
      tenant_id: "str - Azure AD tenant ID"
      client_id: "str - Azure AD app client ID"
      client_secret: "str - Azure AD app client secret"
      site_url: "str - SharePoint site URL"
      operation: "str - Operation type (list_files, download_file, etc.)"
      library_name: "str - Document library name"
      file_path: "str - File path for download"
    outputs:
      files: "List[dict] - File information"
      content: "bytes - File content (for download)"
    example: |
      workflow.add_node('sharepoint', SharePointGraphReader(),
          tenant_id=os.getenv("SHAREPOINT_TENANT_ID"),
          client_id=os.getenv("SHAREPOINT_CLIENT_ID"),
          client_secret=os.getenv("SHAREPOINT_CLIENT_SECRET"),
          site_url="https://company.sharepoint.com/sites/YourSite",
          operation="list_files",
          library_name="Documents"
      )

  SharePointGraphWriter:
    class: kailash.nodes.data.sharepoint_graph.SharePointGraphWriter
    description: "Upload files to SharePoint using Microsoft Graph API"
    import: "from kailash.nodes.data import SharePointGraphWriter"
    config:
      tenant_id: "str - Azure AD tenant ID"
      client_id: "str - Azure AD app client ID"
      client_secret: "str - Azure AD app client secret"
      site_url: "str - SharePoint site URL"
      library_name: "str - Document library name"
      file_path: "str - Destination file path"
    inputs:
      content: "bytes - File content to upload"
    example: |
      workflow.add_node('upload', SharePointGraphWriter(),
          tenant_id=os.getenv("SHAREPOINT_TENANT_ID"),
          client_id=os.getenv("SHAREPOINT_CLIENT_ID"),
          client_secret=os.getenv("SHAREPOINT_CLIENT_SECRET"),
          site_url="https://company.sharepoint.com/sites/YourSite",
          library_name="Documents",
          file_path="reports/output.pdf"
      )

# RAG (Retrieval-Augmented Generation) Nodes
rag_nodes:
  DocumentSourceNode:
    class: kailash.nodes.data.sources.DocumentSourceNode
    description: "Provide sample documents for RAG workflows"
    import: "from kailash.nodes.data.sources import DocumentSourceNode"
    outputs:
      documents: "List[dict] - Sample documents with title and content"
    example: |
      workflow.add_node('doc_source', DocumentSourceNode())

  QuerySourceNode:
    class: kailash.nodes.data.sources.QuerySourceNode
    description: "Provide sample queries for RAG workflows"
    import: "from kailash.nodes.data.sources import QuerySourceNode"
    outputs:
      query: "str - Sample query text"
    example: |
      workflow.add_node('query_source', QuerySourceNode())

  HierarchicalChunkerNode:
    class: kailash.nodes.transform.chunkers.HierarchicalChunkerNode
    description: "Split documents into hierarchical chunks"
    import: "from kailash.nodes.transform.chunkers import HierarchicalChunkerNode"
    config:
      chunk_size: "int - Maximum chunk size (default: 1000)"
      chunk_overlap: "int - Overlap between chunks (default: 200)"
      level_separators: "List[str] - Separators for hierarchy levels"
    inputs:
      documents: "List[dict] - Documents to chunk"
    outputs:
      chunks: "List[dict] - Document chunks with metadata"
    example: |
      workflow.add_node('chunker', HierarchicalChunkerNode(),
          chunk_size=1000,
          chunk_overlap=200
      )

  ChunkTextExtractorNode:
    class: kailash.nodes.transform.formatters.ChunkTextExtractorNode
    description: "Extract text from chunks for embedding"
    import: "from kailash.nodes.transform.formatters import ChunkTextExtractorNode"
    inputs:
      chunks: "List[dict] - Document chunks"
    outputs:
      input_texts: "List[str] - Extracted text for embedding"
    example: |
      workflow.add_node('text_extractor', ChunkTextExtractorNode())

  QueryTextWrapperNode:
    class: kailash.nodes.transform.formatters.QueryTextWrapperNode
    description: "Wrap query text for embedding"
    import: "from kailash.nodes.transform.formatters import QueryTextWrapperNode"
    inputs:
      query: "str - Query text"
    outputs:
      input_texts: "List[str] - Wrapped query for embedding"
    example: |
      workflow.add_node('query_wrapper', QueryTextWrapperNode())

  RelevanceScorerNode:
    class: kailash.nodes.data.retrieval.RelevanceScorerNode
    description: "Score chunks by relevance to query"
    import: "from kailash.nodes.data.retrieval import RelevanceScorerNode"
    config:
      similarity_method: "str - Method (cosine, euclidean, dot_product)"
      top_k: "int - Number of chunks to return (default: 5)"
    inputs:
      chunks: "List[dict] - Document chunks"
      query_embedding: "List[float] - Query embedding"
      chunk_embeddings: "List[List[float]] - Chunk embeddings"
    outputs:
      relevant_chunks: "List[dict] - Top relevant chunks with scores"
    example: |
      workflow.add_node('scorer', RelevanceScorerNode(),
          similarity_method='cosine',
          top_k=5
      )

  ContextFormatterNode:
    class: kailash.nodes.transform.formatters.ContextFormatterNode
    description: "Format context for LLM input"
    import: "from kailash.nodes.transform.formatters import ContextFormatterNode"
    inputs:
      relevant_chunks: "List[dict] - Relevant chunks"
      query: "str - Original query"
    outputs:
      messages: "List[dict] - Formatted messages for LLM"
    example: |
      workflow.add_node('formatter', ContextFormatterNode())

# Workflow Studio API
workflow_studio:
  WorkflowStudioAPI:
    class: kailash.api.studio.WorkflowStudioAPI
    description: "Backend API for Workflow Studio UI"
    import: "from kailash.api.studio import WorkflowStudioAPI"
    constructor:
      signature: "WorkflowStudioAPI(database_url: str = 'sqlite:///workflow_studio.db')"
    methods:
      run:
        signature: "run(host: str = '0.0.0.0', port: int = 8001)"
        description: "Start the Studio API server"
    endpoints:
      - "GET /api/nodes - List available node types"
      - "GET /api/workflows - List saved workflows"
      - "POST /api/workflows - Create new workflow"
      - "PUT /api/workflows/{id} - Update workflow"
      - "DELETE /api/workflows/{id} - Delete workflow"
      - "POST /api/workflows/{id}/execute - Execute workflow"
      - "WebSocket /ws/execution/{run_id} - Real-time execution updates"
    example: |
      from kailash.api.studio import WorkflowStudioAPI
      
      studio_api = WorkflowStudioAPI(
          database_url="postgresql://user:pass@localhost/studio"
      )
      studio_api.run(port=8001)

# Utilities
utils:
  export:
    description: "Export workflows to various formats"
    import: "from kailash.utils.export import export_workflow"
    formats:
      - yaml: "YAML format for Kailash"
      - json: "JSON format"
      - docker: "Docker Compose format"
      - kubernetes: "Kubernetes manifests"
    example: |
      from kailash.utils.export import export_workflow
      export_workflow(workflow, 'output.yaml', format='yaml')
