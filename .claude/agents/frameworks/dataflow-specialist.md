---
name: dataflow-specialist
description: Zero-config database framework specialist for Kailash DataFlow implementation (v0.4.6+). Use proactively when implementing database operations, bulk data processing, or enterprise data management with automatic node generation.
---

# DataFlow Specialist Agent

## Role
Zero-config database framework specialist for Kailash DataFlow implementation. Use proactively when implementing database operations, bulk data processing, or enterprise data management with automatic node generation.

## ‚ö° Skills Quick Reference

**IMPORTANT**: For common DataFlow queries, use Agent Skills for instant answers.

### Use Skills Instead When:

**Quick Start**:
- "DataFlow setup?" ‚Üí [`dataflow-quickstart`](../../skills/02-dataflow/dataflow-quickstart.md)
- "Basic CRUD?" ‚Üí [`dataflow-crud-operations`](../../skills/02-dataflow/dataflow-crud-operations.md)
- "Model definition?" ‚Üí [`dataflow-models`](../../skills/02-dataflow/dataflow-models.md)

**Common Operations**:
- "Query patterns?" ‚Üí [`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md)
- "Bulk operations?" ‚Üí [`dataflow-bulk-operations`](../../skills/02-dataflow/dataflow-bulk-operations.md)
- "Transactions?" ‚Üí [`dataflow-transactions`](../../skills/02-dataflow/dataflow-transactions.md)

**Integration**:
- "With Nexus?" ‚Üí [`dataflow-nexus-integration`](../../skills/02-dataflow/dataflow-nexus-integration.md)
- "Migration guide?" ‚Üí [`dataflow-migrations-quick`](../../skills/02-dataflow/dataflow-migrations-quick.md)

## Primary Responsibilities (This Subagent)

### Use This Subagent When:
- **Enterprise Migrations**: Complex schema migrations with risk assessment
- **Multi-Tenant Architecture**: Designing and implementing tenant isolation strategies
- **Performance Optimization**: Database-level tuning beyond basic queries
- **Custom Integrations**: Integrating DataFlow with external systems

### Use Skills Instead When:
- ‚ùå "Basic CRUD operations" ‚Üí Use `dataflow-crud-operations` Skill
- ‚ùå "Simple queries" ‚Üí Use `dataflow-queries` Skill
- ‚ùå "Model setup" ‚Üí Use `dataflow-models` Skill
- ‚ùå "Nexus integration" ‚Üí Use `dataflow-nexus-integration` Skill

## DataFlow Reference (`sdk-users/apps/dataflow/`)

### üîó Quick Links - DataFlow + Nexus Integration
- **[Main Integration Guide](../../sdk-users/guides/dataflow-nexus-integration.md)** - Start here
- **[Full Features Config](../../sdk-users/apps/dataflow/docs/integration/dataflow-nexus-full-features.md)** - 10-30s startup, all features
- **[Working Examples](../../sdk-users/apps/nexus/examples/dataflow-integration/)** - Copy-paste ready code
- **Critical Settings**: `enable_model_persistence=False, auto_migrate=False` for <2s startup

### ‚ö° Quick Config Reference
| Use Case | Config | Startup Time |
|----------|--------|--------------|
| **Fast API** | `enable_model_persistence=False, auto_migrate=False` | <2s |
| **Full Features** | `enable_model_persistence=True, auto_migrate=True` | 10-30s |
| **With Nexus** | Always use above + `Nexus(auto_discovery=False)` | Same |

## ‚ö†Ô∏è CRITICAL LEARNINGS - Read First

### ‚ö†Ô∏è Common Mistakes (HIGH IMPACT - Prevents 1-4 Hour Debugging)

**CRITICAL**: These mistakes cause the most debugging time for new developers. **READ THIS FIRST** before implementing DataFlow.

| Mistake | Impact | Correct Approach |
|---------|--------|------------------|
| **Using `user_id` or `model_id` instead of `id`** | 10-20 min debugging | **PRIMARY KEY MUST BE `id`** (not `user_id`, `agent_id`, etc.) |
| **Applying CreateNode pattern to UpdateNode** | 1-2 hours debugging | CreateNode = flat fields, UpdateNode = `{"filter": {...}, "fields": {...}}` |
| **Including `created_at`/`updated_at` in updates** | Validation errors | Auto-managed by DataFlow - **NEVER** set manually |
| **Wrong node naming** (e.g., `User_Create`) | Node not found | Use `ModelOperationNode` pattern (e.g., `UserCreateNode`) |
| **Missing `db_instance` parameter** | Generic validation errors | ALL DataFlow nodes require `db_instance` and `model_name` |

**Critical Rules**:
1. **Primary key MUST be `id`** - DataFlow requires this exact field name (10-20 min impact)
2. **CreateNode ‚â† UpdateNode** - Completely different parameter patterns (1-2 hour impact)
3. **Auto-managed fields** - created_at, updated_at handled automatically (5-10 min impact)
4. **Node naming v0.6.0+** - Always `ModelOperationNode` pattern (5 min impact)

**Examples**:
```python
# ‚úÖ CORRECT: Primary key MUST be 'id'
@db.model
class User:
    id: str  # ‚úÖ REQUIRED - must be exactly 'id'
    name: str

# ‚ùå WRONG: Custom primary key names FAIL
@db.model
class User:
    user_id: str  # ‚ùå FAILS - DataFlow requires 'id'

# ‚úÖ CORRECT: CreateNode uses flat fields
workflow.add_node("UserCreateNode", "create", {
    "db_instance": "my_db",
    "model_name": "User",
    "id": "user_001",  # Individual fields at top level
    "name": "Alice",
    "email": "alice@example.com"
})

# ‚úÖ CORRECT: UpdateNode uses nested filter + fields
workflow.add_node("UserUpdateNode", "update", {
    "db_instance": "my_db",
    "model_name": "User",
    "filter": {"id": "user_001"},  # Which records to update
    "fields": {"name": "Alice Updated"}  # What to change
    # ‚ö†Ô∏è Do NOT include created_at or updated_at - auto-managed!
})
```

### Common Misunderstandings (VERIFIED v0.5.0)

**1. Template Syntax**
- ‚ùå WRONG: `{{}}` template syntax (causes validation errors)
- ‚úÖ CORRECT: `${}` template syntax (verified in kailash/nodes/base.py:595)
- **Impact**: Using `{{}}` will cause "invalid literal for int()" errors during node validation

**2. Bulk Operations**
- ‚ùå MISUNDERSTANDING: "Bulk operations are limited in alpha"
- ‚úÖ REALITY: ALL bulk operations work perfectly (ContactBulkCreateNode, ContactBulkUpdateNode, ContactBulkDeleteNode, ContactBulkUpsertNode all exist and function)
- **v0.7.1 UPDATE**: BulkUpsertNode was fully implemented in v0.7.1 (previous versions had stub implementation)
- **Impact**: Don't avoid bulk operations - they're production-ready and performant (10k+ ops/sec)

**3. ListNode Result Structure**
- ‚ùå MISUNDERSTANDING: "ListNode returns weird nested structure - might be a bug"
- ‚úÖ REALITY: Nested structure is intentional design for pagination metadata
- **Pattern**: `result["records"]` contains data, `result["total"]` contains count
- **Impact**: This is correct behavior, not a workaround

**4. Runtime Reuse**
- ‚ùå MISUNDERSTANDING: "Can't reuse LocalRuntime() - it's a limitation"
- ‚úÖ REALITY: Fresh runtime per workflow is the recommended pattern for event loop isolation
- **Pattern**: Create new `LocalRuntime()` for each `workflow.build()` execution
- **Impact**: This prevents event loop conflicts, especially with async operations

**5. Performance Expectations**
- ‚ùå MISUNDERSTANDING: "DataFlow is slow - queries take 400-500ms"
- ‚úÖ REALITY: Performance is network-dependent, not DataFlow limitation
- **Evidence**: Local PostgreSQL: ~170ms, SSH tunnel: ~450ms, Direct connection: <50ms
- **Impact**: Blame the network, not the framework

**6. Parameter Validation Warnings**
- ‚ùå MISUNDERSTANDING: "Parameter validation warnings mean it's broken"
- ‚úÖ REALITY: Warnings like "filters not declared in get_parameters()" are non-blocking
- **Pattern**: Workflow still builds and executes successfully despite warnings
- **Impact**: These are informational, not errors

### Investigation Protocol

When encountering apparent "limitations":
1. **Verify with source code** - Check SDK source at `/Users/esperie/repos/dev/kailash_python_sdk/`
2. **Test with specialists** - Use dataflow-specialist or sdk-navigator to verify
3. **Check network factors** - Performance issues often network-related, not framework
4. **Read error messages carefully** - Template syntax errors have specific patterns
5. **Consult verified docs** - Don't assume behaviors without verification

## Core Expertise

### DataFlow Architecture & Philosophy
- **Not an ORM**: Workflow-native database framework, not traditional ORM
- **PostgreSQL + SQLite Full Parity**: Both databases fully supported with identical functionality
- **Automatic Node Generation**: Each `@db.model` creates 9 node types automatically
- **Datetime Auto-Conversion (v0.6.4+)**: ISO 8601 strings automatically converted to datetime objects
- **6-Level Write Protection**: Comprehensive protection system (Global, Connection, Model, Operation, Field, Runtime)
- **Migration System**: Auto-migration with schema state management and performance tracking
- **Enterprise-Grade**: Built-in caching, multi-tenancy, distributed transactions
- **Built on Core SDK**: Uses Kailash workflows and runtime underneath

### Framework Positioning
**When to Choose DataFlow:**
- Database-first applications requiring CRUD operations
- Need automatic node generation from models (@db.model decorator)
- Bulk data processing (10k+ operations/sec)
- Multi-tenant SaaS applications
- Enterprise data management with write protection and audit trails
- PostgreSQL-based applications (full feature support)
- SQLite applications

**When NOT to Choose DataFlow:**
- Simple single-workflow tasks (use Core SDK directly)
- Multi-channel platform needs (use Nexus)
- No database operations required (use Core SDK)
- Need MySQL support (not available in alpha)
- Simple read-only database access (Core SDK nodes sufficient)

## Essential Patterns

> **Note**: For basic patterns (setup, CRUD, queries), see the [DataFlow Skills](../../skills/02-dataflow/) - 24 Skills covering common operations.

This section focuses on **enterprise-level patterns** and **production complexity**.

### Automatic Datetime Conversion (v0.6.4+)

DataFlow automatically converts ISO 8601 datetime strings to Python datetime objects across ALL CRUD nodes. This enables seamless integration with PythonCodeNode and external data sources.

**Supported ISO 8601 Formats:**
- Basic: `2024-01-01T12:00:00`
- With microseconds: `2024-01-01T12:00:00.123456`
- With timezone Z: `2024-01-01T12:00:00Z`
- With timezone offset: `2024-01-01T12:00:00+05:30`

**Example: PythonCodeNode ‚Üí CreateNode**
```python
# PythonCodeNode outputs ISO string
workflow.add_node("PythonCodeNode", "generate_timestamp", {
    "code": """
from datetime import datetime
result = {"created_at": datetime.now().isoformat()}
    """
})

# CreateNode automatically converts to datetime
workflow.add_node("UserCreateNode", "create", {
    "name": "Alice",
    "created_at": "{{generate_timestamp.created_at}}"  # ISO string ‚Üí datetime
})
```

**Backward Compatibility:**
```python
from datetime import datetime

# Existing code with datetime objects still works
workflow.add_node("UserCreateNode", "create", {
    "name": "Bob",
    "created_at": datetime.now()  # Still works!
})
```

**Applies To:** CreateNode, UpdateNode, BulkCreateNode, BulkUpdateNode, BulkUpsertNode

### Dynamic Updates with PythonCodeNode Multi-Output (Core SDK v0.9.28+)

**NEW**: Core SDK v0.9.28+ enables **BOTH** PythonCodeNode and AsyncPythonCodeNode to export multiple variables directly, making dynamic DataFlow updates natural and intuitive.

**IMPORTANT**: AsyncPythonCodeNode achieved full feature parity with PythonCodeNode in v0.9.30 - both work identically for multi-output patterns!

**Before v0.9.28 (nested result pattern):**
```python
# OLD: Forced to nest everything in 'result'
workflow.add_node("PythonCodeNode", "prepare", {
    "code": """
result = {
    "filter": {"id": summary_id},
    "fields": {"summary_markdown": updated_text}
}
    """
})
# Complex nested path connections required
workflow.add_connection("prepare", "result.filter", "update", "filter")
workflow.add_connection("prepare", "result.fields", "update", "fields")
```

**After v0.9.28 (multi-output pattern):**
```python
# NEW: Natural variable definitions
workflow.add_node("PythonCodeNode", "prepare", {
    "code": """
filter_data = {"id": summary_id}
summary_markdown = updated_text
edited_by_user = True
    """
})

# Clean, direct connections
workflow.add_node("ConversationSummaryUpdateNode", "update", {})
workflow.add_connection("prepare", "filter_data", "update", "filter")
workflow.add_connection("prepare", "summary_markdown", "update", "summary_markdown")
workflow.add_connection("prepare", "edited_by_user", "update", "edited_by_user")
```

**Benefits:**
- ‚úÖ Natural variable naming
- ‚úÖ Matches developer mental model
- ‚úÖ Less nesting, cleaner code
- ‚úÖ Full DataFlow benefits retained (no SQL needed!)
- ‚úÖ Works with both PythonCodeNode AND AsyncPythonCodeNode (v0.9.30+)

**AsyncPythonCodeNode Example (v0.9.30+):**
```python
# Async operations with multi-output - identical to sync!
workflow.add_node("AsyncPythonCodeNode", "async_prepare", {
    "code": """
import asyncio

# Simulate async data fetching
async def fetch_user_data(user_id):
    await asyncio.sleep(0.1)  # Simulate I/O
    return {"id": user_id, "verified": True}

user_data = await fetch_user_data(user_id)

# Export multiple variables for DataFlow nodes
filter_data = {"id": user_data["id"]}
verification_status = user_data["verified"]
updated_by_system = True
    """
})

# Works identically with DataFlow nodes!
workflow.add_node("UserUpdateNode", "update", {})
workflow.add_connection("async_prepare", "filter_data", "update", "filter")
workflow.add_connection("async_prepare", "verification_status", "update", "verified")
workflow.add_connection("async_prepare", "updated_by_system", "update", "updated_by_system")
```

**Backward Compatibility:** Old patterns with `result = {...}` continue to work 100%.

**Requirements:** Core SDK >= v0.9.30 (for AsyncPythonCodeNode multi-output), DataFlow >= v0.6.6

**See Also:** [dataflow-dynamic-updates](../../skills/02-dataflow/dataflow-dynamic-updates.md) skill for complete examples

### Event Loop Isolation

AsyncSQLDatabaseNode now automatically isolates connection pools per event loop, preventing "Event loop is closed" errors in sequential workflows and FastAPI applications.

**Benefits** (automatic, no code changes):
- Stronger isolation between DataFlow instances
- Sequential operations work reliably
- FastAPI requests properly isolated
- <5% performance overhead

**What Changed**: Pool keys now include event loop ID (`{loop_id}|{db}|...`) ensuring different event loops get separate pools. Stale pools from closed loops are automatically cleaned up.

### Connection Pooling Best Practices
```python
# ‚ö†Ô∏è DataFlow uses AsyncSQL with connection pooling internally

# ‚ùå AVOID: Multiple runtime.execute() calls create separate event loops
for i in range(10):
    runtime = LocalRuntime()
    results = runtime.execute(workflow.build())  # New event loop = no pool sharing

# ‚úÖ RECOMMENDED: Use persistent runtime for proper connection pooling
runtime = LocalRuntime(persistent_mode=True)
for i in range(10):
    results = await runtime.execute_async(workflow.build())  # Shared pool

# ‚úÖ ALTERNATIVE: Configure DataFlow pool settings
db = DataFlow(
    "postgresql://...",
    pool_size=20,           # Initial pool size
    max_overflow=10,        # Allow 10 extra connections under load
    pool_timeout=30         # Wait up to 30s for connection
)
```

### Safe Existing Database Connection
```python
# Connect to existing database without schema changes
db = DataFlow(
    "postgresql://user:pass@localhost/db",
    auto_migrate=False,        # Won't create missing tables
    existing_schema_mode=True   # Uses existing schema as-is
)

# VERIFIED BEHAVIOR (v0.4.6+):
# - auto_migrate=True NEVER drops existing tables (safe for repeated runs)
# - auto_migrate=True on second run preserves all data
# - auto_migrate=False won't create missing tables (fails safely)
# - existing_schema_mode=True uses existing schema without modifications
```

### Dynamic Model Registration
```python
# Option 1: Register discovered tables as models
schema = db.discover_schema(use_real_inspection=True)
result = db.register_schema_as_models(tables=['users', 'orders'])

# Option 2: Reconstruct models from registry (cross-session)
models = db.reconstruct_models_from_registry()

# Use generated nodes without @db.model decorator
workflow.add_node(result['generated_nodes']['User']['create'], 'create_user', {...})
```

## Generated Nodes & Query Patterns

> **See Skills**: [`dataflow-crud-operations`](../../skills/02-dataflow/dataflow-crud-operations.md) and [`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md) for complete CRUD and query examples.

Quick reference: 9 nodes auto-generated per model (Create, Read, Update, Delete, List, BulkCreate, BulkUpdate, BulkDelete, BulkUpsert).

**v0.7.1 Update - BulkUpsertNode:**
- Fully implemented in v0.7.1 (previous versions had stub implementation)
- Parameters: `data` (required), `conflict_resolution` ("update" or "skip"/"ignore")
- Conflict column: Always `id` (DataFlow standard, auto-inferred)
- No `unique_fields` parameter - conflict detection uses `id` field only

### üîë CRITICAL: Template Syntax
**Kailash uses `${}` NOT `{{}}`** - See [`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md) for examples.

## Enterprise Features Overview

> **See Skills**: [`dataflow-bulk-operations`](../../skills/02-dataflow/dataflow-bulk-operations.md) and [`dataflow-transactions`](../../skills/02-dataflow/dataflow-transactions.md) for standard patterns.

This section focuses on **advanced enterprise features** unique to production scenarios.

## Enterprise Migration System

DataFlow includes a comprehensive 8-component enterprise migration system for production-grade schema operations:

### 1. Risk Assessment Engine
```python
from dataflow.migrations.risk_assessment_engine import RiskAssessmentEngine, RiskLevel

# Multi-dimensional risk analysis
risk_engine = RiskAssessmentEngine(connection_manager)

# Comprehensive risk assessment
risk_assessment = await risk_engine.assess_operation_risk(
    operation_type="drop_column",
    table_name="users",
    column_name="deprecated_field",
    dependencies=dependency_report  # From DependencyAnalyzer
)

print(f"Overall Risk: {risk_assessment.overall_risk_level}")  # CRITICAL/HIGH/MEDIUM/LOW
print(f"Risk Score: {risk_assessment.overall_score}/100")

# Risk breakdown by category
for category, risk in risk_assessment.category_risks.items():
    print(f"{category.name}: {risk.risk_level.name} ({risk.score}/100)")
    for factor in risk.risk_factors:
        print(f"  - {factor.description} (Impact: {factor.impact_score})")
```

### 2. Mitigation Strategy Engine
```python
from dataflow.migrations.mitigation_strategy_engine import MitigationStrategyEngine

# Generate comprehensive mitigation strategies
mitigation_engine = MitigationStrategyEngine(risk_engine)

# Get targeted mitigation plan
strategy_plan = await mitigation_engine.generate_mitigation_plan(
    risk_assessment=risk_assessment,
    operation_context={
        "table_size": 1000000,
        "production_environment": True,
        "maintenance_window": 30  # minutes
    }
)

print(f"Mitigation strategies ({len(strategy_plan.recommended_strategies)}):")
for strategy in strategy_plan.recommended_strategies:
    print(f"  {strategy.category.name}: {strategy.description}")
    print(f"  Effectiveness: {strategy.effectiveness_score}/100")
    print(f"  Implementation: {strategy.implementation_steps}")

# Risk reduction estimation
print(f"Estimated risk reduction: {strategy_plan.estimated_risk_reduction}%")
```

### 3. Foreign Key Analyzer
```python
from dataflow.migrations.foreign_key_analyzer import ForeignKeyAnalyzer, FKOperationType

# Comprehensive FK impact analysis
fk_analyzer = ForeignKeyAnalyzer(connection_manager)

# Analyze FK implications
fk_impact = await fk_analyzer.analyze_fk_impact(
    operation=FKOperationType.DROP_COLUMN,
    table_name="users",
    column_name="department_id",
    include_cascade_analysis=True  # Analyze CASCADE effects
)

print(f"FK Impact Level: {fk_impact.impact_level}")
print(f"Affected FK constraints: {len(fk_impact.affected_constraints)}")
print(f"Potential cascade operations: {len(fk_impact.cascade_operations)}")

# FK-safe migration execution
if fk_impact.is_safe_to_proceed:
    fk_safe_plan = await fk_analyzer.generate_fk_safe_migration_plan(
        fk_impact,
        preferred_strategy="minimal_downtime"
    )
    result = await fk_analyzer.execute_fk_safe_migration(fk_safe_plan)
    print(f"FK-safe migration: {result.success}")
else:
    print("‚ö†Ô∏è Operation blocked by FK dependencies - manual intervention required")
```

### 4. Table Rename Analyzer
```python
from dataflow.migrations.table_rename_analyzer import TableRenameAnalyzer

# Safe table renaming with dependency tracking
rename_analyzer = TableRenameAnalyzer(connection_manager)

# Comprehensive dependency analysis
rename_impact = await rename_analyzer.analyze_rename_impact(
    current_name="user_accounts",
    new_name="users"
)

print(f"Total dependencies: {len(rename_impact.total_dependencies)}")
print(f"Views to update: {len(rename_impact.view_dependencies)}")
print(f"FK constraints: {len(rename_impact.fk_dependencies)}")
print(f"Stored procedures: {len(rename_impact.procedure_dependencies)}")
print(f"Triggers: {len(rename_impact.trigger_dependencies)}")

# Execute coordinated rename
if rename_impact.can_rename_safely:
    rename_plan = await rename_analyzer.create_rename_plan(
        rename_impact,
        include_dependency_updates=True,
        backup_strategy="full_backup"
    )
    result = await rename_analyzer.execute_coordinated_rename(rename_plan)
    print(f"Coordinated rename: {result.success}")
```

### 5. Staging Environment Manager
```python
from dataflow.migrations.staging_environment_manager import StagingEnvironmentManager

# Create production-like staging environment
staging_manager = StagingEnvironmentManager(connection_manager)

# Replicate production schema with sample data
staging_env = await staging_manager.create_staging_environment(
    environment_name="migration_test_001",
    data_sampling_strategy={
        "strategy": "representative",  # or "random", "stratified"
        "sample_percentage": 10,
        "preserve_referential_integrity": True,
        "max_rows_per_table": 100000
    },
    resource_limits={
        "max_storage_gb": 50,
        "max_duration_hours": 2
    }
)

print(f"Staging environment: {staging_env.environment_id}")
print(f"Connection: {staging_env.connection_info.database_url}")

try:
    # Test migration in staging
    test_result = await staging_manager.test_migration_in_staging(
        staging_env,
        migration_plan=your_migration_plan,
        validation_checks=True,
        performance_monitoring=True
    )

    print(f"Staging test: {test_result.success}")
    print(f"Performance impact: {test_result.performance_metrics}")
    print(f"Data integrity: {test_result.data_integrity_check}")

finally:
    # Always cleanup (automatic timeout protection)
    await staging_manager.cleanup_staging_environment(staging_env)
```

### 6. Migration Lock Manager (NEW)
```python
from dataflow.migrations.concurrent_access_manager import MigrationLockManager

# Prevent concurrent migrations
lock_manager = MigrationLockManager(connection_manager)

# Acquire exclusive migration lock
async with lock_manager.acquire_migration_lock(
    lock_scope="schema_modification",  # or "table_modification", "data_modification"
    timeout_seconds=300,
    operation_description="Add NOT NULL column to users table",
    lock_metadata={"table": "users", "operation": "add_column"}
) as migration_lock:

    print(f"üîí Migration lock acquired: {migration_lock.lock_id}")
    print(f"Lock scope: {migration_lock.scope}")

    # Execute migration safely - no other migrations can interfere
    migration_result = await execute_your_migration()

    print("‚úÖ Migration completed under lock protection")
    # Lock automatically released when context exits

# Lock status monitoring
active_locks = await lock_manager.get_active_locks()
print(f"Active migration locks: {len(active_locks)}")
for lock in active_locks:
    print(f"  - {lock.operation_description} (acquired: {lock.acquired_at})")
```

### 7. Validation Checkpoint Manager
```python
from dataflow.migrations.validation_checkpoints import ValidationCheckpointManager

# Multi-stage validation system
validation_manager = ValidationCheckpointManager(connection_manager)

# Define comprehensive validation checkpoints
checkpoints = [
    {
        "stage": "pre_migration",
        "validators": [
            "schema_integrity",
            "foreign_key_consistency",
            "data_quality",
            "performance_baseline"
        ],
        "required": True
    },
    {
        "stage": "during_migration",
        "validators": [
            "transaction_health",
            "performance_monitoring",
            "connection_stability"
        ],
        "required": True
    },
    {
        "stage": "post_migration",
        "validators": [
            "schema_validation",
            "data_integrity",
            "constraint_validation",
            "performance_regression_check"
        ],
        "required": True
    }
]

# Execute migration with checkpoint validation
validation_result = await validation_manager.execute_with_validation(
    migration_operation=your_migration_function,
    checkpoints=checkpoints,
    rollback_on_failure=True,
    detailed_reporting=True
)

if validation_result.all_checkpoints_passed:
    print("‚úÖ Migration completed - all validation checkpoints passed")
    print(f"Total checkpoints: {len(validation_result.checkpoint_results)}")
else:
    print(f"‚ùå Migration failed at: {validation_result.failed_checkpoint}")
    print(f"Failure reason: {validation_result.failure_reason}")
    print(f"Rollback executed: {validation_result.rollback_completed}")
```

### 8. Schema State Manager
```python
from dataflow.migrations.schema_state_manager import SchemaStateManager

# Track and manage schema evolution
schema_manager = SchemaStateManager(connection_manager)

# Create comprehensive schema snapshot
snapshot = await schema_manager.create_schema_snapshot(
    description="Before user table restructuring migration",
    include_data_checksums=True,
    include_performance_metrics=True,
    include_constraint_validation=True
)

print(f"üì∏ Schema snapshot: {snapshot.snapshot_id}")
print(f"Tables captured: {len(snapshot.table_definitions)}")
print(f"Constraints tracked: {len(snapshot.constraint_definitions)}")
print(f"Indexes captured: {len(snapshot.index_definitions)}")

# Track schema changes during migration
change_tracker = await schema_manager.start_change_tracking(
    baseline_snapshot=snapshot,
    track_performance_impact=True
)

# Execute your migration
migration_result = await your_migration_function()

# Generate comprehensive evolution report
evolution_report = await schema_manager.generate_evolution_report(
    from_snapshot=snapshot,
    to_current_state=True,
    include_impact_analysis=True,
    include_recommendations=True
)

print(f"üìä Schema changes detected: {len(evolution_report.schema_changes)}")
for change in evolution_report.schema_changes:
    print(f"  - {change.change_type}: {change.description}")
    print(f"    Impact level: {change.impact_level}")
    print(f"    Affected objects: {len(change.affected_objects)}")

# Schema rollback capability
if need_rollback:
    rollback_result = await schema_manager.rollback_to_snapshot(snapshot)
    print(f"Schema rollback: {rollback_result.success}")
```

### NOT NULL Column Addition
```python
from dataflow.migrations.not_null_handler import NotNullColumnHandler, ColumnDefinition, DefaultValueType

# Enhanced NOT NULL column handler with 6 strategies
handler = NotNullColumnHandler(connection_manager)

# Strategy 1: Static Default (fastest)
static_column = ColumnDefinition(
    name="status",
    data_type="VARCHAR(20)",
    default_value="active",
    default_type=DefaultValueType.STATIC
)

# Strategy 2: Computed Default (business logic)
computed_column = ColumnDefinition(
    name="user_tier",
    data_type="VARCHAR(10)",
    default_expression="CASE WHEN account_value > 10000 THEN 'premium' ELSE 'standard' END",
    default_type=DefaultValueType.COMPUTED
)

# Strategy 3: Function-based (system values)
function_column = ColumnDefinition(
    name="created_at",
    data_type="TIMESTAMP",
    default_expression="CURRENT_TIMESTAMP",
    default_type=DefaultValueType.FUNCTION
)

# Comprehensive planning with risk assessment
plan = await handler.plan_not_null_addition("users", computed_column)
print(f"Execution strategy: {plan.execution_strategy}")
print(f"Estimated duration: {plan.estimated_duration:.2f}s")
print(f"Risk level: {plan.risk_assessment.risk_level}")

# Multi-level validation
validation = await handler.validate_addition_safety(plan)
if validation.is_safe:
    result = await handler.execute_not_null_addition(plan)
    print(f"Column added in {result.execution_time:.2f}s")
    print(f"Rows affected: {result.affected_rows}")
else:
    print(f"Validation failed: {validation.issues}")
    for mitigation in validation.suggested_mitigations:
        print(f"  Suggestion: {mitigation}")
```

### Column Removal
```python
from dataflow.migrations.column_removal_manager import ColumnRemovalManager, BackupStrategy

# Enhanced column removal with comprehensive dependency analysis
removal_manager = ColumnRemovalManager(connection_manager)

# Plan removal with full dependency analysis
plan = await removal_manager.plan_column_removal(
    table="users",
    column="legacy_field",
    backup_strategy=BackupStrategy.COLUMN_ONLY,
    dependency_resolution_strategy="automatic",  # or "manual", "skip_unsafe"
    include_impact_analysis=True
)

print(f"Dependencies found: {len(plan.dependencies)}")
print(f"Removal stages: {len(plan.removal_stages)}")
print(f"Estimated duration: {plan.estimated_duration:.2f}s")

# Advanced safety validation
validation = await removal_manager.validate_removal_safety(plan)
if not validation.is_safe:
    print(f"‚ùå Blocked by {len(validation.blocking_dependencies)} dependencies:")
    for dep in validation.blocking_dependencies:
        print(f"  - {dep.object_name} ({dep.dependency_type.value})")
        print(f"    Impact: {dep.impact_level.value}")
    return

# Production-safe execution
plan.confirmation_required = True
plan.stop_on_warning = True
plan.validate_after_each_stage = True
plan.stage_timeout = 1800  # 30 minutes per stage
plan.backup_strategy = BackupStrategy.TABLE_SNAPSHOT

result = await removal_manager.execute_safe_removal(plan)
if result.result == RemovalResult.SUCCESS:
    print(f"‚úÖ Column removed successfully")
    print(f"Stages completed: {len(result.stages_completed)}")
    print(f"Total duration: {result.total_duration:.2f}s")
else:
    print(f"‚ùå Removal failed: {result.error_message}")
    if result.rollback_executed:
        print("üîÑ Automatic rollback completed")
```

## Complete Enterprise Migration Workflow

```python
from dataflow.migrations.integrated_risk_assessment_system import IntegratedRiskAssessmentSystem

async def enterprise_migration_workflow(
    operation_type: str,
    table_name: str,
    migration_details: dict,
    connection_manager
) -> bool:
    """Complete enterprise migration with all safety systems."""

    # Step 1: Integrated Risk Assessment
    risk_system = IntegratedRiskAssessmentSystem(connection_manager)

    comprehensive_assessment = await risk_system.perform_complete_assessment(
        operation_type=operation_type,
        table_name=table_name,
        operation_details=migration_details,
        include_performance_analysis=True,
        include_dependency_analysis=True,
        include_fk_analysis=True
    )

    print(f"üéØ Risk Assessment:")
    print(f"  Overall Risk: {comprehensive_assessment.overall_risk_level}")
    print(f"  Risk Score: {comprehensive_assessment.risk_score}/100")

    # Step 2: Generate Comprehensive Mitigation Plan
    mitigation_plan = await risk_system.generate_comprehensive_mitigation_plan(
        assessment=comprehensive_assessment,
        business_requirements={
            "max_downtime_minutes": 5,
            "rollback_time_limit_minutes": 10,
            "data_consistency_critical": True,
            "performance_degradation_acceptable": 5  # 5% max
        }
    )

    print(f"üõ°Ô∏è Mitigation strategies: {len(mitigation_plan.strategies)}")

    # Step 3: Create and Test in Staging Environment
    staging_manager = StagingEnvironmentManager(connection_manager)
    staging_env = await staging_manager.create_staging_environment(
        environment_name=f"migration_{int(time.time())}",
        data_sampling_strategy={"strategy": "representative", "sample_percentage": 5}
    )

    try:
        # Test migration in staging
        staging_test = await staging_manager.test_migration_in_staging(
            staging_env,
            migration_plan={
                "operation": operation_type,
                "table": table_name,
                "details": migration_details
            },
            validation_checks=True,
            performance_monitoring=True
        )

        if not staging_test.success:
            print(f"‚ùå Staging test failed: {staging_test.failure_reason}")
            return False

        print(f"‚úÖ Staging test passed - safe to proceed")
        print(f"üìä Performance impact: {staging_test.performance_metrics}")

        # Step 4: Acquire Migration Lock for Production
        lock_manager = MigrationLockManager(connection_manager)

        async with lock_manager.acquire_migration_lock(
            lock_scope="table_modification",
            timeout_seconds=600,
            operation_description=f"{operation_type} on {table_name}"
        ) as migration_lock:

            print(f"üîí Migration lock acquired: {migration_lock.lock_id}")

            # Step 5: Execute with Multi-Stage Validation
            validation_manager = ValidationCheckpointManager(connection_manager)

            validation_result = await validation_manager.execute_with_validation(
                migration_operation=lambda: execute_actual_migration(
                    operation_type, table_name, migration_details
                ),
                checkpoints=[
                    {
                        "stage": "pre_migration",
                        "validators": ["schema_integrity", "fk_consistency", "data_quality"]
                    },
                    {
                        "stage": "during_migration",
                        "validators": ["transaction_health", "performance_monitoring"]
                    },
                    {
                        "stage": "post_migration",
                        "validators": ["data_integrity", "performance_validation", "constraint_validation"]
                    }
                ],
                rollback_on_failure=True
            )

            if validation_result.all_checkpoints_passed:
                print("‚úÖ Enterprise migration completed successfully")
                return True
            else:
                print(f"‚ùå Migration failed: {validation_result.failure_details}")
                print(f"üîÑ Rollback executed: {validation_result.rollback_completed}")
                return False

    finally:
        # Step 6: Cleanup Staging Environment
        await staging_manager.cleanup_staging_environment(staging_env)

# Usage Example
success = await enterprise_migration_workflow(
    operation_type="add_not_null_column",
    table_name="users",
    migration_details={
        "column_name": "account_status",
        "data_type": "VARCHAR(20)",
        "default_value": "active"
    },
    connection_manager=your_connection_manager
)

print(f"Migration result: {'SUCCESS' if success else 'FAILED'}")
```

## TDD Mode & Testing

> **See Skill**: [`dataflow-testing`](../../skills/02-dataflow/dataflow-testing.md) for TDD patterns and test fixtures.

Quick note: TDD mode enables <100ms test execution with automatic rollback via savepoints.

## Critical Limitations & Workarounds

### PostgreSQL Array Types (Still Limited)
```python
# ‚ùå AVOID - PostgreSQL List[str] fields cause parameter type issues
@db.model
class BlogPost:
    title: str
    tags: List[str] = []  # CAUSES ERRORS - avoid array types

# ‚úÖ WORKAROUND - Use JSON field or separate table
@db.model
class BlogPost:
    title: str
    content: str  # v0.4.0: Now unlimited with TEXT fix!
    tags_json: Dict[str, Any] = {}  # Store as JSON object
```

### JSON Field Behavior
```python
# ‚ùå WRONG - JSON fields are returned as strings, not parsed objects
result = results["create_config"]
config = result["config"]["database"]["host"]  # FAILS - config is a string

# ‚úÖ CORRECT - Handle JSON as string or parse if needed
result = results["create_config"]
config_str = result["config"]  # This is a string representation
if isinstance(config_str, str):
    import json
    config = json.loads(config_str)  # Parse if needed
```

### Result Access Patterns
```python
# Results can vary between direct access and wrapper access
result = results[node_id]

# Check both patterns:
if isinstance(result, dict) and "output" in result:
    data = result["output"]  # Wrapper format
else:
    data = result  # Direct format
```

## Production Configuration Patterns

### Development vs Production Setup
```python
# Development (auto-migration safe)
db = DataFlow(auto_migrate=True)  # Default, preserves existing data

# Production (explicit control)
db = DataFlow(
    auto_migrate=False,
    existing_schema_mode=True  # Use existing schema
)
```

## Key Rules

### Always
- Use PostgreSQL for production, SQLite for development (both fully supported)
- Set `existing_schema_mode=True` for existing databases (CRITICAL SAFETY)
- Use `use_real_inspection=True` for real schema discovery (PostgreSQL only)
- Use bulk operations for >100 records
- Use connections for dynamic values
- Follow 3-tier testing: Unit/Integration/E2E with real infrastructure
- Enable `tdd_mode=True` for <100ms test execution with automatic rollback
- Use TDD fixtures (`tdd_dataflow`, `tdd_test_context`) for test isolation
- Trust that auto_migrate=True preserves data (verified safe)
- **NEW: Perform risk assessment for all schema modifications in production**
- **NEW: Use appropriate migration safety level based on operation risk**
- **NEW: Test high-risk migrations in staging environments**
- **NEW: Use migration locks for concurrent migration prevention**
- **NEW: Validate dependencies before column/table operations**
- **NEW: Monitor migration performance and rollback capabilities**

### Never
- Instantiate models directly (`User()`)
- Use `${}` template syntax
- Worry about datetime conversion - now automatic (v0.6.4+)
- Skip safety checks in production
- Expect MySQL execution in alpha (SQLite works fine!)
- Use mocking in Tier 2-3 tests (NO MOCKING policy enforced)
- Use DROP SCHEMA CASCADE for test cleanup (use TDD savepoints instead)
- Use PostgreSQL array types (`List[str]` fields) - causes parameter type issues
- Assume JSON fields are returned as parsed objects - they return as strings
- Worry about auto_migrate=True dropping tables (it won't)
- **NEW: Skip risk assessment for CRITICAL or HIGH risk migrations**
- **NEW: Execute schema changes without dependency analysis**
- **NEW: Run concurrent migrations without lock coordination**
- **NEW: Drop columns/tables without checking foreign key dependencies**
- **NEW: Ignore staging test failures in enterprise workflows**
- **NEW: Skip validation checkpoints for production migrations**

## Migration Decision Matrix

| Migration Type | Risk Level | Required Tools | Recommended Pattern | Safety Level |
|---------------|------------|----------------|---------------------|-------------|
| **Add nullable column** | LOW | Basic validation | Direct execution | Level 1 |
| **Add NOT NULL column** | MEDIUM | NotNullHandler + validation | Plan ‚Üí Validate ‚Üí Execute | Level 2 |
| **Drop column** | HIGH | DependencyAnalyzer + RiskEngine | Full enterprise workflow | Level 3 |
| **Rename column** | MEDIUM | Dependency analysis + validation | Staging test + validation | Level 2 |
| **Change column type** | HIGH | Risk assessment + mitigation | Staging + enterprise workflow | Level 3 |
| **Rename table** | CRITICAL | TableRenameAnalyzer + FK analysis | Full enterprise protocol | Level 3 |
| **Drop table** | CRITICAL | All migration systems | Maximum safety protocol | Level 3 |
| **Add foreign key** | MEDIUM | FK analyzer + validation | FK-aware pattern | Level 2 |
| **Drop foreign key** | HIGH | FK impact analysis + risk engine | Enterprise workflow | Level 3 |
| **Add index** | LOW | Performance validation | Basic execution | Level 1 |
| **Drop index** | MEDIUM | Dependency + performance analysis | Validation required | Level 2 |

## Core Decision Matrix

| Need | Use |
|------|-----|
| Simple CRUD | Basic nodes |
| Bulk import | BulkCreateNode |
| Complex queries | ListNode + MongoDB filters |
| Existing database | existing_schema_mode=True |
| Dynamic models | register_schema_as_models() |
| Cross-session models | reconstruct_models_from_registry() |
| **Schema changes** | **Enterprise migration system** |
| **Risk assessment** | **RiskAssessmentEngine** |
| **Safe migrations** | **Complete enterprise workflow** |
| **FK operations** | **ForeignKeyAnalyzer** |
| **Table restructuring** | **TableRenameAnalyzer + staging** |

## Documentation Quick Links

> **For detailed capabilities, API reference, and examples**: See [DataFlow README](../../sdk-users/apps/dataflow/README.md) and [complete documentation](../../sdk-users/apps/dataflow/docs/).

### Integration Points

#### With Nexus (CRITICAL UPDATE - v0.4.6+)

**‚ö†Ô∏è CRITICAL: Prevent blocking and slow startup when integrating with Nexus**

```python
# CORRECT: Fast, non-blocking integration pattern
from nexus import Nexus
from dataflow import DataFlow

# Step 1: Create Nexus FIRST with auto_discovery=False
app = Nexus(
    api_port=8000,
    mcp_port=3001,
    auto_discovery=False  # CRITICAL: Prevents infinite blocking
)

# Step 2: Create DataFlow with optimized settings
db = DataFlow(
    database_url="postgresql://...",
    enable_model_persistence=False,  # No workflow execution during init
    auto_migrate=False,
    enable_caching=True,  # Keep performance features
    enable_metrics=True
)

# Step 3: Register models (now instant!)
@db.model
class User:
    id: str
    email: str

# Step 4: Manually register workflows
from kailash.workflow.builder import WorkflowBuilder
workflow = WorkflowBuilder()
workflow.add_node("UserCreateNode", "create", {"email": "{{email}}"})
app.register("create_user", workflow.build())
```

**Why These Settings Are Critical:**
- `auto_discovery=False`: Prevents Nexus from re-importing DataFlow models (causes infinite loop)
- `enable_model_persistence=False`: Prevents database writes during initialization
- `auto_migrate=False`: Skips migration checks during startup

**What You Keep:**
- ‚úÖ All CRUD operations work normally
- ‚úÖ All 9 generated nodes per model
- ‚úÖ Connection pooling, caching, metrics
- ‚úÖ Multi-channel access (API, CLI, MCP)

**What You Lose:**
- ‚ùå Model persistence across restarts
- ‚ùå Automatic migration tracking
- ‚ùå Runtime model discovery

**Integration Documentation:**
- üìö [Main Integration Guide](../../sdk-users/guides/dataflow-nexus-integration.md) - Comprehensive guide with 8 use cases
- üöÄ [Full Features Configuration](../../sdk-users/apps/dataflow/docs/integration/dataflow-nexus-full-features.md) - All features enabled (10-30s startup)
- üîç [Blocking Issue Analysis](../../sdk-users/apps/dataflow/docs/integration/nexus-blocking-issue-analysis.md) - Root cause analysis
- üí° [Technical Solution](../../sdk-users/apps/nexus/docs/technical/dataflow-integration-solution.md) - Complete solution details
- üß™ [Working Examples](../../sdk-users/apps/nexus/examples/dataflow-integration/) - Tested code examples

#### With Core SDK
- All DataFlow nodes are Kailash nodes
- Use in standard WorkflowBuilder patterns
- Compatible with all SDK features
- See: [SDK Integration Patterns](../../sdk-users/guides/dataflow-sdk-integration.md)

## Enterprise Migration Checklist

### Pre-Migration Assessment (Required)
- [ ] **Risk Analysis**: Use RiskAssessmentEngine for comprehensive risk scoring
- [ ] **Dependency Check**: Run DependencyAnalyzer to identify all affected database objects
- [ ] **FK Analysis**: Use ForeignKeyAnalyzer for referential integrity impact assessment
- [ ] **Mitigation Planning**: Generate risk reduction strategies with MitigationStrategyEngine
- [ ] **Staging Environment**: Create production-like staging environment for validation
- [ ] **Performance Baseline**: Capture current performance metrics for comparison

### Migration Execution (Required)
- [ ] **Lock Acquisition**: Acquire appropriate migration lock scope to prevent conflicts
- [ ] **Staging Test**: Validate complete migration workflow in staging first
- [ ] **Validation Checkpoints**: Execute with multi-stage validation and rollback capability
- [ ] **Performance Monitoring**: Track execution metrics and resource utilization
- [ ] **Progress Logging**: Maintain detailed audit trail throughout migration
- [ ] **Rollback Readiness**: Ensure rollback procedures are tested and available

### Post-Migration Validation (Required)
- [ ] **Schema Integrity**: Verify all table structures, constraints, and relationships
- [ ] **Data Integrity**: Check referential integrity and data consistency
- [ ] **Performance Validation**: Compare query performance against baseline metrics
- [ ] **Application Testing**: Validate application functionality with new schema
- [ ] **Documentation Update**: Update schema documentation and migration history
- [ ] **Resource Cleanup**: Release migration locks and cleanup staging environments
- [ ] **Monitoring Setup**: Enhanced monitoring for post-migration performance tracking

---

## For Basic Patterns

See the [DataFlow Skills](../../skills/02-dataflow/) for:
- Quick start guides ([`dataflow-quickstart`](../../skills/02-dataflow/dataflow-quickstart.md))
- Basic CRUD operations ([`dataflow-crud-operations`](../../skills/02-dataflow/dataflow-crud-operations.md))
- Simple queries ([`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md))
- Standard configurations ([`dataflow-models`](../../skills/02-dataflow/dataflow-models.md))
- Common patterns ([`dataflow-bulk-operations`](../../skills/02-dataflow/dataflow-bulk-operations.md), [`dataflow-transactions`](../../skills/02-dataflow/dataflow-transactions.md))
- Nexus integration ([`dataflow-nexus-integration`](../../skills/02-dataflow/dataflow-nexus-integration.md))
does
