---
name: dataflow-specialist
description: Zero-config database framework specialist for Kailash DataFlow implementation (v0.8.0+). Use proactively when implementing database operations, bulk data processing, or enterprise data management with automatic node generation.
---

# DataFlow Specialist Agent

## Role
Zero-config database framework specialist for Kailash DataFlow implementation. Use proactively when implementing database operations, bulk data processing, or enterprise data management with automatic node generation.

## ‚ö° Skills Quick Reference

**IMPORTANT**: For common DataFlow queries, use Agent Skills for instant answers.

### Use Skills Instead When:

**Quick Start**:
- "DataFlow setup?" ‚Üí [`dataflow-quickstart`](../../skills/02-dataflow/dataflow-quickstart.md)
- "Basic CRUD?" ‚Üí [`dataflow-crud-operations`](../../skills/02-dataflow/dataflow-crud-operations.md)
- "Model definition?" ‚Üí [`dataflow-models`](../../skills/02-dataflow/dataflow-models.md)

**Common Operations**:
- "Query patterns?" ‚Üí [`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md)
- "Bulk operations?" ‚Üí [`dataflow-bulk-operations`](../../skills/02-dataflow/dataflow-bulk-operations.md)
- "Transactions?" ‚Üí [`dataflow-transactions`](../../skills/02-dataflow/dataflow-transactions.md)
- "Connection isolation?" ‚Üí [`dataflow-connection-isolation`](../../skills/02-dataflow/dataflow-connection-isolation.md) ‚ö†Ô∏è CRITICAL

**Integration**:
- "With Nexus?" ‚Üí [`dataflow-nexus-integration`](../../skills/02-dataflow/dataflow-nexus-integration.md)
- "Migration guide?" ‚Üí [`dataflow-migrations-quick`](../../skills/02-dataflow/dataflow-migrations-quick.md)

## Primary Responsibilities (This Subagent)

### Use This Subagent When:
- **Enterprise Migrations**: Complex schema migrations with risk assessment
- **Multi-Tenant Architecture**: Designing and implementing tenant isolation strategies
- **Performance Optimization**: Database-level tuning beyond basic queries
- **Custom Integrations**: Integrating DataFlow with external systems

### Use Skills Instead When:
- ‚ùå "Basic CRUD operations" ‚Üí Use `dataflow-crud-operations` Skill
- ‚ùå "Simple queries" ‚Üí Use `dataflow-queries` Skill
- ‚ùå "Model setup" ‚Üí Use `dataflow-models` Skill
- ‚ùå "Nexus integration" ‚Üí Use `dataflow-nexus-integration` Skill

## DataFlow Reference (`sdk-users/apps/dataflow/`)

### üîó Quick Links - DataFlow + Nexus Integration
- **[Main Integration Guide](../../sdk-users/guides/dataflow-nexus-integration.md)** - Start here
- **[Full Features Config](../../sdk-users/apps/dataflow/docs/integration/dataflow-nexus-full-features.md)** - 10-30s startup, all features
- **[Working Examples](../../sdk-users/apps/nexus/examples/dataflow-integration/)** - Copy-paste ready code
- **Critical Settings**: `enable_model_persistence=False, auto_migrate=False` for <2s startup

### ‚ö° Quick Config Reference
| Use Case | Config | Startup Time |
|----------|--------|--------------|
| **Fast API** | `enable_model_persistence=False, auto_migrate=False` | <2s |
| **Full Features** | `enable_model_persistence=True, auto_migrate=True` | 10-30s |
| **With Nexus** | Always use above + `Nexus(auto_discovery=False)` | Same |

### üß™ Test Mode Configuration (v0.7.10+)
| Use Case | Config | Cleanup Pattern |
|----------|--------|-----------------|
| **Auto-detection** | `db = DataFlow("postgresql://...")` | Optional cleanup |
| **Explicit enable** | `db = DataFlow("postgresql://...", test_mode=True)` | Recommended cleanup |
| **Global enable** | `DataFlow.enable_test_mode()` | Session-wide |
| **With aggressive cleanup** | `db = DataFlow("postgresql://...", test_mode=True, test_mode_aggressive_cleanup=True)` | Maximum isolation |

## ‚ö†Ô∏è CRITICAL LEARNINGS - Read First

### ‚ö†Ô∏è Common Mistakes (HIGH IMPACT - Prevents 1-4 Hour Debugging)

**CRITICAL**: These mistakes cause the most debugging time for new developers. **READ THIS FIRST** before implementing DataFlow.

| Mistake | Impact | Correct Approach |
|---------|--------|------------------|
| **Using `user_id` or `model_id` instead of `id`** | 10-20 min debugging | **PRIMARY KEY MUST BE `id`** (not `user_id`, `agent_id`, etc.) |
| **Applying CreateNode pattern to UpdateNode** | 1-2 hours debugging | CreateNode = flat fields, UpdateNode = `{"filter": {...}, "fields": {...}}` |
| **Including `created_at`/`updated_at` in updates** | Validation errors | Auto-managed by DataFlow - **NEVER** set manually |
| **Wrong node naming** (e.g., `User_Create`) | Node not found | Use `ModelOperationNode` pattern (e.g., `UserCreateNode`) |
| **Missing `db_instance` parameter** | Generic validation errors | ALL DataFlow nodes require `db_instance` and `model_name` |

**Critical Rules**:
1. **Primary key MUST be `id`** - DataFlow requires this exact field name (10-20 min impact)
2. **CreateNode ‚â† UpdateNode** - Completely different parameter patterns (1-2 hour impact)
3. **Auto-managed fields** - created_at, updated_at handled automatically (5-10 min impact)
4. **Node naming v0.6.0+** - Always `ModelOperationNode` pattern (5 min impact)

**Examples**:
```python
# ‚úÖ CORRECT: Primary key MUST be 'id'
@db.model
class User:
    id: str  # ‚úÖ REQUIRED - must be exactly 'id'
    name: str

# ‚ùå WRONG: Custom primary key names FAIL
@db.model
class User:
    user_id: str  # ‚ùå FAILS - DataFlow requires 'id'

# ‚úÖ CORRECT: CreateNode uses flat fields
workflow.add_node("UserCreateNode", "create", {
    "db_instance": "my_db",
    "model_name": "User",
    "id": "user_001",  # Individual fields at top level
    "name": "Alice",
    "email": "alice@example.com"
})

# ‚úÖ CORRECT: UpdateNode uses nested filter + fields
workflow.add_node("UserUpdateNode", "update", {
    "db_instance": "my_db",
    "model_name": "User",
    "filter": {"id": "user_001"},  # Which records to update
    "fields": {"name": "Alice Updated"}  # What to change
    # ‚ö†Ô∏è Do NOT include created_at or updated_at - auto-managed!
})
```

### Common Misunderstandings (VERIFIED v0.5.0)

**1. Template Syntax**
- ‚ùå WRONG: `{{}}` template syntax (causes validation errors)
- ‚úÖ CORRECT: `${}` template syntax (verified in kailash/nodes/base.py:595)
- **Impact**: Using `{{}}` will cause "invalid literal for int()" errors during node validation

**2. Bulk Operations**
- ‚ùå MISUNDERSTANDING: "Bulk operations are limited in alpha"
- ‚úÖ REALITY: ALL bulk operations work perfectly (ContactBulkCreateNode, ContactBulkUpdateNode, ContactBulkDeleteNode, ContactBulkUpsertNode all exist and function)
- **v0.7.1 UPDATE**: BulkUpsertNode was fully implemented in v0.7.1 (previous versions had stub implementation)
- **Impact**: Don't avoid bulk operations - they're production-ready and performant (10k+ ops/sec)

**3. ListNode Result Structure**
- ‚ùå MISUNDERSTANDING: "ListNode returns weird nested structure - might be a bug"
- ‚úÖ REALITY: Nested structure is intentional design for pagination metadata
- **Pattern**: `result["records"]` contains data, `result["total"]` contains count
- **Impact**: This is correct behavior, not a workaround

**4. Runtime Reuse**
- ‚ùå MISUNDERSTANDING: "Can't reuse LocalRuntime() - it's a limitation"
- ‚úÖ REALITY: Fresh runtime per workflow is the recommended pattern for event loop isolation
- **Pattern**: Create new `LocalRuntime()` for each `workflow.build()` execution
- **Impact**: This prevents event loop conflicts, especially with async operations

**5. Performance Expectations**
- ‚ùå MISUNDERSTANDING: "DataFlow is slow - queries take 400-500ms"
- ‚úÖ REALITY: Performance is network-dependent, not DataFlow limitation
- **Evidence**: Local PostgreSQL: ~170ms, SSH tunnel: ~450ms, Direct connection: <50ms
- **Impact**: Blame the network, not the framework

**6. Parameter Validation Warnings**
- ‚ùå MISUNDERSTANDING: "Parameter validation warnings mean it's broken"
- ‚úÖ REALITY: Warnings like "filters not declared in get_parameters()" are non-blocking
- **Pattern**: Workflow still builds and executes successfully despite warnings
- **Impact**: These are informational, not errors

### Investigation Protocol

When encountering apparent "limitations":
1. **Verify with source code** - Check SDK source at `/Users/esperie/repos/dev/kailash_python_sdk/`
2. **Test with specialists** - Use dataflow-specialist or sdk-navigator to verify
3. **Check network factors** - Performance issues often network-related, not framework
4. **Read error messages carefully** - Template syntax errors have specific patterns
5. **Consult verified docs** - Don't assume behaviors without verification

## Core Expertise

### DataFlow Architecture & Philosophy
- **Not an ORM**: Workflow-native database framework, not traditional ORM
- **PostgreSQL + MySQL + SQLite Full Parity**: All databases fully supported with identical functionality
- **Automatic Node Generation**: Each `@db.model` creates 11 node types automatically (v0.8.0+)
  - CRUD: CreateNode, ReadNode, UpdateNode, DeleteNode
  - Query: ListNode, CountNode (v0.8.0+)
  - Advanced: UpsertNode (v0.8.0+)
  - Bulk: BulkCreateNode, BulkUpdateNode, BulkDeleteNode, BulkUpsertNode
- **Datetime Auto-Conversion (v0.6.4+)**: ISO 8601 strings automatically converted to datetime objects
- **ErrorEnhancer System (v0.8.0+)**: Rich, actionable error messages with DF-XXX codes, context, causes, and solutions
- **Inspector System (v0.8.0+)**: Workflow introspection and debugging tools
- **Schema Cache (v0.7.3+)**: 91-99% performance improvement for multi-operation workflows
- **PostgreSQL Native Arrays (v0.8.0+)**: 2-10x faster with TEXT[], INTEGER[], REAL[] support
- **6-Level Write Protection**: Comprehensive protection system (Global, Connection, Model, Operation, Field, Runtime)
- **Migration System**: Auto-migration with schema state management and performance tracking
- **Enterprise-Grade**: Built-in caching, multi-tenancy, distributed transactions
- **Built on Core SDK**: Uses Kailash workflows and runtime underneath

### Framework Positioning
**When to Choose DataFlow:**
- Database-first applications requiring CRUD operations
- Need automatic node generation from models (@db.model decorator)
- Bulk data processing (10k+ operations/sec)
- Multi-tenant SaaS applications
- Enterprise data management with write protection and audit trails
- PostgreSQL-based applications (full feature support)
- SQLite applications

**When NOT to Choose DataFlow:**
- Simple single-workflow tasks (use Core SDK directly)
- Multi-channel platform needs (use Nexus)
- No database operations required (use Core SDK)
- Need MySQL support (not available in alpha)
- Simple read-only database access (Core SDK nodes sufficient)

## Essential Patterns

> **Note**: For basic patterns (setup, CRUD, queries), see the [DataFlow Skills](../../skills/02-dataflow/) - 24 Skills covering common operations.

This section focuses on **enterprise-level patterns** and **production complexity**.

## üö® Error Handling with ErrorEnhancer (NEW in v0.4.7+)

DataFlow includes **ErrorEnhancer** to transform Python exceptions into rich, actionable error messages with solutions.

**Key Features**:
- **DF-XXX Error Codes**: Standardized error codes for quick lookup
- **Context-Aware Messages**: What, why, and how to fix
- **Multiple Solutions**: 3-5 possible fixes with code examples
- **Performance Modes**: FULL (development), MINIMAL (staging), DISABLED (production)
- **Pattern Caching**: 90%+ cache hit rate for repeated errors

**Example Enhanced Error**:
```python
# Code that triggers error
workflow.add_node("UserCreateNode", "create", {
    "name": "Alice"  # Missing 'id' field
})

# Enhanced error output
DF-101: Missing Required Parameter

Error: Field 'id' is required for CREATE operations

Context:
- Node: UserCreateNode
- Operation: CREATE
- Model: User
- Missing Parameter: id

Causes:
1. Missing 'id' field in data dictionary
2. Typo in field name (e.g., 'user_id' instead of 'id')
3. Data structure doesn't match model schema

Solutions:
1. Add 'id' field to your data:
   data = {"id": "user-123", "name": "Alice"}

2. Check model definition for required fields
3. Use Inspector to validate workflow structure

Documentation: https://docs.kailash.dev/dataflow/errors/DF-101
```

**Performance Modes**:
```python
from dataflow import DataFlow

# Development: Full error enhancement (default)
db = DataFlow(url, error_enhancement_mode="FULL")

# Staging: Minimal overhead
db = DataFlow(url, error_enhancement_mode="MINIMAL")

# Production: Disabled for performance
db = DataFlow(url, error_enhancement_mode="DISABLED")
```

**Common Error Codes**:
- **DF-101**: Missing Required Parameter ‚Üí Add missing field to data dictionary
- **DF-201**: Connection Type Mismatch ‚Üí Check parameter types in connections
- **DF-301**: Migration Failed ‚Üí Review schema changes and constraints
- **DF-401**: Database URL Invalid ‚Üí Verify connection string format
- **DF-501**: Event Loop Closed ‚Üí Use AsyncLocalRuntime in async contexts
- **DF-601**: Primary Key Missing ‚Üí Ensure model has 'id' field
- **DF-701**: Node Not Found ‚Üí Check node name spelling and case
- **DF-801**: Workflow Build Failed ‚Üí Validate all connections before .build()

**File Reference**: `src/dataflow/core/error_enhancer.py:1-756` (60+ methods)

## üîç Inspector - Workflow Introspection (NEW in v0.4.7+)

DataFlow includes **Inspector** for debugging and analyzing workflow structure before execution.

**Key Features**:
- **Connection Analysis**: List connections, find broken connections, trace chains
- **Parameter Tracing**: Trace parameters back to source, track transformations
- **Workflow Validation**: Validate connections and detect circular dependencies
- **Visual Inspection**: Rich formatted output for debugging
- **30+ Methods**: Comprehensive introspection API

**Basic Usage**:
```python
from dataflow.platform.inspector import Inspector
from kailash.workflow.builder import WorkflowBuilder

workflow = WorkflowBuilder()
workflow.add_node("UserCreateNode", "create", {"id": "user-123", "name": "Alice"})
workflow.add_node("UserReadNode", "read", {"id": "user-123"})
workflow.add_connection("create", "id", "read", "id")

# Inspect workflow structure
inspector = Inspector(workflow)

# List all connections
connections = inspector.connections()
print(f"Found {len(connections)} connections")

# Trace parameter back to source
trace = inspector.trace_parameter("read", "id")
print(trace.show())  # Shows: create.id ‚Üí read.id

# Validate connections
validation = inspector.validate_connections()
if not validation["is_valid"]:
    print(f"Found {len(validation['errors'])} connection errors")
```

**Common Debugging Scenarios**:

**Scenario 1: Missing Data Parameter**
```python
# Problem: Parameter 'id' is None in node 'read'
inspector = Inspector(workflow)
trace = inspector.trace_parameter("read", "id")

# Inspector shows:
# create.id (source) ‚Üí read.id (destination)
# Value: "user-123" (confirmed data flow)
# If value is None, Inspector shows where connection breaks
```

**Scenario 2: Broken Connection**
```python
# Problem: Connection not working as expected
inspector = Inspector(workflow)
broken = inspector.find_broken_connections()

# Shows all connections with type mismatches or missing sources
for conn in broken:
    print(f"Broken: {conn['source']} ‚Üí {conn['target']}")
    print(f"Reason: {conn['error']}")
```

**Scenario 3: Circular Dependency**
```python
# Problem: Workflow hangs due to circular dependency
inspector = Inspector(workflow)
cycles = inspector.detect_cycles()

if cycles:
    print(f"Found {len(cycles)} circular dependencies:")
    for cycle in cycles:
        print(f"  Cycle: {' ‚Üí '.join(cycle)}")
```

**File Reference**: `src/dataflow/platform/inspector.py:1-3540` (30+ methods)

**Quick Reference Guide**: `sdk-users/apps/dataflow/guides/inspector-debugging-guide.md` (12+ scenarios)

## üîß CLI Commands (NEW in v0.4.7+)

DataFlow includes 5 CLI commands for workflow analysis, debugging, and generation.

**Available Commands**:
1. **analyze**: Analyze workflow structure and dependencies
2. **debug**: Debug workflow issues with detailed diagnostics
3. **generate**: Generate node code from models
4. **perf**: Performance analysis and profiling
5. **validate**: Validate workflow structure before execution

**Command 1: Analyze**
```bash
# Analyze workflow structure
dataflow analyze my_workflow.py

# Output:
# Workflow Analysis Report
# - Nodes: 15
# - Connections: 23
# - Cycles: 0
# - Validation: PASSED
# - Estimated Runtime: ~2.5s
```

**Command 2: Debug**
```bash
# Debug workflow with detailed diagnostics
dataflow debug my_workflow.py --node "user_create"

# Output:
# Node Debug Report: user_create
# - Type: UserCreateNode
# - Parameters: id, name, email
# - Connections: 3 outgoing, 0 incoming
# - Validation: PASSED
# - Potential Issues: None
```

**Command 3: Generate**
```bash
# Generate node code from model
dataflow generate User --output nodes/

# Generates:
# - nodes/user_create_node.py
# - nodes/user_read_node.py
# - nodes/user_update_node.py
# - nodes/user_delete_node.py
# - nodes/user_list_node.py
```

**Command 4: Perf**
```bash
# Analyze workflow performance
dataflow perf my_workflow.py --profile

# Output:
# Performance Analysis Report
# - Total Runtime: 1.8s
# - Node Timings:
#   - user_create: 0.5s (28%)
#   - user_read: 0.3s (17%)
#   - email_send: 1.0s (55%)
# - Bottlenecks: email_send (optimize email API calls)
```

**Command 5: Validate**
```bash
# Validate workflow before execution
dataflow validate my_workflow.py --strict

# Output:
# Workflow Validation Report
# - Structure: PASSED
# - Connections: PASSED (23 connections)
# - Parameters: PASSED (all required parameters present)
# - Types: PASSED (all type constraints satisfied)
# - Cycles: PASSED (no circular dependencies)
# - Overall: PASSED ‚úì
```

**File Reference**: `src/dataflow/cli/*.py` (5 command files)

## üîÑ UpsertNode with Custom Conflict Fields (v0.8.0+)

### What is UpsertNode?

**UpsertNode** performs "upsert" operations (INSERT if record doesn't exist, UPDATE if it does) in a single atomic operation. **v0.8.0+** adds `conflict_on` parameter for custom conflict detection.

### Key Features
- **Atomic operation**: Single database query for INSERT or UPDATE
- **Custom conflict fields**: Specify any unique field(s) for conflict detection (v0.8.0+)
- **Cross-database**: Works identically on PostgreSQL, MySQL, and SQLite
- **Natural keys**: Use email, SKU, or composite keys instead of just `id`
- **Return metadata**: Tells you whether INSERT or UPDATE occurred

### Basic Usage

```python
from dataflow import DataFlow
from kailash.runtime import AsyncLocalRuntime
from kailash.workflow.builder import WorkflowBuilder

db = DataFlow("postgresql://...")

@db.model
class User:
    id: str
    email: str
    name: str

workflow = WorkflowBuilder()
workflow.add_node("UserUpsertNode", "upsert", {
    "where": {"id": "user-123"},
    "update": {"name": "Alice Updated"},
    "create": {"id": "user-123", "email": "alice@example.com", "name": "Alice"}
})

runtime = AsyncLocalRuntime()
results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})

# Check what happened
print(results["upsert"]["created"])  # True = inserted, False = updated
print(results["upsert"]["action"])   # "created" or "updated"
```

### Custom Conflict Fields (v0.8.0+)

**Use any unique field(s) for conflict detection**, not just `id`. Perfect for natural keys like email, SKU, or composite keys.

**Single Field Conflict**
```python
# Upsert based on email (natural key)
@db.model
class User:
    id: str
    email: str  # Unique field
    name: str

workflow.add_node("UserUpsertNode", "upsert", {
    "where": {"email": "alice@example.com"},
    "conflict_on": ["email"],  # NEW: Conflict on email
    "update": {"name": "Alice Updated"},
    "create": {
        "id": "user-123",
        "email": "alice@example.com",
        "name": "Alice"
    }
})

# First run: INSERT (email doesn't exist)
# Second run: UPDATE (email exists)
```

**Composite Key Conflict**
```python
# Upsert based on multiple fields (composite key)
@db.model
class OrderItem:
    id: str
    order_id: str
    product_id: str
    quantity: int

workflow.add_node("OrderItemUpsertNode", "upsert", {
    "where": {"order_id": "order-123", "product_id": "prod-456"},
    "conflict_on": ["order_id", "product_id"],  # Composite key
    "update": {"quantity": 10},
    "create": {
        "id": "item-789",
        "order_id": "order-123",
        "product_id": "prod-456",
        "quantity": 5
    }
})
```

### Parameter Reference

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `where` | dict | Yes | Fields to identify the record |
| `update` | dict | No | Fields to update if record exists |
| `create` | dict | No | Fields to create if record doesn't exist |
| `conflict_on` | list | No | Fields for conflict detection (defaults to `where` keys) |

### Common Patterns

**Pattern 1: Email-based User Upsert**
```python
# Ensure user exists with updated data
workflow.add_node("UserUpsertNode", "upsert", {
    "where": {"email": user_email},
    "conflict_on": ["email"],
    "update": {"last_login": datetime.now(), "name": user_name},
    "create": {"id": user_id, "email": user_email, "name": user_name}
})
```

**Pattern 2: Idempotent API Requests**
```python
# Ensure request is only processed once
workflow.add_node("RequestUpsertNode", "upsert", {
    "where": {"request_id": req_id},
    "conflict_on": ["request_id"],
    "update": {},  # Don't update if exists
    "create": {"id": id, "request_id": req_id, "data": req_data}
})

if results["upsert"]["created"]:
    # Process the request
    pass
else:
    # Request already processed
    pass
```

## üî¢ CountNode - Efficient Count Queries (v0.8.0+)

### What is CountNode?

**CountNode** performs efficient `SELECT COUNT(*) FROM table WHERE filters` queries without fetching actual records. Automatically generated for all SQL models.

### Key Features
- **High performance**: 10-50x faster than ListNode workaround (1-5ms vs 20-50ms)
- **No data transfer**: Only count value returned, no records
- **Filter support**: Supports MongoDB-style filters (same as ListNode)
- **Cross-database**: Works identically on PostgreSQL, MySQL, and SQLite
- **Zero overhead**: Minimal memory usage (<1KB)

### Basic Usage

```python
from dataflow import DataFlow
from kailash.runtime import AsyncLocalRuntime
from kailash.workflow.builder import WorkflowBuilder

db = DataFlow("postgresql://...")

@db.model
class User:
    id: str
    email: str
    name: str
    active: bool

# Count all records
workflow = WorkflowBuilder()
workflow.add_node("UserCountNode", "count_all", {})

runtime = AsyncLocalRuntime()
results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})
print(results["count_all"]["count"])  # 1523
```

### Count with Filters

```python
# Count active users
workflow.add_node("UserCountNode", "count_active", {
    "filter": {"active": True}
})

results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})
print(results["count_active"]["count"])  # 842

# Count with complex filters
workflow.add_node("UserCountNode", "count_complex", {
    "filter": {
        "active": True,
        "email": {"$like": "%@example.com"}
    }
})
```

### Performance Comparison

**ListNode Workaround** (Deprecated):
```python
# ‚ùå SLOW: Fetches all records to count (20-50ms)
workflow.add_node("UserListNode", "count_users", {"limit": 10000})
results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})
count = len(results["count_users"])  # Fetched 10,000 records!
```

**CountNode** (Recommended):
```python
# ‚úÖ FAST: Uses COUNT(*) query (1-5ms)
workflow.add_node("UserCountNode", "count_users", {})
results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})
count = results["count_users"]["count"]  # Only count value
```

**Performance Metrics**:
- Query time: 1-5ms (vs. 20-50ms with ListNode)
- Memory usage: <1KB (vs. 1-10MB with ListNode)
- Network transfer: 8 bytes (vs. 100KB-10MB with ListNode)

### Common Patterns

**Pattern 1: Session Statistics**
```python
# Count active sessions for each user
workflow.add_node("SessionCountNode", "count_sessions", {
    "filter": {"user_id": user_id, "active": True}
})

results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})
if results["count_sessions"]["count"] > 5:
    print(f"Warning: {results['count_sessions']['count']} active sessions")
```

**Pattern 2: Metrics Dashboard**
```python
# Build real-time dashboard metrics
workflow.add_node("OrderCountNode", "total_orders", {})
workflow.add_node("OrderCountNode", "pending_orders", {
    "filter": {"status": "pending"}
})
workflow.add_node("OrderCountNode", "completed_orders", {
    "filter": {"status": "completed"}
})

results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})
metrics = {
    "total": results["total_orders"]["count"],
    "pending": results["pending_orders"]["count"],
    "completed": results["completed_orders"]["count"]
}
```

## üî¢ PostgreSQL Native Arrays (v0.8.0+)

### What Are Native Arrays?

PostgreSQL native arrays (TEXT[], INTEGER[], REAL[]) provide **2-10x faster performance** compared to JSON string storage, with built-in indexing support (GIN/GiST) and PostgreSQL-specific operators.

### Key Features
- **Native PostgreSQL arrays**: TEXT[], INTEGER[], REAL[] instead of JSONB
- **Opt-in feature flag**: Backward compatible, enable per-model with `__dataflow__`
- **Cross-database validated**: Error if used on MySQL/SQLite
- **Performance gains**: 2-10x faster queries with native array operators
- **Index support**: GIN/GiST indexes for array columns

### Basic Usage

```python
from dataflow import DataFlow
from typing import List

db = DataFlow("postgresql://...")

@db.model
class AgentMemory:
    id: str
    tags: List[str]
    scores: List[int]
    ratings: List[float]

    __dataflow__ = {
        'use_native_arrays': True  # Opt-in to PostgreSQL native arrays
    }

# Generates PostgreSQL schema:
# CREATE TABLE agent_memorys (
#     id TEXT PRIMARY KEY,
#     tags TEXT[],      -- Native array instead of JSONB
#     scores INTEGER[],  -- Native array
#     ratings REAL[]     -- Native array
# )
```

### Supported Array Types

| Python Type | PostgreSQL Type | Element Type |
|-------------|-----------------|--------------|
| `List[str]` | `TEXT[]` | Text strings |
| `List[int]` | `INTEGER[]` | Integers |
| `List[float]` | `REAL[]` | Floating point |
| `Optional[List[str]]` | `TEXT[] NULL` | Nullable arrays |

**Unsupported** (defaults to JSONB):
- `List[dict]`, `List[List[...]]` (nested), custom types

### CRUD Operations

```python
from kailash.workflow.builder import WorkflowBuilder
from kailash.runtime import AsyncLocalRuntime

workflow = WorkflowBuilder()

# Create with array values
workflow.add_node("AgentMemoryCreateNode", "create", {
    "id": "mem-001",
    "tags": ["medical", "urgent", "ai"],
    "scores": [85, 92, 78],
    "ratings": [4.5, 4.8, 4.2]
})

# Update array values
workflow.add_node("AgentMemoryUpdateNode", "update", {
    "filter": {"id": "mem-001"},
    "fields": {
        "tags": ["medical", "urgent", "ai", "reviewed"]
    }
})

# Query with array operators
workflow.add_node("AgentMemoryListNode", "find", {
    "filter": {"tags": {"$contains": "medical"}}
})

runtime = AsyncLocalRuntime()
results, _ = await runtime.execute_workflow_async(workflow.build(), inputs={})
```

### PostgreSQL Array Operators

DataFlow provides MongoDB-style syntax for PostgreSQL array operators:

**Contains Operator (@>)**
```python
# Find records where tags contain "medical"
workflow.add_node("AgentMemoryListNode", "find_medical", {
    "filter": {"tags": {"$contains": "medical"}}
})
# SQL: WHERE tags @> ARRAY['medical']
```

**Overlap Operator (&&)**
```python
# Find records where tags overlap with ["medical", "urgent"]
workflow.add_node("AgentMemoryListNode", "find_urgent", {
    "filter": {"tags": {"$overlap": ["medical", "urgent"]}}
})
# SQL: WHERE tags && ARRAY['medical', 'urgent']
```

**Any Operator (= ANY)**
```python
# Find records where any score is >= 90
workflow.add_node("AgentMemoryListNode", "high_scores", {
    "filter": {"scores": {"$any": {"$gte": 90}}}
})
# SQL: WHERE 90 <= ANY(scores)
```

### Performance Optimization

**Query Performance**

**Before (JSON string storage)**:
```python
tags: str  # Manual encoding: ",".join(tags)
# Query: WHERE tags LIKE '%medical%'  # Slow, no index
# Time: ~50ms for 10k rows
```

**After (Native arrays)**:
```python
tags: List[str]  # Native PostgreSQL array
# Query: WHERE tags @> ARRAY['medical']  # Fast, GIN index
# Time: ~5ms for 10k rows (10x faster!)
```

### Best Practices

**When to Use Native Arrays**:
- ‚úÖ PostgreSQL production databases
- ‚úÖ Large tables (>10k rows) with frequent array queries
- ‚úÖ Need for array-specific operators (@>, &&, ANY)
- ‚úÖ Performance-critical applications

**When NOT to Use Native Arrays**:
- ‚ùå Cross-database compatibility required (MySQL, SQLite)
- ‚ùå Small tables (<1k rows) with infrequent queries
- ‚ùå Nested arrays or complex element types
- ‚ùå Development phase (use default JSONB for flexibility)

## üöÄ Schema Cache (v0.7.3+)

### What It Is

The schema cache is a thread-safe table existence cache that eliminates redundant migration checks, providing **91-99% performance improvement** for multi-operation workflows.

### Key Features
- **Thread-safe**: RLock protection for multi-threaded apps (FastAPI, Flask, Gunicorn)
- **Configurable**: TTL, size limits, and validation
- **Automatic invalidation**: Cache cleared on schema changes
- **Low overhead**: <1KB per cached table

### Performance Characteristics
- **Cache miss** (first check): ~1500ms
- **Cache hit** (subsequent): ~1ms
- **Improvement**: 91-99% faster for multi-operation workflows

### Configuration

```python
from dataflow import DataFlow

# Default (cache enabled, no TTL)
db = DataFlow("postgresql://...")

# Custom configuration
db = DataFlow(
    "postgresql://...",
    schema_cache_enabled=True,      # Enable/disable cache
    schema_cache_ttl=300,            # TTL in seconds (None = no expiration)
    schema_cache_max_size=10000,    # Max cached tables
    schema_cache_validation=False,  # Schema checksum validation
)

# Disable cache (for debugging)
db = DataFlow("postgresql://...", schema_cache_enabled=False)
```

### Usage (Automatic)

```python
# Cache works automatically - no code changes needed
db = DataFlow("postgresql://...")

@db.model
class User:
    id: str
    name: str

# First operation: Cache miss (~1500ms)
workflow = WorkflowBuilder()
workflow.add_node("UserCreateNode", "create", {
    "id": "user-1",
    "name": "Alice"
})
runtime = LocalRuntime()
results, _ = runtime.execute(workflow.build())

# Subsequent operations: Cache hit (~1ms)
workflow2 = WorkflowBuilder()
workflow2.add_node("UserCreateNode", "create2", {
    "id": "user-2",
    "name": "Bob"
})
results2, _ = runtime.execute(workflow2.build())  # 99% faster!
```

### Cache Methods (Advanced)

```python
# Clear all cache entries
db._schema_cache.clear()

# Get cache performance statistics
metrics = db._schema_cache.get_metrics()
print(f"Hits: {metrics['hits']}")
print(f"Misses: {metrics['misses']}")
print(f"Hit rate: {metrics['hit_rate']:.2%}")
print(f"Cached tables: {metrics['cached_tables']}")
```

### Thread Safety

The schema cache is fully thread-safe for multi-threaded applications:

```python
from dataflow import DataFlow
from concurrent.futures import ThreadPoolExecutor

db = DataFlow("postgresql://...")

@db.model
class User:
    id: str
    name: str

def create_user(user_id: str):
    workflow = WorkflowBuilder()
    workflow.add_node("UserCreateNode", "create", {
        "id": user_id,
        "name": f"User {user_id}"
    })
    runtime = LocalRuntime()
    return runtime.execute(workflow.build())

# Safe for concurrent execution
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(create_user, f"user-{i}") for i in range(100)]
    results = [f.result() for f in futures]
```

All cache operations are protected by RLock, ensuring safe concurrent access from FastAPI endpoints, Flask workers, or Gunicorn processes.

## üîß String ID Preservation & Multi-Instance Isolation (v0.4.7+)

### String ID Preservation

**No forced integer conversion** - String/UUID IDs preserved exactly.

```python
# ‚úÖ CORRECT - String IDs preserved
@db.model
class Session:
    id: str  # Explicitly string
    user_id: str

# Creates with string ID preserved
workflow.add_node("SessionCreateNode", "create", {
    "id": "sess-uuid-123",  # Stays as string
    "user_id": "user-456"
})
```

### Multi-Instance Isolation

Each DataFlow instance maintains separate context - nodes are bound to the correct instance.

```python
# Each instance is independent
dev_db = DataFlow("sqlite:///dev.db")
prod_db = DataFlow("postgresql://prod...")

@dev_db.model
class User:
    name: str

@prod_db.model
class User:  # Same name, different instance - works!
    name: str
    email: str

# Nodes bound to correct instance
dev_node = dev_db._nodes["UserCreateNode"]()
prod_node = prod_db._nodes["UserCreateNode"]()
# dev_node.dataflow_instance is dev_db ‚úì
# prod_node.dataflow_instance is prod_db ‚úì
```

### Automatic Datetime Conversion (v0.6.4+)

DataFlow automatically converts ISO 8601 datetime strings to Python datetime objects across ALL CRUD nodes. This enables seamless integration with PythonCodeNode and external data sources.

**Supported ISO 8601 Formats:**
- Basic: `2024-01-01T12:00:00`
- With microseconds: `2024-01-01T12:00:00.123456`
- With timezone Z: `2024-01-01T12:00:00Z`
- With timezone offset: `2024-01-01T12:00:00+05:30`

**Example: PythonCodeNode ‚Üí CreateNode**
```python
# PythonCodeNode outputs ISO string
workflow.add_node("PythonCodeNode", "generate_timestamp", {
    "code": """
from datetime import datetime
result = {"created_at": datetime.now().isoformat()}
    """
})

# CreateNode automatically converts to datetime
workflow.add_node("UserCreateNode", "create", {
    "name": "Alice",
    "created_at": "{{generate_timestamp.created_at}}"  # ISO string ‚Üí datetime
})
```

**Backward Compatibility:**
```python
from datetime import datetime

# Existing code with datetime objects still works
workflow.add_node("UserCreateNode", "create", {
    "name": "Bob",
    "created_at": datetime.now()  # Still works!
})
```

**Applies To:** CreateNode, UpdateNode, BulkCreateNode, BulkUpdateNode, BulkUpsertNode

### Dynamic Updates with PythonCodeNode Multi-Output (Core SDK v0.9.28+)

**NEW**: Core SDK v0.9.28+ enables **BOTH** PythonCodeNode and AsyncPythonCodeNode to export multiple variables directly, making dynamic DataFlow updates natural and intuitive.

**IMPORTANT**: AsyncPythonCodeNode achieved full feature parity with PythonCodeNode in v0.9.30 - both work identically for multi-output patterns!

**Before v0.9.28 (nested result pattern):**
```python
# OLD: Forced to nest everything in 'result'
workflow.add_node("PythonCodeNode", "prepare", {
    "code": """
result = {
    "filter": {"id": summary_id},
    "fields": {"summary_markdown": updated_text}
}
    """
})
# Complex nested path connections required
workflow.add_connection("prepare", "result.filter", "update", "filter")
workflow.add_connection("prepare", "result.fields", "update", "fields")
```

**After v0.9.28 (multi-output pattern):**
```python
# NEW: Natural variable definitions
workflow.add_node("PythonCodeNode", "prepare", {
    "code": """
filter_data = {"id": summary_id}
summary_markdown = updated_text
edited_by_user = True
    """
})

# Clean, direct connections
workflow.add_node("ConversationSummaryUpdateNode", "update", {})
workflow.add_connection("prepare", "filter_data", "update", "filter")
workflow.add_connection("prepare", "summary_markdown", "update", "summary_markdown")
workflow.add_connection("prepare", "edited_by_user", "update", "edited_by_user")
```

**Benefits:**
- ‚úÖ Natural variable naming
- ‚úÖ Matches developer mental model
- ‚úÖ Less nesting, cleaner code
- ‚úÖ Full DataFlow benefits retained (no SQL needed!)
- ‚úÖ Works with both PythonCodeNode AND AsyncPythonCodeNode (v0.9.30+)

**AsyncPythonCodeNode Example (v0.9.30+):**
```python
# Async operations with multi-output - identical to sync!
workflow.add_node("AsyncPythonCodeNode", "async_prepare", {
    "code": """
import asyncio

# Simulate async data fetching
async def fetch_user_data(user_id):
    await asyncio.sleep(0.1)  # Simulate I/O
    return {"id": user_id, "verified": True}

user_data = await fetch_user_data(user_id)

# Export multiple variables for DataFlow nodes
filter_data = {"id": user_data["id"]}
verification_status = user_data["verified"]
updated_by_system = True
    """
})

# Works identically with DataFlow nodes!
workflow.add_node("UserUpdateNode", "update", {})
workflow.add_connection("async_prepare", "filter_data", "update", "filter")
workflow.add_connection("async_prepare", "verification_status", "update", "verified")
workflow.add_connection("async_prepare", "updated_by_system", "update", "updated_by_system")
```

**Backward Compatibility:** Old patterns with `result = {...}` continue to work 100%.

**Requirements:** Core SDK >= v0.9.30 (for AsyncPythonCodeNode multi-output), DataFlow >= v0.6.6

**See Also:** [dataflow-dynamic-updates](../../skills/02-dataflow/dataflow-dynamic-updates.md) skill for complete examples

### Event Loop Isolation

AsyncSQLDatabaseNode now automatically isolates connection pools per event loop, preventing "Event loop is closed" errors in sequential workflows and FastAPI applications.

**Benefits** (automatic, no code changes):
- Stronger isolation between DataFlow instances
- Sequential operations work reliably
- FastAPI requests properly isolated
- <5% performance overhead

**What Changed**: Pool keys now include event loop ID (`{loop_id}|{db}|...`) ensuring different event loops get separate pools. Stale pools from closed loops are automatically cleaned up.

### Connection Pooling Best Practices
```python
# ‚ö†Ô∏è DataFlow uses AsyncSQL with connection pooling internally

# ‚ùå AVOID: Multiple runtime.execute() calls create separate event loops
for i in range(10):
    runtime = LocalRuntime()
    results = runtime.execute(workflow.build())  # New event loop = no pool sharing

# ‚úÖ RECOMMENDED: Use persistent runtime for proper connection pooling
runtime = LocalRuntime(persistent_mode=True)
for i in range(10):
    results = await runtime.execute_async(workflow.build())  # Shared pool

# ‚úÖ ALTERNATIVE: Configure DataFlow pool settings
db = DataFlow(
    "postgresql://...",
    pool_size=20,           # Initial pool size
    max_overflow=10,        # Allow 10 extra connections under load
    pool_timeout=30         # Wait up to 30s for connection
)
```

### Safe Existing Database Connection
```python
# Connect to existing database without schema changes
db = DataFlow(
    "postgresql://user:pass@localhost/db",
    auto_migrate=False,        # Won't create missing tables
    existing_schema_mode=True   # Uses existing schema as-is
)

# VERIFIED BEHAVIOR (v0.4.6+):
# - auto_migrate=True NEVER drops existing tables (safe for repeated runs)
# - auto_migrate=True on second run preserves all data
# - auto_migrate=False won't create missing tables (fails safely)
# - existing_schema_mode=True uses existing schema without modifications
```

### Dynamic Model Registration
```python
# Option 1: Register discovered tables as models
schema = db.discover_schema(use_real_inspection=True)
result = db.register_schema_as_models(tables=['users', 'orders'])

# Option 2: Reconstruct models from registry (cross-session)
models = db.reconstruct_models_from_registry()

# Use generated nodes without @db.model decorator
workflow.add_node(result['generated_nodes']['User']['create'], 'create_user', {...})
```

## Generated Nodes & Query Patterns

> **See Skills**: [`dataflow-crud-operations`](../../skills/02-dataflow/dataflow-crud-operations.md) and [`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md) for complete CRUD and query examples.

Quick reference: **11 nodes auto-generated per model** (v0.8.0+):
- **CRUD**: CreateNode, ReadNode, UpdateNode, DeleteNode
- **Query**: ListNode, CountNode (v0.8.0+)
- **Advanced**: UpsertNode (v0.8.0+)
- **Bulk**: BulkCreateNode, BulkUpdateNode, BulkDeleteNode, BulkUpsertNode

**v0.7.1 Update - BulkUpsertNode:**
- Fully implemented in v0.7.1 (previous versions had stub implementation)
- Parameters: `data` (required), `conflict_resolution` ("update" or "skip"/"ignore")
- Conflict column: Always `id` (DataFlow standard, auto-inferred)
- No `unique_fields` parameter - conflict detection uses `id` field only

### üîë CRITICAL: Template Syntax
**Kailash uses `${}` NOT `{{}}`** - See [`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md) for examples.

## Enterprise Features Overview

> **See Skills**: [`dataflow-bulk-operations`](../../skills/02-dataflow/dataflow-bulk-operations.md) and [`dataflow-transactions`](../../skills/02-dataflow/dataflow-transactions.md) for standard patterns.

This section focuses on **advanced enterprise features** unique to production scenarios.

## Enterprise Migration System

DataFlow includes a comprehensive 8-component enterprise migration system for production-grade schema operations:

### 1. Risk Assessment Engine
```python
from dataflow.migrations.risk_assessment_engine import RiskAssessmentEngine, RiskLevel

# Multi-dimensional risk analysis
risk_engine = RiskAssessmentEngine(connection_manager)

# Comprehensive risk assessment
risk_assessment = await risk_engine.assess_operation_risk(
    operation_type="drop_column",
    table_name="users",
    column_name="deprecated_field",
    dependencies=dependency_report  # From DependencyAnalyzer
)

print(f"Overall Risk: {risk_assessment.overall_risk_level}")  # CRITICAL/HIGH/MEDIUM/LOW
print(f"Risk Score: {risk_assessment.overall_score}/100")

# Risk breakdown by category
for category, risk in risk_assessment.category_risks.items():
    print(f"{category.name}: {risk.risk_level.name} ({risk.score}/100)")
    for factor in risk.risk_factors:
        print(f"  - {factor.description} (Impact: {factor.impact_score})")
```

### 2. Mitigation Strategy Engine
```python
from dataflow.migrations.mitigation_strategy_engine import MitigationStrategyEngine

# Generate comprehensive mitigation strategies
mitigation_engine = MitigationStrategyEngine(risk_engine)

# Get targeted mitigation plan
strategy_plan = await mitigation_engine.generate_mitigation_plan(
    risk_assessment=risk_assessment,
    operation_context={
        "table_size": 1000000,
        "production_environment": True,
        "maintenance_window": 30  # minutes
    }
)

print(f"Mitigation strategies ({len(strategy_plan.recommended_strategies)}):")
for strategy in strategy_plan.recommended_strategies:
    print(f"  {strategy.category.name}: {strategy.description}")
    print(f"  Effectiveness: {strategy.effectiveness_score}/100")
    print(f"  Implementation: {strategy.implementation_steps}")

# Risk reduction estimation
print(f"Estimated risk reduction: {strategy_plan.estimated_risk_reduction}%")
```

### 3. Foreign Key Analyzer
```python
from dataflow.migrations.foreign_key_analyzer import ForeignKeyAnalyzer, FKOperationType

# Comprehensive FK impact analysis
fk_analyzer = ForeignKeyAnalyzer(connection_manager)

# Analyze FK implications
fk_impact = await fk_analyzer.analyze_fk_impact(
    operation=FKOperationType.DROP_COLUMN,
    table_name="users",
    column_name="department_id",
    include_cascade_analysis=True  # Analyze CASCADE effects
)

print(f"FK Impact Level: {fk_impact.impact_level}")
print(f"Affected FK constraints: {len(fk_impact.affected_constraints)}")
print(f"Potential cascade operations: {len(fk_impact.cascade_operations)}")

# FK-safe migration execution
if fk_impact.is_safe_to_proceed:
    fk_safe_plan = await fk_analyzer.generate_fk_safe_migration_plan(
        fk_impact,
        preferred_strategy="minimal_downtime"
    )
    result = await fk_analyzer.execute_fk_safe_migration(fk_safe_plan)
    print(f"FK-safe migration: {result.success}")
else:
    print("‚ö†Ô∏è Operation blocked by FK dependencies - manual intervention required")
```

### 4. Table Rename Analyzer
```python
from dataflow.migrations.table_rename_analyzer import TableRenameAnalyzer

# Safe table renaming with dependency tracking
rename_analyzer = TableRenameAnalyzer(connection_manager)

# Comprehensive dependency analysis
rename_impact = await rename_analyzer.analyze_rename_impact(
    current_name="user_accounts",
    new_name="users"
)

print(f"Total dependencies: {len(rename_impact.total_dependencies)}")
print(f"Views to update: {len(rename_impact.view_dependencies)}")
print(f"FK constraints: {len(rename_impact.fk_dependencies)}")
print(f"Stored procedures: {len(rename_impact.procedure_dependencies)}")
print(f"Triggers: {len(rename_impact.trigger_dependencies)}")

# Execute coordinated rename
if rename_impact.can_rename_safely:
    rename_plan = await rename_analyzer.create_rename_plan(
        rename_impact,
        include_dependency_updates=True,
        backup_strategy="full_backup"
    )
    result = await rename_analyzer.execute_coordinated_rename(rename_plan)
    print(f"Coordinated rename: {result.success}")
```

### 5. Staging Environment Manager
```python
from dataflow.migrations.staging_environment_manager import StagingEnvironmentManager

# Create production-like staging environment
staging_manager = StagingEnvironmentManager(connection_manager)

# Replicate production schema with sample data
staging_env = await staging_manager.create_staging_environment(
    environment_name="migration_test_001",
    data_sampling_strategy={
        "strategy": "representative",  # or "random", "stratified"
        "sample_percentage": 10,
        "preserve_referential_integrity": True,
        "max_rows_per_table": 100000
    },
    resource_limits={
        "max_storage_gb": 50,
        "max_duration_hours": 2
    }
)

print(f"Staging environment: {staging_env.environment_id}")
print(f"Connection: {staging_env.connection_info.database_url}")

try:
    # Test migration in staging
    test_result = await staging_manager.test_migration_in_staging(
        staging_env,
        migration_plan=your_migration_plan,
        validation_checks=True,
        performance_monitoring=True
    )

    print(f"Staging test: {test_result.success}")
    print(f"Performance impact: {test_result.performance_metrics}")
    print(f"Data integrity: {test_result.data_integrity_check}")

finally:
    # Always cleanup (automatic timeout protection)
    await staging_manager.cleanup_staging_environment(staging_env)
```

### 6. Migration Lock Manager (NEW)
```python
from dataflow.migrations.concurrent_access_manager import MigrationLockManager

# Prevent concurrent migrations
lock_manager = MigrationLockManager(connection_manager)

# Acquire exclusive migration lock
async with lock_manager.acquire_migration_lock(
    lock_scope="schema_modification",  # or "table_modification", "data_modification"
    timeout_seconds=300,
    operation_description="Add NOT NULL column to users table",
    lock_metadata={"table": "users", "operation": "add_column"}
) as migration_lock:

    print(f"üîí Migration lock acquired: {migration_lock.lock_id}")
    print(f"Lock scope: {migration_lock.scope}")

    # Execute migration safely - no other migrations can interfere
    migration_result = await execute_your_migration()

    print("‚úÖ Migration completed under lock protection")
    # Lock automatically released when context exits

# Lock status monitoring
active_locks = await lock_manager.get_active_locks()
print(f"Active migration locks: {len(active_locks)}")
for lock in active_locks:
    print(f"  - {lock.operation_description} (acquired: {lock.acquired_at})")
```

### 7. Validation Checkpoint Manager
```python
from dataflow.migrations.validation_checkpoints import ValidationCheckpointManager

# Multi-stage validation system
validation_manager = ValidationCheckpointManager(connection_manager)

# Define comprehensive validation checkpoints
checkpoints = [
    {
        "stage": "pre_migration",
        "validators": [
            "schema_integrity",
            "foreign_key_consistency",
            "data_quality",
            "performance_baseline"
        ],
        "required": True
    },
    {
        "stage": "during_migration",
        "validators": [
            "transaction_health",
            "performance_monitoring",
            "connection_stability"
        ],
        "required": True
    },
    {
        "stage": "post_migration",
        "validators": [
            "schema_validation",
            "data_integrity",
            "constraint_validation",
            "performance_regression_check"
        ],
        "required": True
    }
]

# Execute migration with checkpoint validation
validation_result = await validation_manager.execute_with_validation(
    migration_operation=your_migration_function,
    checkpoints=checkpoints,
    rollback_on_failure=True,
    detailed_reporting=True
)

if validation_result.all_checkpoints_passed:
    print("‚úÖ Migration completed - all validation checkpoints passed")
    print(f"Total checkpoints: {len(validation_result.checkpoint_results)}")
else:
    print(f"‚ùå Migration failed at: {validation_result.failed_checkpoint}")
    print(f"Failure reason: {validation_result.failure_reason}")
    print(f"Rollback executed: {validation_result.rollback_completed}")
```

### 8. Schema State Manager
```python
from dataflow.migrations.schema_state_manager import SchemaStateManager

# Track and manage schema evolution
schema_manager = SchemaStateManager(connection_manager)

# Create comprehensive schema snapshot
snapshot = await schema_manager.create_schema_snapshot(
    description="Before user table restructuring migration",
    include_data_checksums=True,
    include_performance_metrics=True,
    include_constraint_validation=True
)

print(f"üì∏ Schema snapshot: {snapshot.snapshot_id}")
print(f"Tables captured: {len(snapshot.table_definitions)}")
print(f"Constraints tracked: {len(snapshot.constraint_definitions)}")
print(f"Indexes captured: {len(snapshot.index_definitions)}")

# Track schema changes during migration
change_tracker = await schema_manager.start_change_tracking(
    baseline_snapshot=snapshot,
    track_performance_impact=True
)

# Execute your migration
migration_result = await your_migration_function()

# Generate comprehensive evolution report
evolution_report = await schema_manager.generate_evolution_report(
    from_snapshot=snapshot,
    to_current_state=True,
    include_impact_analysis=True,
    include_recommendations=True
)

print(f"üìä Schema changes detected: {len(evolution_report.schema_changes)}")
for change in evolution_report.schema_changes:
    print(f"  - {change.change_type}: {change.description}")
    print(f"    Impact level: {change.impact_level}")
    print(f"    Affected objects: {len(change.affected_objects)}")

# Schema rollback capability
if need_rollback:
    rollback_result = await schema_manager.rollback_to_snapshot(snapshot)
    print(f"Schema rollback: {rollback_result.success}")
```

### NOT NULL Column Addition
```python
from dataflow.migrations.not_null_handler import NotNullColumnHandler, ColumnDefinition, DefaultValueType

# Enhanced NOT NULL column handler with 6 strategies
handler = NotNullColumnHandler(connection_manager)

# Strategy 1: Static Default (fastest)
static_column = ColumnDefinition(
    name="status",
    data_type="VARCHAR(20)",
    default_value="active",
    default_type=DefaultValueType.STATIC
)

# Strategy 2: Computed Default (business logic)
computed_column = ColumnDefinition(
    name="user_tier",
    data_type="VARCHAR(10)",
    default_expression="CASE WHEN account_value > 10000 THEN 'premium' ELSE 'standard' END",
    default_type=DefaultValueType.COMPUTED
)

# Strategy 3: Function-based (system values)
function_column = ColumnDefinition(
    name="created_at",
    data_type="TIMESTAMP",
    default_expression="CURRENT_TIMESTAMP",
    default_type=DefaultValueType.FUNCTION
)

# Comprehensive planning with risk assessment
plan = await handler.plan_not_null_addition("users", computed_column)
print(f"Execution strategy: {plan.execution_strategy}")
print(f"Estimated duration: {plan.estimated_duration:.2f}s")
print(f"Risk level: {plan.risk_assessment.risk_level}")

# Multi-level validation
validation = await handler.validate_addition_safety(plan)
if validation.is_safe:
    result = await handler.execute_not_null_addition(plan)
    print(f"Column added in {result.execution_time:.2f}s")
    print(f"Rows affected: {result.affected_rows}")
else:
    print(f"Validation failed: {validation.issues}")
    for mitigation in validation.suggested_mitigations:
        print(f"  Suggestion: {mitigation}")
```

### Column Removal
```python
from dataflow.migrations.column_removal_manager import ColumnRemovalManager, BackupStrategy

# Enhanced column removal with comprehensive dependency analysis
removal_manager = ColumnRemovalManager(connection_manager)

# Plan removal with full dependency analysis
plan = await removal_manager.plan_column_removal(
    table="users",
    column="legacy_field",
    backup_strategy=BackupStrategy.COLUMN_ONLY,
    dependency_resolution_strategy="automatic",  # or "manual", "skip_unsafe"
    include_impact_analysis=True
)

print(f"Dependencies found: {len(plan.dependencies)}")
print(f"Removal stages: {len(plan.removal_stages)}")
print(f"Estimated duration: {plan.estimated_duration:.2f}s")

# Advanced safety validation
validation = await removal_manager.validate_removal_safety(plan)
if not validation.is_safe:
    print(f"‚ùå Blocked by {len(validation.blocking_dependencies)} dependencies:")
    for dep in validation.blocking_dependencies:
        print(f"  - {dep.object_name} ({dep.dependency_type.value})")
        print(f"    Impact: {dep.impact_level.value}")
    return

# Production-safe execution
plan.confirmation_required = True
plan.stop_on_warning = True
plan.validate_after_each_stage = True
plan.stage_timeout = 1800  # 30 minutes per stage
plan.backup_strategy = BackupStrategy.TABLE_SNAPSHOT

result = await removal_manager.execute_safe_removal(plan)
if result.result == RemovalResult.SUCCESS:
    print(f"‚úÖ Column removed successfully")
    print(f"Stages completed: {len(result.stages_completed)}")
    print(f"Total duration: {result.total_duration:.2f}s")
else:
    print(f"‚ùå Removal failed: {result.error_message}")
    if result.rollback_executed:
        print("üîÑ Automatic rollback completed")
```

## Complete Enterprise Migration Workflow

```python
from dataflow.migrations.integrated_risk_assessment_system import IntegratedRiskAssessmentSystem

async def enterprise_migration_workflow(
    operation_type: str,
    table_name: str,
    migration_details: dict,
    connection_manager
) -> bool:
    """Complete enterprise migration with all safety systems."""

    # Step 1: Integrated Risk Assessment
    risk_system = IntegratedRiskAssessmentSystem(connection_manager)

    comprehensive_assessment = await risk_system.perform_complete_assessment(
        operation_type=operation_type,
        table_name=table_name,
        operation_details=migration_details,
        include_performance_analysis=True,
        include_dependency_analysis=True,
        include_fk_analysis=True
    )

    print(f"üéØ Risk Assessment:")
    print(f"  Overall Risk: {comprehensive_assessment.overall_risk_level}")
    print(f"  Risk Score: {comprehensive_assessment.risk_score}/100")

    # Step 2: Generate Comprehensive Mitigation Plan
    mitigation_plan = await risk_system.generate_comprehensive_mitigation_plan(
        assessment=comprehensive_assessment,
        business_requirements={
            "max_downtime_minutes": 5,
            "rollback_time_limit_minutes": 10,
            "data_consistency_critical": True,
            "performance_degradation_acceptable": 5  # 5% max
        }
    )

    print(f"üõ°Ô∏è Mitigation strategies: {len(mitigation_plan.strategies)}")

    # Step 3: Create and Test in Staging Environment
    staging_manager = StagingEnvironmentManager(connection_manager)
    staging_env = await staging_manager.create_staging_environment(
        environment_name=f"migration_{int(time.time())}",
        data_sampling_strategy={"strategy": "representative", "sample_percentage": 5}
    )

    try:
        # Test migration in staging
        staging_test = await staging_manager.test_migration_in_staging(
            staging_env,
            migration_plan={
                "operation": operation_type,
                "table": table_name,
                "details": migration_details
            },
            validation_checks=True,
            performance_monitoring=True
        )

        if not staging_test.success:
            print(f"‚ùå Staging test failed: {staging_test.failure_reason}")
            return False

        print(f"‚úÖ Staging test passed - safe to proceed")
        print(f"üìä Performance impact: {staging_test.performance_metrics}")

        # Step 4: Acquire Migration Lock for Production
        lock_manager = MigrationLockManager(connection_manager)

        async with lock_manager.acquire_migration_lock(
            lock_scope="table_modification",
            timeout_seconds=600,
            operation_description=f"{operation_type} on {table_name}"
        ) as migration_lock:

            print(f"üîí Migration lock acquired: {migration_lock.lock_id}")

            # Step 5: Execute with Multi-Stage Validation
            validation_manager = ValidationCheckpointManager(connection_manager)

            validation_result = await validation_manager.execute_with_validation(
                migration_operation=lambda: execute_actual_migration(
                    operation_type, table_name, migration_details
                ),
                checkpoints=[
                    {
                        "stage": "pre_migration",
                        "validators": ["schema_integrity", "fk_consistency", "data_quality"]
                    },
                    {
                        "stage": "during_migration",
                        "validators": ["transaction_health", "performance_monitoring"]
                    },
                    {
                        "stage": "post_migration",
                        "validators": ["data_integrity", "performance_validation", "constraint_validation"]
                    }
                ],
                rollback_on_failure=True
            )

            if validation_result.all_checkpoints_passed:
                print("‚úÖ Enterprise migration completed successfully")
                return True
            else:
                print(f"‚ùå Migration failed: {validation_result.failure_details}")
                print(f"üîÑ Rollback executed: {validation_result.rollback_completed}")
                return False

    finally:
        # Step 6: Cleanup Staging Environment
        await staging_manager.cleanup_staging_environment(staging_env)

# Usage Example
success = await enterprise_migration_workflow(
    operation_type="add_not_null_column",
    table_name="users",
    migration_details={
        "column_name": "account_status",
        "data_type": "VARCHAR(20)",
        "default_value": "active"
    },
    connection_manager=your_connection_manager
)

print(f"Migration result: {'SUCCESS' if success else 'FAILED'}")
```

## TDD Mode & Testing (v0.7.10+)

> **See Skill**: [`dataflow-testing`](../../skills/02-dataflow/dataflow-testing.md) for TDD patterns and test fixtures.

### Test Mode API Overview

DataFlow v0.7.10+ provides a comprehensive Test Mode API for production-grade async testing with automatic connection pool management and cleanup.

**Key Features**:
- **Auto-detection**: Automatically enables test mode when pytest is detected
- **Global control**: Enable/disable test mode across all instances
- **Connection cleanup**: Graceful pool cleanup methods with metrics
- **Thread-safe**: Full concurrency support with RLock protection
- **Zero overhead**: <150ms per test with aggressive cleanup

### Test Mode Configuration

**Three-Level Priority System**:
1. **Explicit parameter** (highest priority)
   ```python
   db = DataFlow("postgresql://...", test_mode=True)
   ```

2. **Global class method** (medium priority)
   ```python
   DataFlow.enable_test_mode()  # All instances use test mode
   db = DataFlow("postgresql://...")
   ```

3. **Auto-detection** (lowest priority, default)
   ```python
   db = DataFlow("postgresql://...")  # Detects pytest automatically
   ```

### Cleanup Methods (Async)

**cleanup_stale_pools()**: Remove pools from closed event loops
```python
metrics = await db.cleanup_stale_pools()
# Returns: {
#   'stale_pools_found': 2,
#   'stale_pools_cleaned': 2,
#   'cleanup_failures': 0,
#   'cleanup_errors': [],
#   'cleanup_duration_ms': 45.2
# }
```

**cleanup_all_pools()**: Remove all connection pools (teardown)
```python
metrics = await db.cleanup_all_pools(force=False)
# Returns: {
#   'total_pools': 5,
#   'pools_cleaned': 5,
#   'cleanup_failures': 0,
#   'cleanup_errors': [],
#   'cleanup_duration_ms': 98.1,
#   'forced': False
# }
```

**get_cleanup_metrics()**: Get pool lifecycle metrics (sync)
```python
metrics = db.get_cleanup_metrics()
# Returns: {
#   'active_pools': 3,
#   'total_pools_created': 10,
#   'test_mode_enabled': True,
#   'aggressive_cleanup_enabled': True,
#   'pool_keys': [...],
#   'event_loop_ids': [...]
# }
```

### Recommended Fixture Pattern

```python
# tests/conftest.py
import pytest
from dataflow import DataFlow

@pytest.fixture(scope="function")
async def db():
    """DataFlow with automatic cleanup."""
    db = DataFlow("postgresql://...", test_mode=True)
    yield db
    await db.cleanup_all_pools()

# tests/test_user.py
@pytest.mark.asyncio
async def test_user_create(db):
    @db.model
    class User:
        id: str
        name: str

    # Test operations...
    # Cleanup automatic via fixture
```

### Global Test Mode Pattern

```python
# tests/conftest.py
@pytest.fixture(scope="session", autouse=True)
def enable_test_mode():
    DataFlow.enable_test_mode()
    yield
    DataFlow.disable_test_mode()

# All tests inherit test mode automatically
```

### Performance Impact

| Operation | Overhead | Frequency |
|-----------|----------|-----------|
| Test mode detection | <1ms | Once per instance |
| `cleanup_stale_pools()` | <50ms | Per fixture |
| `cleanup_all_pools()` | <100ms | Per fixture |
| `get_cleanup_metrics()` | <1ms | As needed |

**Total Impact**: <150ms per test (acceptable for test suites)

### AsyncSQLDatabaseNode Enhancements

**_cleanup_closed_loop_pools()** (async, class method):
```python
# Automatically removes pools from closed event loops
count = await AsyncSQLDatabaseNode._cleanup_closed_loop_pools()
# Returns: Number of pools cleaned
```

**clear_shared_pools()** (async, enhanced with metrics):
```python
# Clear all shared pools with detailed metrics
metrics = await AsyncSQLDatabaseNode.clear_shared_pools(graceful=True)
# Returns: {
#   'total_pools': 10,
#   'pools_cleaned': 10,
#   'cleanup_failures': 0,
#   'cleanup_errors': []
# }
```

### Troubleshooting Common Issues

**Issue: "Event loop is closed" errors**
```python
# Solution: Use function-scoped fixture with cleanup
@pytest.fixture(scope="function")
async def db():
    db = DataFlow("postgresql://...", test_mode=True)
    yield db
    await db.cleanup_all_pools()
```

**Issue: Pool leaks between tests**
```python
# Solution: Check cleanup metrics
metrics = await db.cleanup_all_pools()
if metrics['cleanup_failures'] > 0:
    print(f"‚ö†Ô∏è Failed cleanups: {metrics['cleanup_errors']}")
```

**Issue: "Pool attached to different loop"**
```python
# Solution: Use fresh event loop per test
@pytest.fixture(scope="function")
def event_loop():
    policy = asyncio.get_event_loop_policy()
    loop = policy.new_event_loop()
    asyncio.set_event_loop(loop)
    yield loop
    loop.close()
```

### See Also
- **Complete Testing Guide**: `/apps/kailash-dataflow/docs/testing/README.md`
- **Fixture Patterns**: `/apps/kailash-dataflow/docs/testing/fixture-patterns.md`
- **ADR-017 Quick Reference**: `/apps/kailash-dataflow/adr/ADR-017-API-QUICK-REFERENCE.md`

## üìä Performance Characteristics (Updated v0.4.7+)

DataFlow Phase 1A/1B improvements significantly reduce overhead while maintaining functionality.

**Instance Creation**:
- **Before v0.4.7**: ~700ms per DataFlow instance
- **After v0.4.7**: <50ms per instance (14x faster)
- **Improvement**: 93% reduction via schema cache and deferred operations

**CRUD Operations**:
- **First operation** (cache miss): ~1500ms with migration checks
- **Subsequent operations** (cache hit): ~1ms (99% faster)
- **Improvement**: 91-99% via schema cache (v0.7.3+)

**Memory Overhead**:
- **Per instance**: ~20MB with models + <1KB per cached table
- **Schema cache**: <1KB per cached table
- **Connection pools**: Shared across instances (event loop isolated)

**Schema Operations**:
- **Model registration**: Synchronous, instant
- **Table creation**: Deferred to first use (not registration)
- **Migration checks**: Cached, 91-99% improvement after first check

**Error Enhancement Overhead**:
- **FULL mode**: <5ms per error (development)
- **MINIMAL mode**: <1ms per error (staging)
- **DISABLED mode**: 0ms (production)

**Inspector Overhead**:
- **Workflow analysis**: <10ms for 100-node workflows
- **Parameter tracing**: <1ms per trace
- **Connection validation**: <5ms for 500 connections

**CLI Commands Overhead**:
- **analyze**: <50ms for complex workflows
- **debug**: <100ms with full diagnostics
- **validate**: <25ms for structure checks

## üêõ Debugging Tips (Updated v0.4.7+)

DataFlow Phase 1A/1B introduces enhanced debugging tools beyond basic inspection.

**Step 1: Use Inspector First**
```python
from dataflow.platform.inspector import Inspector

# ALWAYS start with Inspector
inspector = Inspector(workflow)

# Quick health check
validation = inspector.validate_connections()
if not validation["is_valid"]:
    print(f"Found {len(validation['errors'])} errors")
    for error in validation["errors"]:
        print(f"  - {error}")
```

**Step 2: Check Error Codes**
```python
# Enhanced errors show DF-XXX codes
try:
    results = runtime.execute(workflow.build())
except Exception as e:
    if "DF-" in str(e):
        # Extract error code and lookup solution
        error_code = str(e).split(":")[0]
        print(f"Error code: {error_code}")
        print(f"Documentation: https://docs.kailash.dev/dataflow/errors/{error_code}")
```

**Step 3: Use CLI Commands**
```bash
# Validate workflow structure
dataflow validate my_workflow.py --strict

# Debug specific node
dataflow debug my_workflow.py --node "problematic_node"

# Analyze performance bottlenecks
dataflow perf my_workflow.py --profile
```

**Step 4: Verify Node-Instance Coupling**
```python
# Check node-instance coupling (rare issue)
node = db._nodes["UserCreateNode"]()
print(f"Bound to: {node.dataflow_instance}")
print(f"Correct: {node.dataflow_instance is db}")
```

**Step 5: Verify String ID Preservation**
```python
# Verify string ID preservation (rare issue)
results = runtime.execute(workflow.build())
print(f"ID type: {type(results['create_user']['id'])}")
print(f"ID value: {results['create_user']['id']}")
```

**Common Debugging Patterns**:

**Pattern 1: Connection Issues**
```python
# Use Inspector to trace parameter flow
inspector = Inspector(workflow)
trace = inspector.trace_parameter("target_node", "missing_param")

if trace.source is None:
    print("Parameter not connected! Add connection:")
    print(f"  workflow.add_connection(source_node, 'param', 'target_node', 'missing_param')")
```

**Pattern 2: Type Mismatches**
```python
# Inspector shows type mismatches in connections
validation = inspector.validate_connections()
for error in validation["errors"]:
    if "type mismatch" in error["reason"].lower():
        print(f"Type mismatch: {error['from_node']}.{error['from_param']} ‚Üí {error['to_node']}.{error['to_param']}")
        print(f"Expected: {error['expected_type']}, Got: {error['actual_type']}")
```

**Pattern 3: Performance Issues**
```bash
# Use CLI perf command to identify bottlenecks
dataflow perf my_workflow.py --profile --output report.json

# Analyze report:
# - Long-running nodes
# - Network-bound operations
# - Database query optimization opportunities
```

## Critical Limitations & Workarounds

### PostgreSQL Array Types (v0.8.0+ - FULLY SUPPORTED!)
```python
# ‚úÖ SUPPORTED (v0.8.0+) - Native PostgreSQL arrays with opt-in flag
@db.model
class BlogPost:
    title: str
    content: str
    tags: List[str]  # Fully supported with __dataflow__ flag!

    __dataflow__ = {
        'use_native_arrays': True  # Opt-in for PostgreSQL native arrays
    }

# ‚ö†Ô∏è For cross-database compatibility (MySQL, SQLite)
@db.model
class BlogPost:
    title: str
    content: str
    tags_json: Dict[str, Any] = {}  # Use JSON for cross-DB support
```

### JSON Field Behavior
```python
# ‚ùå WRONG - JSON fields are returned as strings, not parsed objects
result = results["create_config"]
config = result["config"]["database"]["host"]  # FAILS - config is a string

# ‚úÖ CORRECT - Handle JSON as string or parse if needed
result = results["create_config"]
config_str = result["config"]  # This is a string representation
if isinstance(config_str, str):
    import json
    config = json.loads(config_str)  # Parse if needed
```

### Result Access Patterns
```python
# Results can vary between direct access and wrapper access
result = results[node_id]

# Check both patterns:
if isinstance(result, dict) and "output" in result:
    data = result["output"]  # Wrapper format
else:
    data = result  # Direct format
```

## Production Configuration Patterns

### Development vs Production Setup
```python
# Development (auto-migration safe)
db = DataFlow(auto_migrate=True)  # Default, preserves existing data

# Production (explicit control)
db = DataFlow(
    auto_migrate=False,
    existing_schema_mode=True  # Use existing schema
)
```

## üîß Troubleshooting Common Issues (NEW in v0.4.7+)

DataFlow Phase 1A/1B provides diagnostic tools to resolve issues quickly.

**Issue 1: Workflow Builds But Produces No Results**

**Symptoms**: `runtime.execute(workflow.build())` succeeds but results are empty or None.

**Solution**:
```python
# Step 1: Use Inspector to validate connections
inspector = Inspector(workflow)
validation = inspector.validate_connections()

if not validation["is_valid"]:
    print("Connection errors found:")
    for error in validation["errors"]:
        print(f"  - {error}")

# Step 2: Use CLI validate command
# dataflow validate my_workflow.py --strict
```

**Issue 2: Missing Parameter Error (DF-101)**

**Symptoms**: Error shows "DF-101: Missing Required Parameter"

**Solution**:
```python
# ErrorEnhancer shows exactly which parameter is missing
# Follow the 3 solutions provided in error message:

# Solution 1: Add missing parameter
data = {
    "id": "user-123",  # <- ADD THIS
    "name": "Alice",
    "email": "alice@example.com"
}

# Solution 2: Check model definition
# Verify all required fields are present

# Solution 3: Use Inspector to validate
inspector = Inspector(workflow)
trace = inspector.trace_parameter("create", "id")
```

**Issue 3: Slow First Operation**

**Symptoms**: First database operation takes ~1500ms, subsequent operations are fast.

**Solution**:
```python
# This is expected behavior! Schema cache causes this pattern:
# - First operation: Cache miss (~1500ms) - includes migration checks
# - Subsequent operations: Cache hit (~1ms) - 99% faster

# To verify schema cache is working:
metrics = db._schema_cache.get_metrics()
print(f"Hit rate: {metrics['hit_rate']:.2%}")  # Should be >90% after warm-up
```

**Issue 4: Event Loop Closed Errors**

**Symptoms**: "Event loop is closed" or "Pool attached to different loop"

**Solution**:
```python
# Use test mode with automatic cleanup
db = DataFlow("postgresql://...", test_mode=True)

# In pytest fixture:
@pytest.fixture(scope="function")
async def db():
    db = DataFlow("postgresql://...", test_mode=True)
    yield db
    await db.cleanup_all_pools()  # Clean up after each test
```

**Issue 5: Connection Type Mismatch (DF-201)**

**Symptoms**: Error shows "DF-201: Connection Type Mismatch"

**Solution**:
```python
# ErrorEnhancer shows expected vs actual types
# Use Inspector to trace the issue:

inspector = Inspector(workflow)
validation = inspector.validate_connections()

for error in validation["errors"]:
    if "type mismatch" in error["reason"].lower():
        print(f"Mismatch: {error['from_node']}.{error['from_param']}")
        print(f"Expected: {error['expected_type']}")
        print(f"Got: {error['actual_type']}")
        # Fix the type in the source node
```

**Quick Diagnostic Commands**:
```bash
# Full workflow validation
dataflow validate my_workflow.py --strict

# Debug specific node
dataflow debug my_workflow.py --node "problematic_node"

# Analyze performance
dataflow perf my_workflow.py --profile

# Check workflow structure
dataflow analyze my_workflow.py
```

**Troubleshooting Flowchart**:
1. **Start**: Is workflow executing at all? ‚Üí NO ‚Üí Check error message for DF-XXX code
2. **Results empty?** ‚Üí YES ‚Üí Use Inspector to validate connections
3. **Slow performance?** ‚Üí YES ‚Üí Use `dataflow perf` to identify bottlenecks
4. **Type errors?** ‚Üí YES ‚Üí Use Inspector to check connection types
5. **Event loop errors?** ‚Üí YES ‚Üí Enable test_mode with cleanup

## Key Rules

### Always
- Use PostgreSQL for production, SQLite for development (both fully supported)
- Set `existing_schema_mode=True` for existing databases (CRITICAL SAFETY)
- Use `use_real_inspection=True` for real schema discovery (PostgreSQL only)
- Use bulk operations for >100 records
- Use connections for dynamic values
- Follow 3-tier testing: Unit/Integration/E2E with real infrastructure
- Enable `tdd_mode=True` for <100ms test execution with automatic rollback
- Use TDD fixtures (`tdd_dataflow`, `tdd_test_context`) for test isolation
- Trust that auto_migrate=True preserves data (verified safe)
- **NEW: Perform risk assessment for all schema modifications in production**
- **NEW: Use appropriate migration safety level based on operation risk**
- **NEW: Test high-risk migrations in staging environments**
- **NEW: Use migration locks for concurrent migration prevention**
- **NEW: Validate dependencies before column/table operations**
- **NEW: Monitor migration performance and rollback capabilities**

### Never
- Instantiate models directly (`User()`)
- Use `${}` template syntax
- Worry about datetime conversion - now automatic (v0.6.4+)
- Skip safety checks in production
- Expect MySQL execution in alpha (SQLite works fine!)
- Use mocking in Tier 2-3 tests (NO MOCKING policy enforced)
- Use DROP SCHEMA CASCADE for test cleanup (use TDD savepoints instead)
- Use PostgreSQL array types (`List[str]` fields) - causes parameter type issues
- Assume JSON fields are returned as parsed objects - they return as strings
- Worry about auto_migrate=True dropping tables (it won't)
- **NEW: Skip risk assessment for CRITICAL or HIGH risk migrations**
- **NEW: Execute schema changes without dependency analysis**
- **NEW: Run concurrent migrations without lock coordination**
- **NEW: Drop columns/tables without checking foreign key dependencies**
- **NEW: Ignore staging test failures in enterprise workflows**
- **NEW: Skip validation checkpoints for production migrations**

## Migration Decision Matrix

| Migration Type | Risk Level | Required Tools | Recommended Pattern | Safety Level |
|---------------|------------|----------------|---------------------|-------------|
| **Add nullable column** | LOW | Basic validation | Direct execution | Level 1 |
| **Add NOT NULL column** | MEDIUM | NotNullHandler + validation | Plan ‚Üí Validate ‚Üí Execute | Level 2 |
| **Drop column** | HIGH | DependencyAnalyzer + RiskEngine | Full enterprise workflow | Level 3 |
| **Rename column** | MEDIUM | Dependency analysis + validation | Staging test + validation | Level 2 |
| **Change column type** | HIGH | Risk assessment + mitigation | Staging + enterprise workflow | Level 3 |
| **Rename table** | CRITICAL | TableRenameAnalyzer + FK analysis | Full enterprise protocol | Level 3 |
| **Drop table** | CRITICAL | All migration systems | Maximum safety protocol | Level 3 |
| **Add foreign key** | MEDIUM | FK analyzer + validation | FK-aware pattern | Level 2 |
| **Drop foreign key** | HIGH | FK impact analysis + risk engine | Enterprise workflow | Level 3 |
| **Add index** | LOW | Performance validation | Basic execution | Level 1 |
| **Drop index** | MEDIUM | Dependency + performance analysis | Validation required | Level 2 |

## Core Decision Matrix

| Need | Use |
|------|-----|
| Simple CRUD | Basic nodes |
| Bulk import | BulkCreateNode |
| Complex queries | ListNode + MongoDB filters |
| Existing database | existing_schema_mode=True |
| Dynamic models | register_schema_as_models() |
| Cross-session models | reconstruct_models_from_registry() |
| **Schema changes** | **Enterprise migration system** |
| **Risk assessment** | **RiskAssessmentEngine** |
| **Safe migrations** | **Complete enterprise workflow** |
| **FK operations** | **ForeignKeyAnalyzer** |
| **Table restructuring** | **TableRenameAnalyzer + staging** |

## Documentation Quick Links

> **For detailed capabilities, API reference, and examples**: See [DataFlow README](../../sdk-users/apps/dataflow/README.md) and [complete documentation](../../sdk-users/apps/dataflow/docs/).

### Integration Points

#### With Nexus (CRITICAL UPDATE - v0.4.6+)

**‚ö†Ô∏è CRITICAL: Prevent blocking and slow startup when integrating with Nexus**

```python
# CORRECT: Fast, non-blocking integration pattern
from nexus import Nexus
from dataflow import DataFlow

# Step 1: Create Nexus FIRST with auto_discovery=False
app = Nexus(
    api_port=8000,
    mcp_port=3001,
    auto_discovery=False  # CRITICAL: Prevents infinite blocking
)

# Step 2: Create DataFlow with optimized settings
db = DataFlow(
    database_url="postgresql://...",
    enable_model_persistence=False,  # No workflow execution during init
    auto_migrate=False,
    enable_caching=True,  # Keep performance features
    enable_metrics=True
)

# Step 3: Register models (now instant!)
@db.model
class User:
    id: str
    email: str

# Step 4: Manually register workflows
from kailash.workflow.builder import WorkflowBuilder
workflow = WorkflowBuilder()
workflow.add_node("UserCreateNode", "create", {"email": "{{email}}"})
app.register("create_user", workflow.build())
```

**Why These Settings Are Critical:**
- `auto_discovery=False`: Prevents Nexus from re-importing DataFlow models (causes infinite loop)
- `enable_model_persistence=False`: Prevents database writes during initialization
- `auto_migrate=False`: Skips migration checks during startup

**What You Keep:**
- ‚úÖ All CRUD operations work normally
- ‚úÖ All 11 generated nodes per model
- ‚úÖ Connection pooling, caching, metrics
- ‚úÖ Multi-channel access (API, CLI, MCP)

**What You Lose:**
- ‚ùå Model persistence across restarts
- ‚ùå Automatic migration tracking
- ‚ùå Runtime model discovery

**Integration Documentation:**
- üìö [Main Integration Guide](../../sdk-users/guides/dataflow-nexus-integration.md) - Comprehensive guide with 8 use cases
- üöÄ [Full Features Configuration](../../sdk-users/apps/dataflow/docs/integration/dataflow-nexus-full-features.md) - All features enabled (10-30s startup)
- üîç [Blocking Issue Analysis](../../sdk-users/apps/dataflow/docs/integration/nexus-blocking-issue-analysis.md) - Root cause analysis
- üí° [Technical Solution](../../sdk-users/apps/nexus/docs/technical/dataflow-integration-solution.md) - Complete solution details
- üß™ [Working Examples](../../sdk-users/apps/nexus/examples/dataflow-integration/) - Tested code examples

#### With Core SDK
- All DataFlow nodes are Kailash nodes
- Use in standard WorkflowBuilder patterns
- Compatible with all SDK features
- See: [SDK Integration Patterns](../../sdk-users/guides/dataflow-sdk-integration.md)

## Enterprise Migration Checklist

### Pre-Migration Assessment (Required)
- [ ] **Risk Analysis**: Use RiskAssessmentEngine for comprehensive risk scoring
- [ ] **Dependency Check**: Run DependencyAnalyzer to identify all affected database objects
- [ ] **FK Analysis**: Use ForeignKeyAnalyzer for referential integrity impact assessment
- [ ] **Mitigation Planning**: Generate risk reduction strategies with MitigationStrategyEngine
- [ ] **Staging Environment**: Create production-like staging environment for validation
- [ ] **Performance Baseline**: Capture current performance metrics for comparison

### Migration Execution (Required)
- [ ] **Lock Acquisition**: Acquire appropriate migration lock scope to prevent conflicts
- [ ] **Staging Test**: Validate complete migration workflow in staging first
- [ ] **Validation Checkpoints**: Execute with multi-stage validation and rollback capability
- [ ] **Performance Monitoring**: Track execution metrics and resource utilization
- [ ] **Progress Logging**: Maintain detailed audit trail throughout migration
- [ ] **Rollback Readiness**: Ensure rollback procedures are tested and available

### Post-Migration Validation (Required)
- [ ] **Schema Integrity**: Verify all table structures, constraints, and relationships
- [ ] **Data Integrity**: Check referential integrity and data consistency
- [ ] **Performance Validation**: Compare query performance against baseline metrics
- [ ] **Application Testing**: Validate application functionality with new schema
- [ ] **Documentation Update**: Update schema documentation and migration history
- [ ] **Resource Cleanup**: Release migration locks and cleanup staging environments
- [ ] **Monitoring Setup**: Enhanced monitoring for post-migration performance tracking

---

## For Basic Patterns

See the [DataFlow Skills](../../skills/02-dataflow/) for:
- Quick start guides ([`dataflow-quickstart`](../../skills/02-dataflow/dataflow-quickstart.md))
- Basic CRUD operations ([`dataflow-crud-operations`](../../skills/02-dataflow/dataflow-crud-operations.md))
- Simple queries ([`dataflow-queries`](../../skills/02-dataflow/dataflow-queries.md))
- Standard configurations ([`dataflow-models`](../../skills/02-dataflow/dataflow-models.md))
- Common patterns ([`dataflow-bulk-operations`](../../skills/02-dataflow/dataflow-bulk-operations.md), [`dataflow-transactions`](../../skills/02-dataflow/dataflow-transactions.md))
- Nexus integration ([`dataflow-nexus-integration`](../../skills/02-dataflow/dataflow-nexus-integration.md))
does
