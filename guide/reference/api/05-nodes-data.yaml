# Kailash SDK API Reference - Data Nodes
# Module: kailash.nodes.data
# Version: 0.1.4
# Last Updated: 2025-01-06

nodes:
  # Data Input/Output Nodes
  csv_reader:
    class: kailash.nodes.data.CSVReaderNode
    description: "Read CSV files"
    import: "from kailash.nodes.data import CSVReaderNode"
    config:
      file_path: "str - Path to CSV file (required)"
      delimiter: "str - CSV delimiter (default: ',')"
      has_header: "bool - First row contains headers (default: True)"
      encoding: "str - File encoding (default: 'utf-8')"
    outputs:
      default: "List[Dict] - Parsed CSV data as list of dictionaries"
    example: |
      workflow.add_node('reader', CSVReaderNode(), file_path='data.csv', delimiter=',')

  csv_writer:
    class: kailash.nodes.data.CSVWriterNode
    description: "Write data to CSV files"
    import: "from kailash.nodes.data import CSVWriterNode"
    config:
      file_path: "str - Output CSV file path (required)"
      delimiter: "str - CSV delimiter (default: ',')"
      write_header: "bool - Write header row (default: True)"
    inputs:
      default: "List[Dict] - Data to write"
    example: |
      workflow.add_node('writer', CSVWriterNode(), file_path='output.csv')

  json_reader:
    class: kailash.nodes.data.JSONReaderNode
    description: "Read JSON files"
    import: "from kailash.nodes.data import JSONReaderNode"
    config:
      file_path: "str - Path to JSON file (required)"
      encoding: "str - File encoding (default: 'utf-8')"
    outputs:
      default: "Any - Parsed JSON data"
    example: |
      workflow.add_node('reader', JSONReaderNode(), file_path='data.json')

  json_writer:
    class: kailash.nodes.data.JSONWriterNode
    description: "Write data to JSON files"
    import: "from kailash.nodes.data import JSONWriterNode"
    config:
      file_path: "str - Output JSON file path (required)"
      indent: "int - JSON indentation (default: 2)"
      ensure_ascii: "bool - Escape non-ASCII (default: False)"
    inputs:
      default: "Any - Data to write"
    example: |
      workflow.add_node('writer', JSONWriterNode(), file_path='output.json', indent=4)

  text_reader:
    class: kailash.nodes.data.TextReaderNode
    description: "Read text files"
    import: "from kailash.nodes.data import TextReaderNode"
    config:
      file_path: "str - Path to text file (required)"
      encoding: "str - File encoding (default: 'utf-8')"
    outputs:
      default: "str - Text content"
    example: |
      workflow.add_node('reader', TextReaderNode(), file_path='document.txt')

  text_writer:
    class: kailash.nodes.data.TextWriterNode
    description: "Write text to files"
    import: "from kailash.nodes.data import TextWriterNode"
    config:
      file_path: "str - Output text file path (required)"
      encoding: "str - File encoding (default: 'utf-8')"
      append: "bool - Append to file (default: False)"
    inputs:
      default: "str - Text to write"
    example: |
      workflow.add_node('writer', TextWriterNode(), file_path='output.txt')

# SharePoint Integration
sharepoint:
  SharePointGraphReader:
    class: kailash.nodes.data.sharepoint_graph.SharePointGraphReader
    description: "Read files from SharePoint using Microsoft Graph API"
    import: "from kailash.nodes.data import SharePointGraphReader"
    config:
      tenant_id: "str - Azure AD tenant ID"
      client_id: "str - Azure AD app client ID"
      client_secret: "str - Azure AD app client secret"
      site_url: "str - SharePoint site URL"
      operation: "str - Operation type (list_files, download_file, etc.)"
      library_name: "str - Document library name"
      file_path: "str - File path for download"
    outputs:
      files: "List[dict] - File information"
      content: "bytes - File content (for download)"
    example: |
      workflow.add_node('sharepoint', SharePointGraphReader(),
          tenant_id=os.getenv("SHAREPOINT_TENANT_ID"),
          client_id=os.getenv("SHAREPOINT_CLIENT_ID"),
          client_secret=os.getenv("SHAREPOINT_CLIENT_SECRET"),
          site_url="https://company.sharepoint.com/sites/YourSite",
          operation="list_files",
          library_name="Documents"
      )

  SharePointGraphWriter:
    class: kailash.nodes.data.sharepoint_graph.SharePointGraphWriter
    description: "Upload files to SharePoint using Microsoft Graph API"
    import: "from kailash.nodes.data import SharePointGraphWriter"
    config:
      tenant_id: "str - Azure AD tenant ID"
      client_id: "str - Azure AD app client ID"
      client_secret: "str - Azure AD app client secret"
      site_url: "str - SharePoint site URL"
      library_name: "str - Document library name"
      file_path: "str - Destination file path"
    inputs:
      content: "bytes - File content to upload"
    example: |
      workflow.add_node('upload', SharePointGraphWriter(),
          tenant_id=os.getenv("SHAREPOINT_TENANT_ID"),
          client_id=os.getenv("SHAREPOINT_CLIENT_ID"),
          client_secret=os.getenv("SHAREPOINT_CLIENT_SECRET"),
          site_url="https://company.sharepoint.com/sites/YourSite",
          library_name="Documents",
          file_path="reports/output.pdf"
      )

# RAG (Retrieval-Augmented Generation) Nodes
rag_nodes:
  DocumentSourceNode:
    class: kailash.nodes.data.sources.DocumentSourceNode
    description: "Provide sample documents for RAG workflows"
    import: "from kailash.nodes.data.sources import DocumentSourceNode"
    outputs:
      documents: "List[dict] - Sample documents with title and content"
    example: |
      workflow.add_node('doc_source', DocumentSourceNode())

  QuerySourceNode:
    class: kailash.nodes.data.sources.QuerySourceNode
    description: "Provide sample queries for RAG workflows"
    import: "from kailash.nodes.data.sources import QuerySourceNode"
    outputs:
      query: "str - Sample query text"
    example: |
      workflow.add_node('query_source', QuerySourceNode())

  RelevanceScorerNode:
    class: kailash.nodes.data.retrieval.RelevanceScorerNode
    description: "Score chunks by relevance to query"
    import: "from kailash.nodes.data.retrieval import RelevanceScorerNode"
    config:
      similarity_method: "str - Method (cosine, euclidean, dot_product)"
      top_k: "int - Number of chunks to return (default: 5)"
    inputs:
      chunks: "List[dict] - Document chunks"
      query_embedding: "List[float] - Query embedding"
      chunk_embeddings: "List[List[float]] - Chunk embeddings"
    outputs:
      relevant_chunks: "List[dict] - Top relevant chunks with scores"
    example: |
      workflow.add_node('scorer', RelevanceScorerNode(),
          similarity_method='cosine',
          top_k=5
      )