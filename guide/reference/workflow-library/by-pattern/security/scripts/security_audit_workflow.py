#!/usr/bin/env python3
"""
Security Audit Workflow
=======================

Demonstrates security auditing and compliance patterns using Kailash SDK.
This workflow performs security assessments, vulnerability scanning,
and compliance checking with automated reporting.

Patterns demonstrated:
1. Multi-layer security assessment
2. Vulnerability detection and scoring
3. Compliance framework checking
4. Risk assessment and prioritization
"""

import os
import json
from kailash import Workflow
from kailash.nodes.transform import DataTransformer
from kailash.nodes.data import JSONWriterNode
from kailash.nodes.logic import MergeNode
from kailash.runtime import LocalRuntime


def create_security_audit_workflow() -> Workflow:
    """Create a comprehensive security audit workflow."""
    workflow = Workflow(
        workflow_id="security_audit_001",
        name="security_audit_workflow",
        description="Perform comprehensive security assessment and compliance checking",
    )

    # === SECURITY SCANNING ===

    # Simulate vulnerability scanning across multiple components
    vulnerability_scanner = DataTransformer(
        id="vulnerability_scanner",
        transformations=[
            """
# Perform vulnerability scanning across system components
import random
from datetime import datetime, timedelta

# Define system components to scan
components = [
    {"name": "web-application", "type": "application", "criticality": "high", "exposure": "external"},
    {"name": "api-gateway", "type": "infrastructure", "criticality": "high", "exposure": "external"},
    {"name": "user-database", "type": "database", "criticality": "critical", "exposure": "internal"},
    {"name": "payment-processor", "type": "service", "criticality": "critical", "exposure": "internal"},
    {"name": "admin-panel", "type": "application", "criticality": "medium", "exposure": "internal"},
    {"name": "log-aggregator", "type": "infrastructure", "criticality": "medium", "exposure": "internal"},
    {"name": "backup-storage", "type": "storage", "criticality": "high", "exposure": "internal"},
    {"name": "ci-cd-pipeline", "type": "infrastructure", "criticality": "medium", "exposure": "internal"}
]

# Common vulnerability types and their characteristics
vulnerability_types = [
    {"id": "CVE-2024-0001", "name": "SQL Injection", "severity": "high", "category": "injection", "cvss_score": 8.5},
    {"id": "CVE-2024-0002", "name": "Cross-Site Scripting", "severity": "medium", "category": "xss", "cvss_score": 6.1},
    {"id": "CVE-2024-0003", "name": "Insecure Direct Object Reference", "severity": "medium", "category": "access_control", "cvss_score": 5.4},
    {"id": "CVE-2024-0004", "name": "Security Misconfiguration", "severity": "medium", "category": "misconfiguration", "cvss_score": 5.0},
    {"id": "CVE-2024-0005", "name": "Sensitive Data Exposure", "severity": "high", "category": "data_exposure", "cvss_score": 7.5},
    {"id": "CVE-2024-0006", "name": "XML External Entity", "severity": "medium", "category": "xxe", "cvss_score": 5.5},
    {"id": "CVE-2024-0007", "name": "Broken Authentication", "severity": "critical", "category": "authentication", "cvss_score": 9.8},
    {"id": "CVE-2024-0008", "name": "Insecure Deserialization", "severity": "high", "category": "deserialization", "cvss_score": 8.1},
    {"id": "CVE-2024-0009", "name": "Using Components with Known Vulnerabilities", "severity": "high", "category": "components", "cvss_score": 7.3},
    {"id": "CVE-2024-0010", "name": "Insufficient Logging & Monitoring", "severity": "low", "category": "logging", "cvss_score": 3.1}
]

scan_results = []
current_time = datetime.now()

for component in components:
    # Simulate scanning each component
    # Higher criticality components more likely to have vulnerabilities found
    vuln_probability = 0.3 if component["criticality"] == "critical" else 0.2 if component["criticality"] == "high" else 0.1
    
    component_vulns = []
    
    # Check for vulnerabilities
    for vuln_type in vulnerability_types:
        if random.random() < vuln_probability:
            # Adjust severity based on component exposure and criticality
            base_score = vuln_type["cvss_score"]
            if component["exposure"] == "external":
                adjusted_score = min(10.0, base_score + 1.0)  # External exposure increases risk
            else:
                adjusted_score = base_score
                
            if component["criticality"] == "critical":
                adjusted_score = min(10.0, adjusted_score + 0.5)  # Critical components increase impact
            
            vulnerability = {
                "vulnerability_id": vuln_type["id"],
                "vulnerability_name": vuln_type["name"],
                "category": vuln_type["category"],
                "severity": "critical" if adjusted_score >= 9.0 else "high" if adjusted_score >= 7.0 else "medium" if adjusted_score >= 4.0 else "low",
                "cvss_score": round(adjusted_score, 1),
                "base_score": base_score,
                "component_name": component["name"],
                "component_type": component["type"],
                "component_criticality": component["criticality"],
                "component_exposure": component["exposure"],
                "discovered_at": current_time.isoformat(),
                "status": "open",
                "remediation_effort": "high" if adjusted_score >= 8.0 else "medium" if adjusted_score >= 6.0 else "low",
                "exploitability": "high" if component["exposure"] == "external" and adjusted_score >= 7.0 else "medium" if adjusted_score >= 5.0 else "low"
            }
            component_vulns.append(vulnerability)
    
    scan_result = {
        "component": component,
        "vulnerabilities": component_vulns,
        "vulnerability_count": len(component_vulns),
        "highest_severity": max([v["cvss_score"] for v in component_vulns]) if component_vulns else 0,
        "scan_timestamp": current_time.isoformat()
    }
    scan_results.append(scan_result)

# Calculate overall statistics
total_vulns = sum(result["vulnerability_count"] for result in scan_results)
all_vulns = [vuln for result in scan_results for vuln in result["vulnerabilities"]]

severity_counts = {"critical": 0, "high": 0, "medium": 0, "low": 0}
for vuln in all_vulns:
    severity_counts[vuln["severity"]] += 1

result = {
    "scan_results": scan_results,
    "summary": {
        "total_components_scanned": len(scan_results),
        "total_vulnerabilities": total_vulns,
        "severity_breakdown": severity_counts,
        "components_with_vulns": sum(1 for result in scan_results if result["vulnerability_count"] > 0),
        "highest_cvss_score": max([vuln["cvss_score"] for vuln in all_vulns]) if all_vulns else 0,
        "average_cvss_score": round(sum(vuln["cvss_score"] for vuln in all_vulns) / len(all_vulns), 2) if all_vulns else 0
    },
    "scan_metadata": {
        "scan_type": "comprehensive",
        "scan_timestamp": current_time.isoformat(),
        "scanner_version": "1.0",
        "scan_duration_minutes": 45  # Simulated scan duration
    }
}
"""
        ],
    )
    workflow.add_node("vulnerability_scanner", vulnerability_scanner)

    # === COMPLIANCE CHECKING ===

    # Check compliance against security frameworks
    compliance_checker = DataTransformer(
        id="compliance_checker",
        transformations=[
            """
# Check compliance against security frameworks (SOC2, ISO27001, PCI-DSS)
import random
from datetime import datetime, timedelta

# WORKAROUND: DataTransformer dict output bug
print(f"COMPLIANCE_CHECKER DEBUG - Input type: {type(data)}, Content: {data}")

if isinstance(data, list):
    # Bug case: received list of keys instead of dict
    print("WORKAROUND: Handling DataTransformer dict output bug in compliance_checker")
    # Create mock vulnerability data
    all_vulns = [
        {"vulnerability_id": "CVE-2024-0007", "category": "authentication", "severity": "critical", "cvss_score": 9.8, "component_name": "web-application"},
        {"vulnerability_id": "CVE-2024-0001", "category": "injection", "severity": "high", "cvss_score": 8.5, "component_name": "api-gateway"},
        {"vulnerability_id": "CVE-2024-0005", "category": "data_exposure", "severity": "high", "cvss_score": 7.5, "component_name": "user-database"},
        {"vulnerability_id": "CVE-2024-0002", "category": "xss", "severity": "medium", "cvss_score": 6.1, "component_name": "admin-panel"}
    ]
    summary = {
        "total_vulnerabilities": 4,
        "severity_breakdown": {"critical": 1, "high": 2, "medium": 1, "low": 0},
        "highest_cvss_score": 9.8
    }
    bug_detected = True
else:
    # Expected case: received dict as intended
    scan_results = data.get("scan_results", [])
    summary = data.get("summary", {})
    all_vulns = [vuln for result in scan_results for vuln in result.get("vulnerabilities", [])]
    bug_detected = False

# Define compliance frameworks and their requirements
compliance_frameworks = {
    "SOC2": {
        "name": "SOC 2 Type II",
        "categories": ["access_control", "authentication", "data_exposure", "logging", "misconfiguration"],
        "critical_threshold": 0,  # No critical vulnerabilities allowed
        "high_threshold": 2,      # Max 2 high severity vulnerabilities
        "requirements": {
            "access_control": "Strong access controls must be implemented",
            "authentication": "Multi-factor authentication required for all admin access",
            "data_exposure": "Sensitive data must be encrypted at rest and in transit",
            "logging": "Comprehensive audit logging must be enabled",
            "misconfiguration": "Security configurations must be hardened"
        }
    },
    "ISO27001": {
        "name": "ISO 27001:2013",
        "categories": ["access_control", "authentication", "data_exposure", "components", "misconfiguration"],
        "critical_threshold": 0,  # No critical vulnerabilities allowed
        "high_threshold": 3,      # Max 3 high severity vulnerabilities
        "requirements": {
            "access_control": "Access control policy and procedures must be documented",
            "authentication": "Strong authentication mechanisms required",
            "data_exposure": "Information classification and handling procedures required",
            "components": "Asset management and vulnerability management required",
            "misconfiguration": "Security baseline configurations must be maintained"
        }
    },
    "PCI_DSS": {
        "name": "PCI DSS v4.0",
        "categories": ["injection", "authentication", "data_exposure", "components", "misconfiguration"],
        "critical_threshold": 0,  # No critical vulnerabilities allowed for payment processing
        "high_threshold": 1,      # Max 1 high severity vulnerability
        "requirements": {
            "injection": "Input validation must prevent injection attacks",
            "authentication": "Strong authentication required for cardholder data access",
            "data_exposure": "Cardholder data must be encrypted and protected",
            "components": "All software components must be up to date",
            "misconfiguration": "Default passwords and configurations must be changed"
        }
    }
}

compliance_results = {}

for framework_id, framework in compliance_frameworks.items():
    # Check vulnerabilities against framework requirements
    framework_vulns = [vuln for vuln in all_vulns if vuln.get("category") in framework["categories"]]
    
    # Count vulnerabilities by severity
    critical_count = sum(1 for vuln in framework_vulns if vuln.get("severity") == "critical")
    high_count = sum(1 for vuln in framework_vulns if vuln.get("severity") == "high")
    medium_count = sum(1 for vuln in framework_vulns if vuln.get("severity") == "medium")
    
    # Determine compliance status
    is_compliant = (critical_count <= framework["critical_threshold"] and 
                   high_count <= framework["high_threshold"])
    
    # Calculate compliance score
    total_possible_issues = len(framework["categories"]) * 2  # 2 potential issues per category
    actual_issues = critical_count * 2 + high_count * 1.5 + medium_count * 0.5
    compliance_score = max(0, round((total_possible_issues - actual_issues) / total_possible_issues * 100, 1))
    
    # Identify failing requirements
    failing_requirements = []
    for vuln in framework_vulns:
        if vuln.get("severity") in ["critical", "high"]:
            category = vuln.get("category")
            if category in framework["requirements"]:
                failing_requirements.append({
                    "category": category,
                    "requirement": framework["requirements"][category],
                    "violation": vuln.get("vulnerability_name"),
                    "severity": vuln.get("severity"),
                    "component": vuln.get("component_name")
                })
    
    compliance_result = {
        "framework_name": framework["name"],
        "is_compliant": is_compliant,
        "compliance_score": compliance_score,
        "total_violations": len(framework_vulns),
        "critical_violations": critical_count,
        "high_violations": high_count,
        "medium_violations": medium_count,
        "failing_requirements": failing_requirements,
        "next_assessment_date": (datetime.now() + timedelta(days=90)).isoformat(),
        "assessment_timestamp": datetime.now().isoformat()
    }
    
    compliance_results[framework_id] = compliance_result

# Overall compliance summary
overall_compliance = all(result["is_compliant"] for result in compliance_results.values())
average_score = round(sum(result["compliance_score"] for result in compliance_results.values()) / len(compliance_results), 1)

result = {
    "compliance_results": compliance_results,
    "overall_compliance": {
        "is_compliant": overall_compliance,
        "average_compliance_score": average_score,
        "frameworks_assessed": len(compliance_results),
        "compliant_frameworks": sum(1 for result in compliance_results.values() if result["is_compliant"]),
        "non_compliant_frameworks": sum(1 for result in compliance_results.values() if not result["is_compliant"])
    },
    "bug_detected": bug_detected,
    "assessment_timestamp": datetime.now().isoformat()
}
"""
        ],
    )
    workflow.add_node("compliance_checker", compliance_checker)
    workflow.connect(
        "vulnerability_scanner", "compliance_checker", mapping={"result": "data"}
    )

    # === RISK ASSESSMENT ===

    # Calculate risk scores and prioritize remediation
    risk_assessor = DataTransformer(
        id="risk_assessor",
        transformations=[
            """
# Calculate risk scores and prioritize security remediation
import datetime

# WORKAROUND: DataTransformer dict output bug
print(f"RISK_ASSESSOR DEBUG - Input type: {type(data)}, Content: {data}")

if isinstance(data, list):
    # Bug case: received list of keys instead of dict
    print("WORKAROUND: Handling DataTransformer dict output bug in risk_assessor")
    # Create mock vulnerability data
    all_vulns = [
        {"vulnerability_id": "CVE-2024-0007", "category": "authentication", "severity": "critical", "cvss_score": 9.8, "component_name": "web-application", "component_exposure": "external", "component_criticality": "high", "exploitability": "high"},
        {"vulnerability_id": "CVE-2024-0001", "category": "injection", "severity": "high", "cvss_score": 8.5, "component_name": "api-gateway", "component_exposure": "external", "component_criticality": "high", "exploitability": "high"},
        {"vulnerability_id": "CVE-2024-0005", "category": "data_exposure", "severity": "high", "cvss_score": 7.5, "component_name": "user-database", "component_exposure": "internal", "component_criticality": "critical", "exploitability": "medium"},
        {"vulnerability_id": "CVE-2024-0002", "category": "xss", "severity": "medium", "cvss_score": 6.1, "component_name": "admin-panel", "component_exposure": "internal", "component_criticality": "medium", "exploitability": "low"}
    ]
    bug_detected = True
else:
    # Expected case: received dict as intended
    scan_results = data.get("scan_results", [])
    all_vulns = [vuln for result in scan_results for vuln in result.get("vulnerabilities", [])]
    bug_detected = False

if not all_vulns:
    result = {"error": "No vulnerability data available for risk assessment", "bug_detected": bug_detected}
else:
    # Risk scoring matrix
    risk_factors = {
        "cvss_weight": 0.4,           # Technical severity
        "exposure_weight": 0.25,      # External vs internal exposure
        "criticality_weight": 0.25,   # Component business criticality
        "exploitability_weight": 0.1  # Ease of exploitation
    }
    
    # Scoring scales
    exposure_scores = {"external": 10, "internal": 5, "private": 2}
    criticality_scores = {"critical": 10, "high": 7, "medium": 5, "low": 2}
    exploitability_scores = {"high": 10, "medium": 6, "low": 3}
    
    risk_assessments = []
    
    for vuln in all_vulns:
        # Calculate individual risk factors
        cvss_normalized = vuln.get("cvss_score", 0)  # Already 0-10 scale
        exposure_score = exposure_scores.get(vuln.get("component_exposure", "internal"), 5)
        criticality_score = criticality_scores.get(vuln.get("component_criticality", "medium"), 5)
        exploitability_score = exploitability_scores.get(vuln.get("exploitability", "medium"), 6)
        
        # Calculate weighted risk score
        risk_score = (
            cvss_normalized * risk_factors["cvss_weight"] +
            exposure_score * risk_factors["exposure_weight"] +
            criticality_score * risk_factors["criticality_weight"] +
            exploitability_score * risk_factors["exploitability_weight"]
        )
        
        # Determine risk level
        if risk_score >= 8.5:
            risk_level = "critical"
            priority = 1
            remediation_timeline = "immediate"
        elif risk_score >= 7.0:
            risk_level = "high"
            priority = 2
            remediation_timeline = "1 week"
        elif risk_score >= 5.0:
            risk_level = "medium"
            priority = 3
            remediation_timeline = "1 month"
        else:
            risk_level = "low"
            priority = 4
            remediation_timeline = "3 months"
        
        # Estimate remediation effort and cost
        remediation_effort = vuln.get("remediation_effort", "medium")
        effort_hours = {"low": 8, "medium": 24, "high": 80}.get(remediation_effort, 24)
        estimated_cost = effort_hours * 150  # $150/hour developer rate
        
        # Business impact assessment
        if vuln.get("component_criticality") == "critical" and vuln.get("severity") in ["critical", "high"]:
            business_impact = "high"
            potential_loss = "500000+"  # Revenue loss, regulatory fines
        elif vuln.get("component_exposure") == "external" and vuln.get("severity") in ["critical", "high"]:
            business_impact = "medium"
            potential_loss = "100000-500000"  # Reputation damage, incident response
        else:
            business_impact = "low"
            potential_loss = "10000-100000"  # Operational disruption
        
        risk_assessment = {
            "vulnerability_id": vuln.get("vulnerability_id"),
            "vulnerability_name": vuln.get("vulnerability_name"),
            "component_name": vuln.get("component_name"),
            "risk_score": round(risk_score, 2),
            "risk_level": risk_level,
            "priority": priority,
            "remediation_timeline": remediation_timeline,
            "business_impact": business_impact,
            "potential_loss_usd": potential_loss,
            "remediation_effort_hours": effort_hours,
            "estimated_cost_usd": estimated_cost,
            "risk_factors": {
                "technical_severity": cvss_normalized,
                "exposure_risk": exposure_score,
                "business_criticality": criticality_score,
                "exploitability": exploitability_score
            },
            "recommended_actions": [
                f"Patch {vuln.get('vulnerability_name')} in {vuln.get('component_name')}",
                f"Test fix in staging environment",
                f"Deploy fix within {remediation_timeline}",
                "Update security documentation"
            ],
            "assessment_timestamp": datetime.datetime.now().isoformat()
        }
        
        risk_assessments.append(risk_assessment)
    
    # Sort by risk score descending (highest risk first)
    risk_assessments.sort(key=lambda x: x["risk_score"], reverse=True)
    
    # Calculate portfolio risk metrics
    total_risk_score = sum(assessment["risk_score"] for assessment in risk_assessments)
    average_risk_score = total_risk_score / len(risk_assessments)
    
    risk_distribution = {"critical": 0, "high": 0, "medium": 0, "low": 0}
    for assessment in risk_assessments:
        risk_distribution[assessment["risk_level"]] += 1
    
    total_remediation_cost = sum(assessment["estimated_cost_usd"] for assessment in risk_assessments)
    critical_remediation_cost = sum(assessment["estimated_cost_usd"] for assessment in risk_assessments if assessment["risk_level"] == "critical")
    
    # Generate executive summary
    executive_summary = {
        "overall_risk_level": "critical" if risk_distribution["critical"] > 0 else "high" if risk_distribution["high"] > 2 else "medium",
        "total_vulnerabilities": len(risk_assessments),
        "highest_risk_score": risk_assessments[0]["risk_score"] if risk_assessments else 0,
        "average_risk_score": round(average_risk_score, 2),
        "risk_distribution": risk_distribution,
        "immediate_action_required": risk_distribution["critical"] + risk_distribution["high"],
        "total_remediation_cost": total_remediation_cost,
        "critical_remediation_cost": critical_remediation_cost,
        "recommended_budget": critical_remediation_cost + (total_remediation_cost * 0.2)  # 20% contingency
    }
    
    result = {
        "risk_assessments": risk_assessments,
        "executive_summary": executive_summary,
        "remediation_roadmap": {
            "immediate": [a for a in risk_assessments if a["risk_level"] == "critical"],
            "short_term": [a for a in risk_assessments if a["risk_level"] == "high"],
            "medium_term": [a for a in risk_assessments if a["risk_level"] == "medium"],
            "long_term": [a for a in risk_assessments if a["risk_level"] == "low"]
        },
        "bug_detected": bug_detected,
        "assessment_timestamp": datetime.datetime.now().isoformat()
    }
"""
        ],
    )
    workflow.add_node("risk_assessor", risk_assessor)
    workflow.connect(
        "vulnerability_scanner", "risk_assessor", mapping={"result": "data"}
    )

    # === SECURITY REPORTING ===

    # Merge compliance and risk data for comprehensive reporting
    security_merger = MergeNode(id="security_merger", merge_type="merge_dict")
    workflow.add_node("security_merger", security_merger)
    workflow.connect(
        "compliance_checker", "security_merger", mapping={"result": "data1"}
    )
    workflow.connect("risk_assessor", "security_merger", mapping={"result": "data2"})

    # Generate comprehensive security report
    security_reporter = DataTransformer(
        id="security_reporter",
        transformations=[
            """
# Generate comprehensive security audit report
import datetime

# WORKAROUND: DataTransformer dict output bug
print(f"SECURITY_REPORTER DEBUG - Input type: {type(data)}, Content: {data}")

if isinstance(data, list):
    # Bug case: received list of keys instead of dict
    print("WORKAROUND: Handling DataTransformer dict output bug in security_reporter")
    # Create mock merged data
    compliance_data = {
        "overall_compliance": {"is_compliant": False, "average_compliance_score": 75.5, "compliant_frameworks": 1, "non_compliant_frameworks": 2},
        "compliance_results": {
            "SOC2": {"is_compliant": True, "compliance_score": 85.0, "critical_violations": 0, "high_violations": 1},
            "PCI_DSS": {"is_compliant": False, "compliance_score": 65.0, "critical_violations": 1, "high_violations": 2}
        }
    }
    risk_data = {
        "executive_summary": {"overall_risk_level": "critical", "total_vulnerabilities": 4, "highest_risk_score": 9.2, "immediate_action_required": 3, "total_remediation_cost": 45000},
        "risk_assessments": [
            {"vulnerability_id": "CVE-2024-0007", "risk_level": "critical", "priority": 1, "remediation_timeline": "immediate", "estimated_cost_usd": 12000},
            {"vulnerability_id": "CVE-2024-0001", "risk_level": "high", "priority": 2, "remediation_timeline": "1 week", "estimated_cost_usd": 18000}
        ]
    }
    merged_data = {**compliance_data, **risk_data}
    bug_detected = True
else:
    # Expected case: received dict as intended
    merged_data = data
    bug_detected = False

# Extract key information
compliance_results = merged_data.get("compliance_results", {})
overall_compliance = merged_data.get("overall_compliance", {})
risk_summary = merged_data.get("executive_summary", {})
risk_assessments = merged_data.get("risk_assessments", [])

# Determine overall security posture
overall_risk_level = risk_summary.get("overall_risk_level", "unknown")
is_compliant = overall_compliance.get("is_compliant", False)

if overall_risk_level == "critical" or not is_compliant:
    security_posture = "CRITICAL"
    posture_color = "red"
elif overall_risk_level == "high" or overall_compliance.get("average_compliance_score", 100) < 80:
    security_posture = "HIGH RISK"
    posture_color = "orange"
elif overall_risk_level == "medium":
    security_posture = "MODERATE RISK"
    posture_color = "yellow"
else:
    security_posture = "LOW RISK"
    posture_color = "green"

# Generate executive dashboard
current_time = datetime.datetime.now()
executive_dashboard = {
    "security_posture": security_posture,
    "posture_color": posture_color,
    "overall_compliance": overall_compliance.get("is_compliant", False),
    "compliance_score": f"{overall_compliance.get('average_compliance_score', 0):.1f}%",
    "total_vulnerabilities": risk_summary.get("total_vulnerabilities", 0),
    "critical_high_vulns": risk_summary.get("immediate_action_required", 0),
    "highest_risk_score": risk_summary.get("highest_risk_score", 0),
    "remediation_budget_required": f"${risk_summary.get('total_remediation_cost', 0):,}",
    "frameworks_compliant": f"{overall_compliance.get('compliant_frameworks', 0)}/{overall_compliance.get('compliant_frameworks', 0) + overall_compliance.get('non_compliant_frameworks', 0)}",
    "report_timestamp": current_time.isoformat()
}

# Generate key findings
key_findings = []

# Security findings
if risk_summary.get("immediate_action_required", 0) > 0:
    key_findings.append({
        "type": "security",
        "severity": "critical",
        "finding": f"{risk_summary.get('immediate_action_required', 0)} vulnerabilities require immediate attention",
        "impact": "High risk of security breach or data compromise",
        "recommendation": "Prioritize patching of critical and high-risk vulnerabilities"
    })

# Compliance findings
non_compliant_frameworks = []
for framework_id, result in compliance_results.items():
    if not result.get("is_compliant", True):
        non_compliant_frameworks.append(result.get("framework_name", framework_id))

if non_compliant_frameworks:
    key_findings.append({
        "type": "compliance",
        "severity": "major",
        "finding": f"Non-compliant with: {', '.join(non_compliant_frameworks)}",
        "impact": "Regulatory penalties, audit failures, business risk",
        "recommendation": "Address compliance violations to meet regulatory requirements"
    })

# Cost findings
if risk_summary.get("total_remediation_cost", 0) > 100000:
    key_findings.append({
        "type": "financial",
        "severity": "major",
        "finding": f"High remediation cost: ${risk_summary.get('total_remediation_cost', 0):,}",
        "impact": "Significant budget impact for security improvements",
        "recommendation": "Prioritize fixes by risk score to maximize security ROI"
    })

# Generate action plan
action_plan = {
    "immediate_actions": [
        f"Address {risk_summary.get('immediate_action_required', 0)} critical/high vulnerabilities",
        "Activate incident response team for critical findings",
        "Implement temporary mitigations for external-facing vulnerabilities"
    ],
    "short_term_actions": [
        "Complete vulnerability remediation within defined timelines",
        "Update security policies and procedures",
        "Conduct security awareness training"
    ],
    "long_term_actions": [
        "Implement continuous security monitoring",
        "Establish regular penetration testing schedule",
        "Enhance security development lifecycle (SDLC)"
    ],
    "budget_requirements": {
        "immediate": risk_summary.get("total_remediation_cost", 0),
        "quarterly": risk_summary.get("total_remediation_cost", 0) * 0.3,
        "annual": risk_summary.get("total_remediation_cost", 0) * 1.5
    }
}

# Generate detailed sections
vulnerability_summary = {
    "total_found": risk_summary.get("total_vulnerabilities", 0),
    "by_risk_level": {
        "critical": len([a for a in risk_assessments if a.get("risk_level") == "critical"]),
        "high": len([a for a in risk_assessments if a.get("risk_level") == "high"]),
        "medium": len([a for a in risk_assessments if a.get("risk_level") == "medium"]),
        "low": len([a for a in risk_assessments if a.get("risk_level") == "low"])
    },
    "top_vulnerabilities": risk_assessments[:5] if risk_assessments else []
}

compliance_summary = {
    "overall_status": overall_compliance,
    "framework_details": compliance_results,
    "next_assessment": (current_time + datetime.timedelta(days=90)).isoformat()
}

# Final comprehensive report
report = {
    "security_audit_report": {
        "executive_dashboard": executive_dashboard,
        "key_findings": key_findings,
        "vulnerability_summary": vulnerability_summary,
        "compliance_summary": compliance_summary,
        "action_plan": action_plan,
        "detailed_assessments": {
            "vulnerabilities": risk_assessments,
            "compliance": compliance_results
        }
    },
    "report_metadata": {
        "generated_at": current_time.isoformat(),
        "report_type": "comprehensive_security_audit",
        "version": "1.0",
        "bug_detected": bug_detected,
        "next_audit_date": (current_time + datetime.timedelta(days=90)).isoformat(),
        "audit_scope": "full_application_infrastructure"
    },
    "recommendations": {
        "priority_1": [finding["recommendation"] for finding in key_findings if finding["severity"] == "critical"],
        "priority_2": [finding["recommendation"] for finding in key_findings if finding["severity"] == "major"],
        "priority_3": [finding["recommendation"] for finding in key_findings if finding["severity"] == "minor"]
    }
}

result = report
"""
        ],
    )
    workflow.add_node("security_reporter", security_reporter)
    workflow.connect(
        "security_merger", "security_reporter", mapping={"merged_data": "data"}
    )

    # === OUTPUTS ===

    # Save comprehensive security audit report
    audit_writer = JSONWriterNode(
        id="audit_writer", file_path="data/outputs/security_audit_report.json"
    )
    workflow.add_node("audit_writer", audit_writer)
    workflow.connect("security_reporter", "audit_writer", mapping={"result": "data"})

    # Save vulnerability details for tracking
    vuln_writer = JSONWriterNode(
        id="vuln_writer", file_path="data/outputs/vulnerability_details.json"
    )
    workflow.add_node("vuln_writer", vuln_writer)
    workflow.connect("vulnerability_scanner", "vuln_writer", mapping={"result": "data"})

    return workflow


def run_security_audit():
    """Execute the security audit workflow."""
    workflow = create_security_audit_workflow()
    runtime = LocalRuntime()

    parameters = {}

    try:
        print("Starting Security Audit Workflow...")
        print("🔍 Scanning for vulnerabilities...")

        result, run_id = runtime.execute(workflow, parameters=parameters)

        print("\\n✅ Security Audit Complete!")
        print("📁 Outputs generated:")
        print("   - Security audit report: data/outputs/security_audit_report.json")
        print("   - Vulnerability details: data/outputs/vulnerability_details.json")

        # Show executive dashboard
        audit_result = result.get("security_reporter", {}).get("result", {})
        security_report = audit_result.get("security_audit_report", {})
        executive_dashboard = security_report.get("executive_dashboard", {})

        print(
            f"\\n📊 Security Posture: {executive_dashboard.get('security_posture', 'UNKNOWN')}"
        )
        print(
            f"   - Compliance Status: {'✅ Compliant' if executive_dashboard.get('overall_compliance') else '❌ Non-Compliant'}"
        )
        print(
            f"   - Compliance Score: {executive_dashboard.get('compliance_score', 'N/A')}"
        )
        print(
            f"   - Total Vulnerabilities: {executive_dashboard.get('total_vulnerabilities', 0)}"
        )
        print(
            f"   - Critical/High Risk: {executive_dashboard.get('critical_high_vulns', 0)}"
        )
        print(
            f"   - Highest Risk Score: {executive_dashboard.get('highest_risk_score', 0)}/10"
        )
        print(
            f"   - Remediation Budget: {executive_dashboard.get('remediation_budget_required', 'N/A')}"
        )

        # Show key findings
        key_findings = security_report.get("key_findings", [])
        if key_findings:
            print("\\n🚨 KEY FINDINGS:")
            for finding in key_findings[:3]:  # Show top 3 findings
                print(
                    f"   - [{finding.get('severity', 'unknown').upper()}] {finding.get('finding', 'N/A')}"
                )

        return result

    except Exception as e:
        print(f"❌ Security Audit failed: {str(e)}")
        raise


def main():
    """Main entry point."""
    # Create output directories
    os.makedirs("data/outputs", exist_ok=True)

    # Run the security audit workflow
    run_security_audit()

    # Display generated reports
    print("\\n=== Security Audit Report Preview ===")
    try:
        with open("data/outputs/security_audit_report.json", "r") as f:
            report = json.load(f)
            executive_dashboard = report["security_audit_report"]["executive_dashboard"]
            print(json.dumps(executive_dashboard, indent=2))

        print("\\n=== Vulnerability Summary ===")
        with open("data/outputs/vulnerability_details.json", "r") as f:
            vulns = json.load(f)
            summary = vulns["summary"]
            print(f"Total Vulnerabilities: {summary['total_vulnerabilities']}")
            print(f"Severity Breakdown: {summary['severity_breakdown']}")
            print(f"Highest CVSS Score: {summary['highest_cvss_score']}")
    except Exception as e:
        print(f"Could not read reports: {e}")


if __name__ == "__main__":
    main()
