# AI Registry App Configuration - Pure Kailash Implementation

app:
  name: "ai-registry"
  version: "1.0.0"
  description: "AI Registry with RAG capabilities using pure Kailash SDK"

server:
  transport: "stdio"  # or "http" for REST API
  enable_cache: true
  enable_metrics: true
  enable_formatting: true

cache:
  enabled: true
  default_ttl: 300
  pdf_analysis_ttl: 3600      # 1 hour for PDF analysis
  search_ttl: 300             # 5 minutes for search
  statistics_ttl: 600         # 10 minutes for statistics
  embeddings_ttl: 7200        # 2 hours for embeddings

modules:
  rag:
    models:
      document_analyzer: "gpt-4o-mini"
      use_case_extractor: "gpt-4o-mini"
      metadata_enricher: "gpt-4o-mini"
      embedding_model: "text-embedding-3-small"
      trend_analyzer: "gpt-4o"        # Complex reasoning
      synthesis_model: "gpt-4o"       # Cross-domain analysis
    processing:
      chunk_size: 2000
      chunk_overlap: 200
      max_tokens: 4000
      temperature: 0.1
    extraction:
      max_use_cases_per_section: 50
      confidence_threshold: 0.8
      validation_enabled: true
    embeddings:
      dimension: 1536
      batch_size: 100
      similarity_threshold: 0.75

  search:
    default_limit: 20
    max_limit: 100
    similarity_threshold: 0.75
    enable_semantic: true

  analysis:
    trend_analysis_model: "gpt-4o"
    synthesis_model: "gpt-4o"
    enable_cross_domain: true

data:
  registry_file: "data/combined_ai_registry.json"
  knowledge_base_path: "data/knowledge_base"
  embeddings_path: "data/embeddings"
  pdf_sources:
    - "data/2021 - Section 7.pdf"
    - "data/2024 - Section 7.pdf"

workflows:
  timeouts:
    pdf_processing: 300     # 5 minutes
    search: 30             # 30 seconds
    analysis: 120          # 2 minutes
    embedding_generation: 180  # 3 minutes

# Environment-specific overrides
environments:
  development:
    cache:
      enabled: false
    logging:
      level: "DEBUG"
    rag:
      models:
        document_analyzer: "llama3.1:8b"  # Use local model for dev

  testing:
    cache:
      enabled: false
    metrics:
      enabled: false
    rag:
      mock_responses: true

  production:
    cache:
      enabled: true
    metrics:
      enabled: true
    logging:
      level: "INFO"

# Model configuration for cost optimization
model_selection:
  cost_optimization: true
  fallback_to_local: true
  budget_constraints: false

# API keys and credentials (use environment variables in production)
api:
  openai_api_key: "${OPENAI_API_KEY}"
  ollama_base_url: "http://localhost:11434"

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: null  # Use stdout by default
