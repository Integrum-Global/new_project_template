---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: nexus-production
  labels:
    app: nexus
    component: alertmanager
    environment: production
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'nexus-alerts@company.com'
      smtp_auth_username: 'nexus-alerts@company.com'
      smtp_auth_password: 'your-smtp-password'

    # Template files for notifications
    templates:
    - '/etc/alertmanager/templates/*.tmpl'

    # Route all alerts to appropriate receivers
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: critical-alerts
        continue: true
      - match:
          severity: warning
        receiver: warning-alerts
        continue: true
      - match:
          alertname: NexusAppDown
        receiver: app-down-alerts
        group_wait: 0s
        group_interval: 5m
        repeat_interval: 5m

    # Receivers define how to send notifications
    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://nexus-webhook-service:8080/alerts'
        send_resolved: true
        http_config:
          bearer_token: 'webhook-secret-token'

    - name: 'critical-alerts'
      email_configs:
      - to: 'ops-team@company.com'
        subject: 'üö® CRITICAL: {{ .GroupLabels.alertname }} in {{ .GroupLabels.cluster }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          Started: {{ .StartsAt }}
          {{ end }}
        headers:
          X-Priority: '1'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#ops-critical'
        title: 'Critical Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          üö® *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          *Labels:* {{ range .Labels.SortedPairs }}`{{ .Name }}={{ .Value }}` {{ end }}
          *Started:* {{ .StartsAt }}
          {{ end }}
        send_resolved: true

    - name: 'warning-alerts'
      email_configs:
      - to: 'dev-team@company.com'
        subject: '‚ö†Ô∏è WARNING: {{ .GroupLabels.alertname }} in {{ .GroupLabels.cluster }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          Started: {{ .StartsAt }}
          {{ end }}
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#ops-warnings'
        title: 'Warning: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          ‚ö†Ô∏è *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          *Labels:* {{ range .Labels.SortedPairs }}`{{ .Name }}={{ .Value }}` {{ end }}
          *Started:* {{ .StartsAt }}
          {{ end }}
        send_resolved: true

    - name: 'app-down-alerts'
      email_configs:
      - to: 'oncall@company.com'
        subject: 'üî• APP DOWN: {{ .GroupLabels.alertname }} - IMMEDIATE ACTION REQUIRED'
        body: |
          NEXUS APPLICATION IS DOWN!

          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          Started: {{ .StartsAt }}
          {{ end }}

          Please check the application immediately!
        headers:
          X-Priority: '1'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#ops-critical'
        title: 'üî• NEXUS APPLICATION DOWN'
        text: |
          <!channel> IMMEDIATE ACTION REQUIRED
          {{ range .Alerts }}
          üî• *{{ .Annotations.summary }}*
          {{ .Annotations.description }}
          *Labels:* {{ range .Labels.SortedPairs }}`{{ .Name }}={{ .Value }}` {{ end }}
          *Started:* {{ .StartsAt }}
          {{ end }}
        send_resolved: true

    # Inhibition rules to suppress redundant alerts
    inhibit_rules:
    - source_match:
        alertname: NexusAppDown
      target_match:
        severity: warning
      equal: ['cluster', 'service']
    - source_match:
        severity: critical
      target_match:
        severity: warning
      equal: ['alertname', 'cluster', 'service']

  alert-templates.tmpl: |
    {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}

    {{ define "slack.title" }}
    [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }} ({{ template "cluster" . }})
    {{ end }}

    {{ define "slack.text" }}
    {{ range .Alerts -}}
    *Alert:* {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
    *Description:* {{ .Annotations.description }}
    *Graph:* <{{ .GeneratorURL }}|üìä View Graph>
    *Details:*
    {{ range .Labels.SortedPairs }} ‚Ä¢ *{{ .Name }}:* `{{ .Value }}`
    {{ end }}
    {{ end }}
    {{ end }}

    {{ define "email.subject" }}
    [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
    {{ end }}

    {{ define "email.html" }}
    <h2>{{ .GroupLabels.alertname }}</h2>
    <h3>{{ .Status | title }} ({{ if eq .Status "firing" }}{{ .Alerts.Firing | len }}{{ else }}{{ .Alerts.Resolved | len }}{{ end }} alert{{ if ne (len .Alerts) 1 }}s{{ end }})</h3>

    {{ range .Alerts }}
    <div style="border-left: 4px solid {{ if eq .Status "firing" }}#d73027{{ else }}#1a9850{{ end }}; padding-left: 10px; margin: 10px 0;">
      <h4>{{ .Annotations.summary }}</h4>
      <p><strong>Description:</strong> {{ .Annotations.description }}</p>
      <p><strong>Started:</strong> {{ .StartsAt }}</p>
      {{ if .EndsAt }}<p><strong>Ended:</strong> {{ .EndsAt }}</p>{{ end }}

      <h5>Labels:</h5>
      <ul>
      {{ range .Labels.SortedPairs }}
        <li><strong>{{ .Name }}:</strong> {{ .Value }}</li>
      {{ end }}
      </ul>

      {{ if .GeneratorURL }}
      <p><a href="{{ .GeneratorURL }}">View in Prometheus</a></p>
      {{ end }}
    </div>
    {{ end }}
    {{ end }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: nexus-production
  labels:
    app: nexus
    component: alertmanager
    environment: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      serviceAccountName: nexus-monitoring-service-account
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        args:
          - '--config.file=/etc/alertmanager/alertmanager.yml'
          - '--storage.path=/alertmanager'
          - '--web.external-url=http://alertmanager.nexus.local'
          - '--cluster.listen-address=0.0.0.0:9094'
          - '--log.level=info'
        ports:
        - containerPort: 9093
          name: web
        - containerPort: 9094
          name: cluster
        volumeMounts:
        - name: config-volume
          mountPath: /etc/alertmanager
        - name: storage-volume
          mountPath: /alertmanager
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9093
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9093
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config-volume
        configMap:
          name: alertmanager-config
      - name: storage-volume
        persistentVolumeClaim:
          claimName: alertmanager-storage-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alertmanager-storage-pvc
  namespace: nexus-production
  labels:
    app: nexus
    component: alertmanager
    environment: production
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 2Gi
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: nexus-production
  labels:
    app: nexus
    component: alertmanager
    environment: production
spec:
  selector:
    app: alertmanager
  ports:
  - name: web
    port: 9093
    targetPort: 9093
  - name: cluster
    port: 9094
    targetPort: 9094
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: alertmanager-network-policy
  namespace: nexus-production
  labels:
    app: nexus
    component: alertmanager
    environment: production
spec:
  podSelector:
    matchLabels:
      app: alertmanager
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # Allow ingress from Prometheus
  - from:
    - podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 9093
  # Allow ingress from Grafana
  - from:
    - podSelector:
        matchLabels:
          app: grafana
    ports:
    - protocol: TCP
      port: 9093
  # Allow cluster communication
  - from:
    - podSelector:
        matchLabels:
          app: alertmanager
    ports:
    - protocol: TCP
      port: 9094
  egress:
  # Allow DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53
  # Allow HTTPS for webhooks and email
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 587
    - protocol: TCP
      port: 25
