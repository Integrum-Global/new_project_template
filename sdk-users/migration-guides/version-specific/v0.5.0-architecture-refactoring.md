# Kailash SDK v0.5.0 Migration Guide - Architecture Refactoring

This guide covers the major architectural changes in v0.5.0 that address critical issues identified in CORE_SDK_PROBLEMS.md. Each change is explained with the underlying problems it solves and why the new approach is better.

> **Last Updated**: Fixed DataFrame comparison issue, async execution in middleware, and all tests now passing (788 non-slow tests)

## Table of Contents
1. [Sync/Async Node Separation](#1-syncasync-node-separation)
2. [Execute/Run API Standardization](#2-executerun-api-standardization)
3. [WorkflowBuilder API Unification](#3-workflowbuilder-api-unification)
4. [Resource Management](#4-resource-management)
5. [Parameter Resolution Optimization](#5-parameter-resolution-optimization)

## 1. Sync/Async Node Separation

### Why This Change Was Necessary

The original design attempted to be "helpful" by auto-detecting whether a node was sync or async, but this created more problems than it solved:

1. **Production Failures**: In production environments with complex async contexts, the auto-detection would fail unpredictably
2. **Performance Impact**: Every node execution had to check method signatures and attempt various execution strategies
3. **Debugging Nightmare**: Stack traces were convoluted due to multiple layers of async wrapping
4. **Hidden Complexity**: Developers couldn't easily understand how their nodes would execute

### What Was Wrong
The old Node base class attempted to auto-detect whether a node implemented sync or async execution:

```python
# OLD: Complex auto-detection in base Node.run()
def run(self, **kwargs):
    # Check if this node has implemented async_run
    async_run_method = getattr(self.__class__, "async_run", None)
    base_async_run = getattr(Node, "async_run", None)

    if async_run_method and async_run_method != base_async_run:
        # Try to run async method synchronously
        import asyncio
        try:
            loop = asyncio.get_running_loop()
            import nest_asyncio
            nest_asyncio.apply()
            return asyncio.run(self.async_run(**kwargs))
        except RuntimeError:
            return asyncio.run(self.async_run(**kwargs))
    else:
        raise NotImplementedError(
            f"Node must implement either run() or async_run()"
        )
```

**Problems:**
- Runtime type detection overhead on every execution
- nest_asyncio patches causing unpredictable behavior
- Potential deadlocks when mixing sync/async code
- Confusing error messages
- Performance overhead from detection logic

### New Implementation

```python
# NEW: Clear separation - Synchronous nodes
from kailash.nodes.base import Node

class MySyncNode(Node):
    def run(self, **kwargs):
        # Synchronous implementation only
        result = process_data_sync(kwargs["data"])
        return {"result": result}

# NEW: Asynchronous nodes
from kailash.nodes.base_async import AsyncNode

class MyAsyncNode(AsyncNode):
    async def async_run(self, **kwargs):
        # Asynchronous implementation only
        result = await process_data_async(kwargs["data"])
        return {"result": result}
```

### Migration Steps

1. **Identify your node type:**
   - Does it use `await`, `async with`, or async libraries? → Use `AsyncNode`
   - Otherwise → Use `Node`

2. **Update imports:**
   ```python
   # OLD
   from kailash.nodes.base import Node

   # NEW - for sync nodes
   from kailash.nodes.base import Node

   # NEW - for async nodes
   from kailash.nodes.base import AsyncNode
   ```

3. **Update class inheritance:**
   ```python
   # OLD - Mixed sync/async
   class MyNode(Node):
       async def async_run(self, **kwargs):
           ...

   # NEW - Explicitly async
   class MyNode(AsyncNode):
       async def async_run(self, **kwargs):
           ...
   ```

4. **Remove any sync run() method from async nodes:**
   ```python
   # OLD - Don't do this anymore
   class MyNode(Node):
       def run(self, **kwargs):
           # Fallback sync implementation
           pass

       async def async_run(self, **kwargs):
           # Async implementation
           pass

   # NEW - AsyncNode only needs async_run
   class MyNode(AsyncNode):
       async def async_run(self, **kwargs):
           # Async implementation only
           pass
   ```

## 2. Execute/Run API Standardization

### What Was Wrong

The old implementation had confusing method names and inconsistent behavior:
- Some nodes called `run()` directly
- Some had `execute()` that called `run()`
- Some had `async_run()` and `execute_async()`
- Base Node had both methods trying to auto-detect behavior

### New Implementation

**Clear API contract:**
- `execute()` - Public API, handles validation, metrics, error handling
- `run()` - Implementation method for sync nodes (developers implement this)
- `async_run()` - Implementation method for async nodes (developers implement this)

```python
# NEW: Sync node - implement run() only
class MySyncNode(Node):
    def get_parameters(self):
        return {
            "data": NodeParameter(name="data", type=list, required=True)
        }

    def run(self, **kwargs):
        # Your implementation here
        data = kwargs["data"]
        result = process_data(data)
        return {"result": result}

# NEW: Async node - implement async_run() only
class MyAsyncNode(AsyncNode):
    def get_parameters(self):
        return {
            "url": NodeParameter(name="url", type=str, required=True)
        }

    async def async_run(self, **kwargs):
        # Your async implementation here
        url = kwargs["url"]
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                data = await response.json()
        return {"data": data}
```

### Migration Steps

1. **Never override execute() or execute_async()** - these are framework methods
2. **For sync nodes:** implement `run()` only
3. **For async nodes:** implement `async_run()` only
4. **Let the framework handle validation and error wrapping**

### Important Notes for Async Nodes

When using `AsyncNode` in an async context (like middleware or async tests):
- Call `execute_async()` directly: `await node.execute_async(**kwargs)`
- Don't call `execute()` from async context as it creates a new event loop
- The framework handles event loop management automatically

## 3. WorkflowBuilder API Unification

### What Was Wrong

```python
# OLD: Inconsistent APIs
# Workflow accepts instances, classes, or strings
workflow.add_node("reader", CSVReaderNode())  # Instance
workflow.add_node("processor", DataProcessor)  # Class
workflow.add_node("writer", "JSONWriterNode")  # String

# WorkflowBuilder only accepted strings
builder.add_node("CSVReaderNode", "reader", config)  # String only
```

### New Implementation

```python
# NEW: WorkflowBuilder now accepts all forms like Workflow
builder = WorkflowBuilder()

# All these now work:
builder.add_node("CSVReaderNode", "reader", config)      # String
builder.add_node(CSVReaderNode, "reader", config)        # Class
builder.add_node(CSVReaderNode(), "reader")              # Instance

# Convenience methods for clarity:
builder.add_node_type("CSVReaderNode", "reader", config) # String-based
builder.add_node_instance(my_node, "reader")             # Instance-based
```

### Migration Steps

1. **No breaking changes** - existing string-based code still works
2. **New capability** - you can now pass instances and classes to WorkflowBuilder
3. **Best practice** - use the method that matches your use case:
   - Building from config/JSON? Use strings
   - Building programmatically? Use instances or classes

### Example Migration

```python
# OLD: Had to use different patterns
workflow = Workflow("example", "Example")
workflow.add_node("reader", CSVReaderNode())  # Direct instance

builder = WorkflowBuilder()
builder.add_node("CSVReaderNode", "reader", {"file": "data.csv"})  # String only

# NEW: Consistent patterns work everywhere
workflow = Workflow("example", "Example")
workflow.add_node("reader", CSVReaderNode())  # Still works

builder = WorkflowBuilder()
# All these work now:
builder.add_node(CSVReaderNode(), "reader")  # Instance
builder.add_node(CSVReaderNode, "reader", {"file": "data.csv"})  # Class
builder.add_node("CSVReaderNode", "reader", {"file": "data.csv"})  # String
```

## 4. Resource Management

### What Was Wrong

```python
# OLD: Manual session management, potential leaks
class HTTPRequestNode(Node):
    def run(self, **kwargs):
        session = requests.Session()  # Created every time
        try:
            response = session.get(url)
            return {"data": response.json()}
        finally:
            session.close()  # Easy to forget

# OLD: Async version with resource leaks
class AsyncHTTPRequestNode(AsyncNode):
    def __init__(self):
        self._session = None

    async def async_run(self, **kwargs):
        if self._session is None:
            self._session = aiohttp.ClientSession()  # Never closed!
        response = await self._session.get(url)
        return {"data": await response.json()}
```

### New Implementation

```python
# NEW: Automatic connection pooling with proper cleanup
from kailash.utils.resource_manager import ResourcePool, managed_resource

# Global connection pools (defined in http.py)
_http_session_pool = ResourcePool(
    factory=lambda: requests.Session(),
    max_size=20,
    timeout=30.0,
    cleanup=lambda session: session.close()
)

class HTTPRequestNode(Node):
    def run(self, **kwargs):
        # Automatic pooling and cleanup
        with _http_session_pool.acquire() as session:
            response = session.get(url)
            return {"data": response.json()}

# For custom resources
class DatabaseNode(Node):
    def __init__(self):
        self.pool = ResourcePool(
            factory=lambda: psycopg2.connect(DSN),
            max_size=10,
            cleanup=lambda conn: conn.close()
        )

    def run(self, **kwargs):
        with self.pool.acquire() as conn:
            cursor = conn.cursor()
            cursor.execute(kwargs["query"])
            return {"results": cursor.fetchall()}
```

### Migration Steps

1. **Identify resource creation** in your nodes (sessions, connections, files)
2. **Create resource pools** for expensive resources
3. **Use context managers** for automatic cleanup
4. **Monitor resource usage** with the resource tracker

```python
from kailash.utils.resource_manager import get_resource_tracker

# Check resource metrics
tracker = get_resource_tracker()
metrics = tracker.get_metrics()
print(f"Active HTTP sessions: {metrics['http_session']['active']}")
```

## 5. Parameter Resolution Optimization

### What Was Wrong

```python
# OLD: 4-phase resolution running every time
def validate_inputs(self, **kwargs):
    params = self.get_parameters()  # Called every time

    # Phase 1: Direct matches
    for param in params:
        if param in kwargs:
            resolved[param] = kwargs[param]

    # Phase 2: Workflow aliases
    for param in params:
        if param.workflow_alias in kwargs:
            resolved[param] = kwargs[param.workflow_alias]

    # Phase 3: Auto-mapping
    for param in params:
        for alt in param.auto_map_from:
            if alt in kwargs:
                resolved[param] = kwargs[alt]

    # Phase 4: Primary detection
    # ... more iterations
```

### New Implementation

```python
# NEW: Single-pass resolution with caching
def validate_inputs(self, **kwargs):
    # Parameters cached after first call
    params = self._get_cached_parameters()

    # Check cache for this input pattern
    cache_key = self._get_cache_key(kwargs)
    if cache_key in self._param_cache:
        # Use cached mapping
        return self._apply_cached_mapping(kwargs, self._param_cache[cache_key])

    # Single optimized pass for all resolution phases
    resolved = self._resolve_parameters(kwargs, params)

    # Cache the mapping pattern
    self._param_cache[cache_key] = self._extract_mapping_pattern(kwargs, resolved)

    return resolved
```

### Performance Benefits

- **First execution**: ~15% faster due to single-pass algorithm
- **Repeated patterns**: ~80% faster due to caching
- **Memory efficient**: Only caches mapping patterns, not values

### Migration Steps

No code changes needed! The optimization is automatic. However, you can:

1. **Benefit from caching** by reusing node instances in workflows
2. **Monitor cache effectiveness** with statistics:
   ```python
   stats = node.get_cache_stats()
   print(f"Cache hits: {stats['hits']}, misses: {stats['misses']}, hit rate: {stats['hit_rate']:.1%}")
   ```
3. **Clear cache if needed** with `node.clear_cache()`
4. **Warm the cache** for known patterns:
   ```python
   node.warm_cache([
       {"data": sample_data, "threshold": 100},
       {"data": sample_data, "threshold": 200}
   ])
   ```
5. **Control cache behavior** with environment variables:
   - `KAILASH_DISABLE_PARAM_CACHE=true` - Disable caching
   - `KAILASH_PARAM_CACHE_SIZE=256` - Set cache size (default: 128)

### Cache Implementation Details

The parameter cache now handles special cases:
- **DataFrame/Series comparison**: Uses identity comparison to avoid pandas errors
- **NumPy arrays**: Uses identity comparison for performance
- **Thread safety**: All cache operations are thread-safe
- **LRU eviction**: Least recently used entries are evicted when cache is full

## Testing Your Migration

After migrating, test your nodes:

```python
# Test sync node
def test_sync_node():
    node = MySyncNode()
    result = node.execute(data=[1, 2, 3])
    assert "result" in result

# Test async node
async def test_async_node():
    node = MyAsyncNode()
    result = await node.execute_async(url="https://api.example.com")
    assert "data" in result

# Test in workflow
def test_workflow():
    workflow = Workflow("test", "Test")
    workflow.add_node("sync", MySyncNode())
    workflow.add_node("async", MyAsyncNode())

    runtime = LocalRuntime()
    results, run_id = runtime.execute(workflow)
    assert results["sync"]["result"] is not None
```

## Common Migration Issues

1. **Import errors**: Update imports for AsyncNode
2. **Method not found**: Ensure sync nodes implement run(), async implement async_run()
3. **Validation errors**: execute() now strictly validates - check your parameters
4. **Resource warnings**: Old nodes may leak resources - use context managers
5. **Performance**: Clear parameter cache if you dynamically change parameters
6. **DataFrame comparison errors**: Fixed in latest version - cache handles DataFrames correctly
7. **Async context errors**: Use `execute_async()` when calling from async context

## Test Suite Status

All architectural refactoring tests are now passing! The test suite validates:
- ✅ Sync/Async node separation (including middleware async execution)
- ✅ Execute/Run API standardization
- ✅ WorkflowBuilder API unification
- ✅ Resource management implementation
- ✅ Parameter resolution optimization (with DataFrame handling)
- ✅ Integration scenarios
- ✅ Cycle execution with proper iteration counting
- ✅ Parameter cache with thread safety and LRU eviction

**Total: 788 non-slow tests, all passing.**

## Summary

These changes make the SDK:
- **More predictable**: No runtime type detection
- **More performant**: Connection pooling, parameter caching
- **More maintainable**: Clear separation of concerns
- **More reliable**: Proper resource management
- **Backward compatible**: Most code continues to work

The migration is designed to be gradual - you can update nodes one at a time as needed.

## Implementation Notes

During implementation, several improvements were made to the core implementations based on test feedback:

1. **Parameter Caching**:
   - Added `_get_cached_parameters()` method to cache parameter definitions after first call
   - Implemented LRU eviction using OrderedDict for memory efficiency
   - Added cache statistics methods: `get_cache_stats()`, `clear_cache()`, `warm_cache()`
   - Fixed DataFrame comparison issue with `_safe_compare()` method

2. **Initialization Order**:
   - Moved cache initialization before `_validate_config()` to ensure proper setup
   - Cache is now initialized even before parameter validation

3. **Async Execution**:
   - Fixed middleware to call `execute_async()` for AsyncNode instances
   - AsyncNode's `execute()` always creates new event loop for sync contexts

4. **Bug Fixes**:
   - Fixed cycle iteration counting to use `loop_count` instead of `cycle_state.iteration`
   - Fixed parameter cache value collision when multiple parameters had same value
   - Fixed resource tracker to handle weak references properly

5. **Performance Testing**:
   - Improved performance test methodology to accurately measure caching benefits
   - Added comprehensive parameter cache test suite

## Additional Resources

- **Architecture Decision Records**: See [sdk-contributors/architecture/adr/007-parameter-resolution-caching.md](../../sdk-contributors/architecture/adr/007-parameter-resolution-caching.md)
- **Bug Documentation**: See [shared/mistakes/008-parameter-cache-value-collision.md](../../shared/mistakes/008-parameter-cache-value-collision.md)
- **Test Examples**: See `tests/unit/nodes/test_parameter_cache.py` for comprehensive cache testing patterns
