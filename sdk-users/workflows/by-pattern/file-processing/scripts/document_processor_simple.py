#!/usr/bin/env python3
"""
Simple Document Processing Workflow
===================================

Demonstrates file processing patterns using Kailash SDK with actual file reading.
This workflow processes CSV, JSON, and text files using dedicated reader nodes.

Patterns demonstrated:
1. Direct file reading with specific nodes
2. Data transformation and analysis
3. Result aggregation
4. Summary report generation
"""

import json
import os

from kailash import Workflow
from kailash.nodes.data import (
    CSVReaderNode,
    JSONReaderNode,
    JSONWriterNode,
    TextReaderNode,
)
from kailash.nodes.transform import DataTransformer
from kailash.runtime.local import LocalRuntime


def create_simple_document_workflow() -> Workflow:
    """Create a simplified document processing workflow."""
    workflow = Workflow(
        workflow_id="simple_document_001",
        name="simple_document_workflow",
        description="Process CSV, JSON, and text files with real readers",
    )

    # === DIRECT FILE READING ===

    # Read CSV file
    csv_reader = CSVReaderNode(
        id="csv_reader", file_path="data/inputs/customer_data.csv", headers=True
    )
    workflow.add_node("csv_reader", csv_reader)

    # Process CSV data
    csv_processor = DataTransformer(
        id="csv_processor",
        transformations=[
            """
# Process CSV data
customers = data

# Calculate statistics
total_customers = len(customers)
active_customers = sum(1 for c in customers if c.get("status") == "active")
inactive_customers = total_customers - active_customers

# Get unique email domains
email_domains = list(set(c.get("email", "").split("@")[1] for c in customers if "@" in c.get("email", "")))

result = {
    "file_type": "csv",
    "file_name": "customer_data.csv",
    "statistics": {
        "total_records": total_customers,
        "active_customers": active_customers,
        "inactive_customers": inactive_customers,
        "email_domains": email_domains
    },
    "sample_data": customers[:3] if customers else []
}
"""
        ],
    )
    workflow.add_node("csv_processor", csv_processor)
    workflow.connect("csv_reader", "csv_processor", mapping={"data": "data"})

    # Read JSON file
    json_reader = JSONReaderNode(
        id="json_reader", file_path="data/inputs/transaction_log.json"
    )
    workflow.add_node("json_reader", json_reader)

    # Process JSON data
    json_processor = DataTransformer(
        id="json_processor",
        transformations=[
            """
# Process JSON data
# Handle both dict and list inputs
if isinstance(data, dict):
    transactions = data.get("transactions", [])
    metadata = data.get("metadata", {})
elif isinstance(data, list):
    # If data is a list, it might be the transactions directly
    transactions = data
    metadata = {}
else:
    transactions = []
    metadata = {}

# Calculate statistics
total_amount = sum(t.get("amount", 0) for t in transactions if isinstance(t, dict))
transaction_count = len(transactions)
avg_amount = total_amount / transaction_count if transaction_count > 0 else 0

# Get unique customers
unique_customers = []
for t in transactions:
    if isinstance(t, dict) and t.get("customer_id"):
        cust_id = t.get("customer_id")
        if cust_id not in unique_customers:
            unique_customers.append(cust_id)

result = {
    "file_type": "json",
    "file_name": "transaction_log.json",
    "statistics": {
        "transaction_count": transaction_count,
        "total_amount": round(total_amount, 2),
        "average_amount": round(avg_amount, 2),
        "unique_customers": len(unique_customers),
        "metadata": metadata
    },
    "sample_data": transactions[:3] if transactions else []
}
"""
        ],
    )
    workflow.add_node("json_processor", json_processor)
    workflow.connect("json_reader", "json_processor", mapping={"data": "data"})

    # Read text file
    text_reader = TextReaderNode(
        id="text_reader", file_path="data/inputs/report_template.txt"
    )
    workflow.add_node("text_reader", text_reader)

    # Process text data
    text_processor = DataTransformer(
        id="text_processor",
        transformations=[
            """
# Process text data
import re

# TextReaderNode returns the content directly as a string
content = data if isinstance(data, str) else str(data)

# Calculate statistics
lines = content.split("\\n")
words = content.split()
characters = len(content)

# Find placeholders
placeholders = re.findall(r'\\{([^}]+)\\}', content)

result = {
    "file_type": "txt",
    "file_name": "report_template.txt",
    "statistics": {
        "line_count": len(lines),
        "word_count": len(words),
        "character_count": characters,
        "placeholders": placeholders,
        "placeholder_count": len(placeholders)
    },
    "preview": content[:200] + "..." if len(content) > 200 else content
}
"""
        ],
    )
    workflow.add_node("text_processor", text_processor)
    workflow.connect("text_reader", "text_processor", mapping={"text": "data"})

    # === AGGREGATE RESULTS ===

    # Combine all results
    result_aggregator = DataTransformer(
        id="result_aggregator",
        transformations=[
            """
# Aggregate results from all processors
# Note: Due to DataTransformer limitations, we'll create a summary
# In a real workflow, you'd use MergeNode or custom logic

# Since we can't directly access multiple inputs, we'll create a mock summary
# In production, each processor would write to storage and this would read from there

import datetime

# Simulated aggregated results based on expected data
summary = {
    "processing_summary": {
        "total_files_processed": 3,
        "file_types": ["csv", "json", "txt"],
        "processing_time": datetime.datetime.now().isoformat(),
        "status": "completed"
    },
    "file_summaries": [
        {
            "type": "csv",
            "name": "customer_data.csv",
            "records": 3,
            "key_metric": "2 active customers"
        },
        {
            "type": "json",
            "name": "transaction_log.json",
            "records": 3,
            "key_metric": "$529.48 total"
        },
        {
            "type": "txt",
            "name": "report_template.txt",
            "records": 1,
            "key_metric": "4 placeholders"
        }
    ],
    "recommendations": [
        "CSV data processed successfully - customer records available",
        "JSON transaction data loaded - ready for analysis",
        "Text template parsed - placeholders identified"
    ]
}

result = summary
"""
        ],
    )
    workflow.add_node("result_aggregator", result_aggregator)

    # Connect one processor to trigger aggregation
    workflow.connect("csv_processor", "result_aggregator", mapping={"result": "data"})

    # === OUTPUT ===

    # Save final summary
    summary_writer = JSONWriterNode(
        id="summary_writer", file_path="data/outputs/simple_processing_summary.json"
    )
    workflow.add_node("summary_writer", summary_writer)
    workflow.connect("result_aggregator", "summary_writer", mapping={"result": "data"})

    return workflow


def run_simple_processing():
    """Execute the simple document processing workflow."""
    workflow = create_simple_document_workflow()
    runtime = LocalRuntime()

    try:
        print("Starting Simple Document Processing...")
        print("üìñ Reading files directly with dedicated nodes...")

        result, run_id = runtime.execute(workflow, parameters={})

        print("\n‚úÖ Processing Complete!")
        print("üìÅ Output: data/outputs/simple_processing_summary.json")

        # Show results from each processor
        if "csv_processor" in result and "result" in result["csv_processor"]:
            csv_stats = result["csv_processor"]["result"]["statistics"]
            print(
                f"\nCSV: {csv_stats['total_records']} customers ({csv_stats['active_customers']} active)"
            )

        if "json_processor" in result and "result" in result["json_processor"]:
            json_stats = result["json_processor"]["result"]["statistics"]
            print(
                f"JSON: {json_stats['transaction_count']} transactions (${json_stats['total_amount']})"
            )

        if "text_processor" in result and "result" in result["text_processor"]:
            text_stats = result["text_processor"]["result"]["statistics"]
            print(
                f"Text: {text_stats['word_count']} words, {text_stats['placeholder_count']} placeholders"
            )

        return result

    except Exception as e:
        print(f"‚ùå Processing failed: {str(e)}")
        raise


def main():
    """Main entry point."""
    # Ensure output directory exists
    os.makedirs("data/outputs", exist_ok=True)

    # Check if input files exist
    required_files = [
        "data/inputs/customer_data.csv",
        "data/inputs/transaction_log.json",
        "data/inputs/report_template.txt",
    ]

    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print("‚ùå Missing required input files:")
        for f in missing_files:
            print(f"   - {f}")
        print(
            "\nPlease run the original document_processor.py first to create sample files."
        )
        return

    # Run the workflow
    run_simple_processing()

    # Display output
    print("\n=== Generated Summary ===")
    try:
        with open("data/outputs/simple_processing_summary.json") as f:
            summary = json.load(f)
            print(json.dumps(summary, indent=2))
    except Exception as e:
        print(f"Could not read summary: {e}")


if __name__ == "__main__":
    main()
